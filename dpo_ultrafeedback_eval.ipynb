{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5858b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824b6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b11c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and shuffle the full test split\n",
    "train_ultrafeedback = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\",\n",
    "                                   revision=\"292c16329d921287c4166934cac1a6ad1e13a6c5\",\n",
    "                                   split = 'train_prefs')\n",
    "\n",
    "test_ultrafeedback = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", \n",
    "                        revision=\"292c16329d921287c4166934cac1a6ad1e13a6c5\", \n",
    "                        split=\"test_prefs\").shuffle(seed=42)\n",
    "\n",
    "test_sample = test_ultrafeedback.select(range(100))\n",
    "eval_prompts = test_sample['prompt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b384a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = (\n",
    "    \"{% set image_count = namespace(value=0) %}\"\n",
    "    \"{% set video_count = namespace(value=0) %}\"\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{% if loop.first and message['role'] != 'system' %}\"\n",
    "    \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "    \"{% endif %}\"\n",
    "    \"<|im_start|>{{ message['role'] }}\\n\"\n",
    "    \"{% if message['content'] is string %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}\"\n",
    "    \"{% generation %}\"\n",
    "    \"{{ message['content'] }}\"\n",
    "    \"{% endgeneration %}\"\n",
    "    \"{% else %}\"\n",
    "    \"{{ message['content'] }}\"\n",
    "    \"{% endif %}\"\n",
    "    \"<|im_end|>\\n\"\n",
    "    \"{% else %}\"\n",
    "    \"{% for content in message['content'] %}\"\n",
    "    \"{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}\"\n",
    "    \"{% set image_count.value = image_count.value + 1 %}\"\n",
    "    \"{% if add_vision_id %}\"\n",
    "    \"Picture {{ image_count.value }}: \"\n",
    "    \"{% endif %}\"\n",
    "    \"<|vision_start|><|image_pad|><|vision_end|>\"\n",
    "    \"{% elif content['type'] == 'video' or 'video' in content %}\"\n",
    "    \"{% set video_count.value = video_count.value + 1 %}\"\n",
    "    \"{% if add_vision_id %}\"\n",
    "    \"Video {{ video_count.value }}: \"\n",
    "    \"{% endif %}\"\n",
    "    \"<|vision_start|><|video_pad|><|vision_end|>\"\n",
    "    \"{% elif 'text' in content %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}\"\n",
    "    \"{% generation %}\"\n",
    "    \"{{ content['text'] }}\"\n",
    "    \"{% endgeneration %}\"\n",
    "    \"{% else %}\"\n",
    "    \"{{ content['text'] }}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"<|im_end|>\\n\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"<|im_start|>assistant\\n\"\n",
    "    \"{% endif %}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3d5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "tokenizer.padding_side='left'\n",
    "tokenizer.add_special_tokens({'pad_token': '<|pad|>',\n",
    "                              'bos_token': '<|im_start|>',\n",
    "                              'eos_token': '<|im_end|>'})\n",
    "\n",
    "\n",
    "# sft_model = AutoModelForCausalLM.from_pretrained(SFT_MODEL, torch_dtype=torch.float16,).to(device)\n",
    "\n",
    "# sft_model.eval()\n",
    "\n",
    "# reference model\n",
    "# dpo_model = AutoModelForCausalLM.from_pretrained(DPO_MODEL, torch_dtype=torch.float16,).to(device)\n",
    "# dpo_model = PeftModel.from_pretrained(AutoModelForCausalLM.from_pretrained(SFT_MODEL, torch_dtype=torch.float16), \n",
    "#                                       DPO_MODEL).to(device)\n",
    "dpo_model = AutoModelForCausalLM.from_pretrained(\"./dpo_model\", torch_dtype=torch.float16,).to(device)\n",
    "\n",
    "dpo_model.eval()\n",
    "\n",
    "dpo_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "dpo_model.config.bos_token_id = tokenizer.bos_token_id\n",
    "dpo_model.config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd91469e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Write a 1,000-word op-ed piece in a formal tone, analyzing and providing examples of the ways in which social media platforms have been utilized to spread extremist and violent ideologies. In your analysis, discuss the specific tactics that these groups use to spread their messages online and the effects of these tactics on both individuals and society. Additionally, provide possible solutions that could be implemented to combat the spread of these dangerous ideologies on social media. Your piece should be well-researched, citing reputable sources to support your arguments.',\n",
       "  'role': 'user'},\n",
       " {'content': \"Title: The Noxious Alliance of Social Media and Extremism: A Threat to Human Progress\\n\\nThe advent of social media platforms has irrevocably revolutionized the way we communicate, share and consume information. While these platforms provide access to information for a global audience and allow individuals to connect regardless of geographical barriers, they have also become a breeding ground for the spread of extremist and violent ideologies, presenting a significant threat to human progress. The pervasiveness of social media, coupled with the anonymity afforded by these platforms, has made it easier for extremists to disseminate their messages, recruit new members, and incite violence against targeted groups, resulting in devastating consequences.\\n\\nExtremist groups often exploit the viral nature of social media to propagate their ideologies. Fast-paced videos, catchy slogans, and emotionally-charged content are a few of the strategies employed by these groups. Swift and aggressive tactics are used to capture the attention of the audience and to stir emotions, especially fear, anger, and hatred. The dissemination of misinformation and conspiracy theories facilitates the spread of extremist ideologies as they exploit the breakdown of trust in traditional media sources. Social media algorithms, designed to show users content that they are likely to engage with, can inadvertently promote more extreme content, further exacerbating visibility.\\n\\nThe Rohingya crisis in Myanmar provides a striking example of the harmful effects of social media promotion on extremism. In 2017, a blitz of Facebook posts filled with false information portrayed the Rohingya, a Muslim minority, as a threat to the predominantly Buddhist country. These posts, often containing graphic images and victim-blaming narratives, led to real-world violence, displacement, and genocide. As noted by the United Nations, social media played a “determining role” in creating hostility towards the Rohingya.\\n\\nThe influence of extremist groups on social media goes beyond victims in the immediate affected region. The diffusion of extremist ideologies online can impact anyone, regardless of geography. Individuals can be targeted and recruited into groups, potentially leading to real-world violence or radicalization. This phenomenon has been observed internationally, from the rise of extremist attacks across Europe to the influence of white supremacist groups in the United States.\\n\\nRadicalization can start with something as innocuous as an online discussion or the sharing of a controversial thread. Individuals vulnerable to extremist ideologies, often seeking answers or meaning in a rapidly changing world, can be drawn into the narratives presented by extremist groups. This further perpetuates social divisions and increases polarization, making it difficult to engage in productive dialogue and find solutions for societal issues.\\n\\nThe fight against the spread of extremist ideologies on social media requires a multifaceted approach, incorporating both technical and policy measures:\\n\\n1. Social Media Platforms' Responsibility: There should be a significant increase in monitoring and regulation by these platforms. Strict policies need to be enforced to prevent the spread of fake news, hate speech, and incitement to violence. Platforms should provide easy mechanisms for users to report harmful content, ensuring swift action in response.\\n\\n2. Education and Awareness: Individuals must be made aware of the tactics used by extremist groups on social media. This can be achieved through educational initiatives in schools and communities that teach about critical thinking, media literacy, and the promotion of respectful discourse.\\n\\n3. Counter-Narratives: Effective counter-narratives that challenge the extremist narratives should be encouraged and shared on these platforms. This can include sharing positive stories of diversity, resilience, and inclusion, showcasing the beauty of global humanity, and aggressively promoting fact-based content.\\n\\n4. Collaboration: Governments, civil society organizations, and tech giants should work together to combat this issue. Governments should enact and enforce laws to prevent the promotion of extremist ideologies online, while tech giants need to take investment, to design more effective algorithms that prioritize content promoting healthy social interactions and factual accuracy.\\n\\n5. Encourage Technology to Fight Extremism: The use of Artificial Intelligence and Machine Learning technologies can be instrumental in detecting and removing harmful content. These systems can analyze patterns in online discourse, flag content worthy of review, and even predict extreme behaviors before they escalate into violent actions.\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = train_ultrafeedback[0]\n",
    "\n",
    "ex[\"chosen\"]\n",
    "\n",
    "\n",
    "# ex = [{'content': 'how does the fed impact markets. give me 200 words.',\n",
    "#   'role': 'user'},\n",
    "#  {'content': \"The Federal Reserve, or the Fed, has a significant impact on financial markets through its monetary policy decisions. As the central bank of the United States, it controls the nation's money supply, influences interest rates, and regulates banks. Here's how the Fed affects markets in 200 words:\\n\\n1. Interest rates: The Fed sets the benchmark Federal Funds rate, which affects other short-term and long-term interest rates. Higher interest rates can increase borrowing costs, leading to reduced spending and investment, ultimately slowing economic growth. Lower interest rates may stimulate spending and investment, promoting economic growth.\\n2. Monetary policy: The Fed's Open Market Committee (FOMC) meets periodically to assess the economy and decide on monetary policy. Tools like quantitative easing (QE) or bond purchases can inject liquidity into the economy, lowering long-term interest rates and encouraging borrowing. Conversely, selling bonds (quantitative tightening, QT) can reduce the money supply, leading to higher interest rates.\\n3. Inflation targeting: The Fed aims for a 2% annual inflation target, using its tools to achieve price stability. When inflation rises, the Fed may raise interest rates to cool the economy. If deflation threatens, it may lower rates to stimulate growth.\\n4. Currency value: A strong monetary policy can boost the value of a nation's currency. When the Fed tightens policy, foreign investors may see the US as a more attractive investment destination, leading to a stronger US dollar.\\n5. Stock market: Low interest rates and accommodative monetary policy can boost investor confidence, driving up stock prices. Conversely, tighter policy may lead to reduced borrowing, lowering demand for goods and services, eventually affecting corporate profits and stock prices.\\n6. Fixed-income markets: The Fed's actions directly impact bond yields. Higher interest rates lead to higher yields and vice versa. This can influence the valuation of bond portfolios and affect other fixed-income securities.\\n7. Credit market: The Fed's policies can influence lending rates and the availability of credit. Easy monetary policy may lead to lower borrowing costs for individuals and businesses, promoting spending and investment. Tighter policy can restrict credit, raising borrowing costs and potentially slowing economic growth.\\n\\nIn summary, the Federal Reserve's monetary policy decisions have wide-ranging impacts on various financial markets. Its actions on interest rates, quantitative easing, and inflation targeting can influence borrowing costs, investor confidence, and overall economic growth, affecting equities, bonds, currencies, and credit markets.\",\n",
    "#   'role': 'assistant'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfc909c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Truncation error: Second sequence not provided",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokenized = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax_length\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43monly_second\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m torch.set_printoptions(profile=\u001b[33m\"\u001b[39m\u001b[33mfull\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1667\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.apply_chat_template\u001b[39m\u001b[34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[39m\n\u001b[32m   1664\u001b[39m     rendered_chat = rendered_chat[\u001b[32m0\u001b[39m]\n\u001b[32m   1666\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenize:\n\u001b[32m-> \u001b[39m\u001b[32m1667\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrendered_chat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1673\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1674\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n\u001b[32m   1677\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2867\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2865\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2866\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2867\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2868\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2869\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2977\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2955\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_encode_plus(\n\u001b[32m   2956\u001b[39m         batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001b[32m   2957\u001b[39m         add_special_tokens=add_special_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2974\u001b[39m         **kwargs,\n\u001b[32m   2975\u001b[39m     )\n\u001b[32m   2976\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2977\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2978\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2980\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2982\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2986\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2987\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2988\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2989\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2990\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2991\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2992\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2993\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2994\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2995\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2996\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2997\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2998\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3052\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3023\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3024\u001b[39m \u001b[33;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[32m   3025\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3040\u001b[39m \u001b[33;03m        method).\u001b[39;00m\n\u001b[32m   3041\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3043\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3044\u001b[39m     padding=padding,\n\u001b[32m   3045\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3049\u001b[39m     **kwargs,\n\u001b[32m   3050\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3052\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3055\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3056\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3058\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3059\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3060\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3061\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3062\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplit_special_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:615\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_encode_plus\u001b[39m(\n\u001b[32m    592\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    593\u001b[39m     text: Union[TextInput, PreTokenizedInput],\n\u001b[32m   (...)\u001b[39m\u001b[32m    612\u001b[39m     **kwargs,\n\u001b[32m    613\u001b[39m ) -> BatchEncoding:\n\u001b[32m    614\u001b[39m     batched_input = [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m     batched_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[32m    638\u001b[39m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:541\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens != split_special_tokens:\n\u001b[32m    539\u001b[39m     \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens = split_special_tokens\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[32m    548\u001b[39m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[32m    549\u001b[39m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[32m    551\u001b[39m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[32m    552\u001b[39m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[32m    553\u001b[39m tokens_and_encodings = [\n\u001b[32m    554\u001b[39m     \u001b[38;5;28mself\u001b[39m._convert_encoding(\n\u001b[32m    555\u001b[39m         encoding=encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    564\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[32m    565\u001b[39m ]\n",
      "\u001b[31mException\u001b[39m: Truncation error: Second sequence not provided"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.apply_chat_template(\n",
    "    ex[\"messages\"],\n",
    "    tokenize = True,\n",
    "    max_length = 50,\n",
    "    padding = 'max_length',\n",
    "    truncation = 'only_second',\n",
    "    return_dict = True,\n",
    "    return_assistant_tokens_mask=True,\n",
    "    add_generation_prompt = False,\n",
    "    chat_template = chat_template,\n",
    "    return_tensors = 'pt'\n",
    ")\n",
    "torch.set_printoptions(profile=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09aa6b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "         151665, 151665, 151644,   8948,    198,   2610,    525,    264,  10950,\n",
       "          17847,     13, 151645,    198, 151644,    198, 151645,    198, 151644,\n",
       "            198, 151645,    198, 151644,    198, 151645,    198, 151644,    198,\n",
       "         151645,    198, 151644,    198, 151645,    198, 151644,    198, 151645,\n",
       "            198, 151644,    198, 151645,    198]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c10a8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\", \n",
    "    api_key=\"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\"  # MY KEY\n",
    ")\n",
    "\n",
    "def get_reward_score(prompt, response):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"nvidia/llama-3.1-nemotron-70b-reward\",\n",
    "        messages=messages\n",
    "    )\n",
    "    content = result.choices[0].message.content.strip()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9891607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, prompts, model, tokenizer, type = 'sft'):\n",
    "    outputs_list = []\n",
    "    \n",
    "    rep_penalty = 1.22 if type == 'dpo' else 1 + 1e-5\n",
    "    # rep_penalty = 1e-5\n",
    "    max_len = 590\n",
    "\n",
    "\n",
    "    for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "        batch = prompts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        output_sequences = model.generate(\n",
    "            input_ids=inputs['input_ids'].to(model.device),\n",
    "            attention_mask=inputs['attention_mask'].to(model.device),\n",
    "            tokenizer = tokenizer,\n",
    "            do_sample=False, # disable sampling to test if batching affects output\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            forced_eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=rep_penalty,\n",
    "            stop_strings = '<|im_end|>',\n",
    "            exponential_decay_length_penalty = (int(max_len * 0.7),1.1),\n",
    "            max_new_tokens= max_len\n",
    "        )\n",
    "        completions_only = output_sequences[:, inputs['input_ids'].shape[1]:]\n",
    "        outputs_decoded = tokenizer.batch_decode(completions_only, skip_special_tokens=True)\n",
    "        # print(output_completions)\n",
    "        # print(output_sequences)\n",
    "        outputs_list.extend(outputs_decoded)\n",
    "    return outputs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c375594d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:45<00:00,  8.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "dpo_wins = 0\n",
    "total_evals = 0\n",
    "\n",
    "# sft_completions = generate_batch(BATCH_SIZE, eval_prompts, sft_model, tokenizer, type = 'sft')\n",
    "# with open('sft_completions_nr.pkl', 'rb') as f:\n",
    "#     sft_completions = pickle.load(f)\n",
    "dpo_completions = generate_batch(BATCH_SIZE, eval_prompts, dpo_model, tokenizer, type = 'dpo')\n",
    "scores = {'dpo': [], 'sft': []}\n",
    "\n",
    "for prompt, sft_response, dpo_response in zip(eval_prompts, sft_completions, dpo_completions):\n",
    "    sft_reward = get_reward_score(prompt, sft_response)\n",
    "    sft_reward = float(sft_reward.split(':')[-1])\n",
    "    dpo_reward = get_reward_score(prompt, dpo_response)\n",
    "    dpo_reward = float(dpo_reward.split(':')[-1])\n",
    "\n",
    "    scores['dpo'].append(dpo_reward)\n",
    "    scores['sft'].append(sft_reward) \n",
    "    \n",
    "    if dpo_reward >= sft_reward:\n",
    "        dpo_wins += 1\n",
    "        \n",
    "    total_evals += 1\n",
    "    \n",
    "winrate = dpo_wins/total_evals\n",
    "print(winrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff9ee08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sft_completions_nr.pkl', 'wb') as f:\n",
    "#     pickle.dump(sft_completions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7420ca3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"How can I use C++ to analyze a painting's form, content, and context, while also evaluating its use of color, composition, and brushwork? Additionally, how can I consider the artist's intentions and the painting's cultural significance, as well as any influences from previous artists or movements? Finally, how can I assess the painting's impact on subsequent works of art and discuss its reflection of societal values and beliefs of its time period? Please provide specific C++ code examples and utilize art terminology in your analysis. [Attach Painting]\",\n",
       " 'Given the text: With the rise of digital channels and platforms, there are more marketing technology solutions available than ever before. According to Scott Brinker\\'s annual #MarTech landscape, there were more than 5,000 software solutions available for marketers in 2017.\\nBut all this marketing technology is nothing without a strategy. Ultimately, a marketer\\'s goal is to increase sales and drive customer retention.\\nWhat\\'s interesting about this intersection of marketing and technology is how machine learning is giving marketers more insights into what people are doing and searching. Despite the proliferation of software tools, I firmly believe there\\'s a shift back to the good old days of personalization at a 1:1 level.\\nAgain, the biggest challenge is aligning new technology with your marketing strategy to connect with the people you want to buy from your company. If you\\'re a CMO, you need to consider how this technology is going to help drive engagement at each stage of the buyer\\'s journey and customer experience. Adding a new tool isn\\'t useful unless you know how it will work to drive new revenue.\\nAt the end of the day, regardless of whether you\\'re in marketing for a business-to-business (B2B) or business-to-consumer (B2C) organization, it\\'s about creating human-to-human (H2H) connections.\\nEven if you\\'re selling on eCommerce, it\\'s a human on the other side of the computer writing marketing copy convincing another human to buy something. Until the rise of the robot overlords, people buy from other people.\\nHere are six reasons why I believe the future of marketing is H2H.\\n1. \"Engagement is the new form fill.\"\\nThis is a quote from Kristen Wendel, one of the smartest women working in digital marketing operations today. For marketers, we know that not everyone who fills out a form on our website is going to buy from our company. Less than 1% of leads ever become revenue-generating customers.\\nWith H2H, engagement is a better way to measure success to see if the right people matching your ideal customer profile are interested in your company\\'s offering.Look beyond a form fill or content download and use the data in your customer relationship management (CRM) tool to find other touch points of engagement.\\nTracking meaningful conversations is much more important than the number of cold calls and email blasts your team does every week. The future of H2H focuses on new ways to get in front of your target buyer.\\nInstead of having sales reps leave countless voicemails for prospects, there are a ton of marketing technology solutions to leave a personalized message. Two of my favorite tools are Vidyard for 1:1 videos and Drift for online chat. Our sales team at my company, Terminus, has found tons of success for reaching the right people in target account using these tools.\\nYour buyer isn\\'t just on one channel anymore. Back in the day, it was all about email. Now for personalization at scale, it\\'s a combination of email, targeted advertising, and direct mail. Especially if you\\'re selling to multiple people in the same organization, it\\'s about taking an account-based marketing (ABM) approach. Don\\'t assume that only one channel will work for your campaign.\\nAcross an array of industries, there\\'s a focus on building a community of like-minded individuals. Your community will be bigger than your current customer base, and the more humans that are part of that community, the more opportunities you\\'ll create for marketing and sales.\\nWith H2H marketing, it\\'s about creating a compelling narrative, a story that people will actually care about. It\\'s extremely rare to get people to care about your brand unless you already have a certain amount of prestige associated with your company. Crafting a story that makes people care about what you\\'re selling will have a much stronger impact.\\nIn the modern digital world, it\\'s so difficult for a salesperson to break through the noise. To truly make a connection, salespeople have to assume the role of a consultant or trusted advisor. Marketers can support sales by serving up content-;e-books, white papers, webinars, blogs, you name it-;to help create thought leadership for the brand.\\nI can\\'t stress enough how important it is to connect with your potential customers, prospects, and opportunities so they care about what your company has to offer.\\nAt the end of the day, nobody likes to buy from a company: people buy from people. Your customers don\\'t care whether you\\'re in marketing or sales, but they care if you can help solve the problem they have.\\nWe\\'re all humans, and none of us is perfect. The least we can do is help each other. It goes beyond marketing; it\\'s about caring.\\nHow can marketers align new technology with their marketing strategy to drive engagement at each stage of the buyer\\'s journey and customer experience?',\n",
       " 'List 13 colors in a rainbow.',\n",
       " 'Adam Mitchel Lambert (born January 29, 1982) is an American singer, songwriter and stage actor. Since 2009, he has sold over 3 million albums and 5 million singles worldwide. Lambert rose to fame in 2009 after finishing as runner-up on the eighth season of American Idol. Later that year, he released his debut album, For Your Entertainment, which debuted at number three on the U.S. Billboard 200.\\n\\nLambert is best known for his theatrical performance style and meticulous attention to detail in all aspects of his personal presentation. He draws upon extensive stage experience in the ease with which he can refine and define his image through fashion and other imagery, which are essential to how he chooses to inhabit his songs, rivet his audiences and showcase his individuality. While a contestant on American Idol, Lambert\\'s precise yet varied stagings of himself kept audiences and judges glued as much to his presentation as to his vocal talent. His signature flamboyance and glam rock styling was a break-out moment in men\\'s fashion, duly noted by fashion publications and taste-makers, who compared him to Lady Gaga in terms of crossing style boundaries and being unabashedly individual.  Lambert made three fashion related TV appearances at the close of 2010. He fused his passion for music and fashion on MTV\\'s \"Talk@Playground\", appearing in discussion with Skingraft designer Jonny Cota. He was a guest judge on Project Runway, in an episode that styled a rock band for their upcoming Rolling Stone cover. He was the subject for whom the young designers of \"All on the Line with Joe Zee\" created a modern look, which he then critiqued along with the show\\'s hosts.  Lambert continued to grace the covers of magazines, moving more specifically into the fashion and culture space. Reflecting the mood and concept behind his album Trespassing, the Fault Magazine fashion shoot exemplified Lambert\\'s commitment to aligning the elements of his artistic vision so that a cohesive narrative emerges. When Lambert appeared on the December 2012 cover of London-based high style magazine Fiasco\\'s \"Obsession\" issue, he again took the opportunity to manipulate and provoke with his image and style. Sporting a sophisticated, minimalist look that recalled old Hollywood, Lambert played with male stereotypes and representations; and in the interview, emphasized that his fashion and presentation are often disparate from gay as well as straight regimes: \"For the general audience, they look at the way I style myself and they go, \\'Errrr, that\\'s gay\\', but you ask a handful of gay guys and they\\'re like, \\'I would never wear that!\\'\" In August, 2015, he was one of four artists to appear on the cover of Billboard\\'s \"Music\\'s Men of Style\" issue. He discussed his natural shift towards a cleaner, more classic look; and reiterated that the intersection of music and fashion--the constant motion of trends--is a fascination and part of being a pop musician.  Lambert is represented by London-based MiLK Management modelling agency as of July 2016.\\n\\nUsing a quote from the above article, answer the following question: What designers does he use for his style?',\n",
       " 'how to commit a folder which has been commited to another repository?',\n",
       " 'Discuss the negative effects of large-scale monoculture forestry practices on soil quality, air and water quality, biodiversity, and local communities. Analyze the long-term consequences of deforestation and habitat destruction, and consider alternative forestry practices that prioritize sustainability and conservation. Use scientific evidence and case studies to support your arguments and recommendations for policy changes.',\n",
       " \"Here's a challenge for you: Can you name a country whose capital city has the same number of letters as the highest level of education you have completed? And can you then tell me an interesting fact about that country? Good luck!\",\n",
       " 'Detailed Instructions: In this task, you are presented with a term, a description of the term, and an expected answer (\\'yes\\' or \\'no\\'). You should write a yes-no question about the given term such that the answer is the one provided to you (i.e., If the answer is \"No\", you should ask a question that its answer would be \"No\", and if the answer is \"Yes\", you should ask a question that its answer is \"Yes\". ). The question should have a definitive answer (as opposed to ambiguous or subjective questions, e.g., Is Batman a good movie?). Create a question such that its answer can not be found easily on a single web page (e.g., mentioned in a Wikipedia page). This can be accomplished if answering the question requires more than one fact (facts = statements that can be found on a Wikipedia page or Google).  For example, answering the question \\'did Aristotle use a laptop?\\', one needs the know about the invention of the laptop and the death of Aristotle. Avoid questions should not just compare the properties of objects (e.g., Is a door bigger than an elephant?) or those that refer to details in the given description.\\nProblem:Term: Bing (search engine), Description: Web search engine from Microsoft, Answer:Yes\\nSolution:',\n",
       " 'Is it a bad idea to put bananas in the refrigerator?',\n",
       " 'Can you confirm if the Durkee Embroidery Hoop - 24cm x 24cm (9\"x9\") Square is compatible with the Brother PR600/Baby Lock Series machines? Answer according to: NOTE: This item comes in a GREY Color, not available in Black. Note: Can only use up to 8x9\" designs on Brother PR machines, unless you are splitting designs on your embroidery software or Brother\\'s PE Design software first. Double height, 24cm x 24cm (9″×9″) square frame. Fits Brother PR600/Baby Lock Series machines. Common uses: Children’s clothing, larger square designs that round hoops cannot accommodate. This hoop fits the Brother PR600, PR-600C, PR600II, PR620, PR650, PR1000, and the PR1000e and the Baby Lock EMP6, BMP8, BMP9 and ENT10 embroidery machines. The hoop carries a LIFETIME WARRANTY on the plastic components and feature Durkee’s exclusive EZ TURN Thumbscrew. Ships with the brackets attached. Grids not available. *To embroider a design of 5 x 7\" you must allow for the design itself, as well as, the sewing head. The next size up for that from Durkee would be our 9” x 9” hoop. As long as that size hoop will fit into the project you are embroidering, that would be your best bet from Durkee. It also happens to be the most popular hoop size we sell to the home embroidery market. It is our pleasure to detail the guarantee provided for our tubular hoop line. We guarantee the plastic component of the round hoops for a lifetime against manufacturer\\'s defects. We will exchange the plastic component at no cost provided the breakage is not due to abuse or using the parts in a manner not consistent with commercial embroidery practices. The Durkee Embroidery Hoop - 24cm x 24cm (9\"x9\") Square is great for embroidering children’s clothing and larger square designs that round hoops cannot accommodate.This Durkee Embroidery Hoop - 24cm x 24cm (9\"x9\") Square is constructed of high impact plastic making it virtually indestructible. The Durkee Embroidery Hoop - 24cm x 24cm (9\"x9\") Square has brass inserts for all screws, light texture on the hooping surface, directional guides, and extra height, which aids in holding thicker, harder to hold garments. The adjusting rings\\' thumb screws on the Durkee Embroidery Hoop - 24cm x 24cm (9\"x9\") Square are large and long to provide more torque and have a wider space for adjustments. The metal clips on the Durkee Embroidery Hoop - 24cm x 24cm (9\"x9\") Square are extra long, aiding in vibration reduction. The Durkee Embroidery Hoop - 24cm x 24cm (9\"x9\") Square metal clips are zinc plated to provide reduced wear, and they come complete with plastic screws and lock washers. Thumb screw is larger and longer than our competitors\\'. Wide space thumb screw adjusting. For optimum design registration, it is important to use a hoop that is closest to the size of the design. This will help eliminate puckering as well as wasting other supplies, such as backing or topping. Very efficient and the website worked out great for me! The hoop is wonderful. It allows for a larger size design than a 5x7 on a kid shirt. the only problem is that the threads strip after a while leaving the hoop unusable. I love the hoop though so I am about to order my 4th one. Will order again from your site! The price was right. I\\'ll be shopping here again. Love you guys... just you have so many items that sometimes it is hard to find what I am looking for. I purchased this hoop in 2011 and recently wanted to use it for an 8\" x 8\". The hoop is not large enough as the corners are too rounded and the hoop will not accommodate a square design. The wrong Durkee hoop was sent but Delores was wonderful and took care of the problem to my complete satisfaction. Will definitely order from All Brands again! I will order from Allbrands before any other internet business when possible because of the friendly, prompt response to all orders and the best prices on nearly all items. Very rarely do I find an item priced cheaper, but I still order from Allbrands because of the very prompt filling of orders. Shipment was fast, price was good. Love the hoop.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_prompts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88802ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8afe1466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I use C++ to analyze a painting's form, content, and context, while also evaluating its use\n",
      "of color, composition, and brushwork? Additionally, how can I consider the artist's intentions and\n",
      "the painting's cultural significance, as well as any influences from previous artists or movements?\n",
      "Finally, how can I assess the painting's impact on subsequent works of art and discuss its\n",
      "reflection of societal values and beliefs of its time period? Please provide specific C++ code\n",
      "examples and utilize art terminology in your analysis. [Attach Painting]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import textwrap\n",
    "\n",
    "wrapped_text = textwrap.fill(eval_prompts[idx], width=100)\n",
    "\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aee92c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Code Example]: Here is an example of using C++ to analyze a painting: ``` #include <iostream>\n",
      "using namespace std; int main() {     // Load image     Image img = loadImage(\"painting.jpg\");\n",
      "// Analyze form     int width = getWidth(img);     int height = getHeight(img);     cout << \"Width:\n",
      "\";     cout << width << endl;     cout << \"Height: \";     cout << height << endl;          //\n",
      "Evaluate content     string text = getWordCount(getText(img));     cout << \"Content: \" << text <<\n",
      "endl;          // Consider colors     Color c1 = getColorAt(0, 50);     Color c2 = getColorAt(width\n",
      "- 30, height - 40);     if (c1 != c2) {         cout << \"Colors do not match\" << endl;     } } ```\n",
      "This code loads an image file into memory, analyzes it for shape and texture, evaluates its content\n",
      "by counting words within the text area, considers the artwork's intended meaning through word count\n",
      "calculations, and then discusses potential impacts. This approach allows you to evaluate multiple\n",
      "aspects of the painting simultaneously without having access to all information about each element\n",
      "individually. The resulting output will be comprehensive insights that take into account various\n",
      "factors such as artistic intent, historical context, social implications, and more.  Here are some\n",
      "additional considerations when analyzing paintings:  - When considering the painter's\n",
      "intentionality, think about their style, technique, and overall aesthetic vision rather than just\n",
      "focusing solely on what they created physically. - Take note of other artworks related to this\n",
      "piece, including those made during different periods or with similar themes, which may offer\n",
      "valuable perspectives on the work itself. - Be aware of possible biases or assumptions inherent in\n",
      "certain interpretations or evaluations – always strive to remain objective and open-minded\n",
      "throughout the process. - Don't underestimate the importance of contextual understanding; even\n",
      "seemingly unrelated elements like lighting conditions, environmental settings, or personal emotions\n",
      "surrounding the creation can significantly influence our interpretation of a particular work\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wrapped_text = textwrap.fill(dpo_completions[idx], width=100)\n",
    "\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "660c94f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Attach C++ Code] To analyze a painting's form, content, and context, you can use C++ to create a\n",
      "program that takes in the painting's dimensions, content, and context, and then evaluates its form,\n",
      "content, and context using various C++ functions and algorithms. For example, you can use the\n",
      "`std::cout` and `std::cin` libraries to read the dimensions and content of the painting, and then\n",
      "use C++ functions such as `std::cout << \"Form: \" << form << std::endl;` and `std::cout << \"Content:\n",
      "\" << content << std::endl;` to display the form and content of the painting. To evaluate the use of\n",
      "color, composition, and brushwork, you can use C++ functions such as `std::cout << \"Color: \" <<\n",
      "color << std::endl;` and `std::cout << \"Composition: \" << composition << std::endl;` to display the\n",
      "color and composition of the painting. To consider the artist's intentions and the painting's\n",
      "cultural significance, you can use C++ functions such as `std::cout << \"Artist's Intention: \" <<\n",
      "artist << std::endl;` and `std::cout << \"Cultural Significance: \" << cultural significance <<\n",
      "std::endl;` to display the artist's intention and cultural significance of the painting. To assess\n",
      "the painting's impact on subsequent works of art, you can use C++ functions such as `std::cout <<\n",
      "\"Impact: \" << impact << std::endl;` to display the impact of the painting on subsequent works of\n",
      "art. To discuss the painting's reflection of societal values and beliefs of its time period, you can\n",
      "use C++ functions such as `std::cout << \"Reflection: \" << reflection << std::endl;` to display the\n",
      "reflection of societal values and beliefs of the painting in the time period. Here is an example of\n",
      "how you can use C++ to analyze a painting's form, content, and context, evaluate its use of color,\n",
      "composition, and brush\n"
     ]
    }
   ],
   "source": [
    "wrapped_text = textwrap.fill(sft_completions[idx], width=100)\n",
    "\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b47ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_reward = get_reward_score(eval_prompts[idx], sft_completions[idx])\n",
    "sft_reward = float(sft_reward.split(':')[-1])\n",
    "dpo_reward = get_reward_score(eval_prompts[idx], dpo_completions[idx])\n",
    "dpo_reward = float(dpo_reward.split(':')[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35145d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-26.75, -23.625)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_reward, dpo_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e62185b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ref_completions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mref_completions\u001b[49m[-\u001b[32m1\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'ref_completions' is not defined"
     ]
    }
   ],
   "source": [
    "ref_completions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ac6f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21642546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# from vllm import LLM, SamplingParams\n",
    "\n",
    "# # Sample prompts.\n",
    "# prompts = [\n",
    "#     \"Hello, my name is\",\n",
    "#     \"The president of the United States is\",\n",
    "#     \"The capital of France is\",\n",
    "#     \"The future of AI is\",\n",
    "# ]\n",
    "# # Create a sampling params object.\n",
    "# sampling_params = SamplingParams(repetition_penalty=1.5, max_tokens=590)\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     # Create an LLM.\n",
    "#     llm = LLM(model=\"facebook/opt-125m\")\n",
    "    \n",
    "#     llm = LLM(model=\"./finished_smoltalk\")\n",
    "#     # Generate texts from the prompts.\n",
    "#     # The output is a list of RequestOutput objects\n",
    "#     # that contain the prompt, generated text, and other information.\n",
    "#     outputs = llm.generate(prompts, sampling_params)\n",
    "#     # Print the outputs.\n",
    "#     print(\"\\nGenerated Outputs:\\n\" + \"-\" * 60)\n",
    "#     for output in outputs:\n",
    "#         prompt = output.prompt\n",
    "#         generated_text = output.outputs[0].text\n",
    "#         print(f\"Prompt:    {prompt!r}\")\n",
    "#         print(f\"Output:    {generated_text!r}\")\n",
    "#         print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ray\n",
    "# from ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor\n",
    "# import numpy as np\n",
    "\n",
    "# config = vLLMEngineProcessorConfig(\n",
    "#     model_source=\"unsloth/Llama-3.1-8B-Instruct\",\n",
    "#     engine_kwargs={\n",
    "#         \"enable_chunked_prefill\": True,\n",
    "#         \"max_num_batched_tokens\": 4096,\n",
    "#         \"max_model_len\": 16384,\n",
    "#     },\n",
    "#     concurrency=1,\n",
    "#     batch_size=32,\n",
    "# )\n",
    "# processor = build_llm_processor(\n",
    "#     config,\n",
    "#     preprocess=lambda row: dict(\n",
    "#         messages=[\n",
    "#             {\"role\": \"user\", \"content\": row[\"item\"]}\n",
    "#         ],\n",
    "#         sampling_params=dict(\n",
    "#             temperature=0.3,\n",
    "#             max_tokens=250,\n",
    "#         )\n",
    "#     ),\n",
    "#     postprocess=lambda row: dict(\n",
    "#         answer=row[\"generated_text\"],\n",
    "#         **row  # This will return all the original columns in the dataset.\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# ds = ray.data.from_items([\"Start of the haiku is: Complete this for me...\"])\n",
    "\n",
    "# ds = processor(ds)\n",
    "# ds.show(limit=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
