{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5858b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-10 00:16:02.759499: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749514562.768953   12993 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749514562.774050   12993 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e4b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824b6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b11c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load and shuffle the full test split\n",
    "# train_ultrafeedback = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\",\n",
    "#                                    revision=\"292c16329d921287c4166934cac1a6ad1e13a6c5\",\n",
    "#                                    split = 'train_prefs')\n",
    "\n",
    "# test_ultrafeedback = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", \n",
    "#                         revision=\"292c16329d921287c4166934cac1a6ad1e13a6c5\", \n",
    "#                         split=\"test_prefs\").shuffle(seed=42)\n",
    "\n",
    "# test_sample = test_ultrafeedback.select(range(100))\n",
    "# eval_prompts = test_sample['prompt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b384a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = (\n",
    "    \"{% set image_count = namespace(value=0) %}\"\n",
    "    \"{% set video_count = namespace(value=0) %}\"\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{% if loop.first and message['role'] != 'system' %}\"\n",
    "    \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "    \"{% endif %}\"\n",
    "    \"<|im_start|>{{ message['role'] }}\\n\"\n",
    "    \"{% if message['content'] is string %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}\"\n",
    "    \"{% generation %}\"\n",
    "    \"{{ message['content'] }}\"\n",
    "    \"{% endgeneration %}\"\n",
    "    \"{% else %}\"\n",
    "    \"{{ message['content'] }}\"\n",
    "    \"{% endif %}\"\n",
    "    \"<|im_end|>\\n\"\n",
    "    \"{% else %}\"\n",
    "    \"{% for content in message['content'] %}\"\n",
    "    \"{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}\"\n",
    "    \"{% set image_count.value = image_count.value + 1 %}\"\n",
    "    \"{% if add_vision_id %}\"\n",
    "    \"Picture {{ image_count.value }}: \"\n",
    "    \"{% endif %}\"\n",
    "    \"<|vision_start|><|image_pad|><|vision_end|>\"\n",
    "    \"{% elif content['type'] == 'video' or 'video' in content %}\"\n",
    "    \"{% set video_count.value = video_count.value + 1 %}\"\n",
    "    \"{% if add_vision_id %}\"\n",
    "    \"Video {{ video_count.value }}: \"\n",
    "    \"{% endif %}\"\n",
    "    \"<|vision_start|><|video_pad|><|vision_end|>\"\n",
    "    \"{% elif 'text' in content %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}\"\n",
    "    \"{% generation %}\"\n",
    "    \"{{ content['text'] }}\"\n",
    "    \"{% endgeneration %}\"\n",
    "    \"{% else %}\"\n",
    "    \"{{ content['text'] }}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"<|im_end|>\\n\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"<|im_start|>assistant\\n\"\n",
    "    \"{% endif %}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc3d5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "tokenizer.padding_side='left'\n",
    "tokenizer.add_special_tokens({'pad_token': '<|pad|>',\n",
    "                              'bos_token': '<|im_start|>',\n",
    "                              'eos_token': '<|im_end|>'})\n",
    "\n",
    "\n",
    "# sft_model = AutoModelForCausalLM.from_pretrained(SFT_MODEL, torch_dtype=torch.float16,).to(device)\n",
    "\n",
    "# sft_model.eval()\n",
    "\n",
    "# reference model\n",
    "# dpo_model = AutoModelForCausalLM.from_pretrained(DPO_MODEL, torch_dtype=torch.float16,).to(device)\n",
    "# dpo_model = PeftModel.from_pretrained(AutoModelForCausalLM.from_pretrained(SFT_MODEL, torch_dtype=torch.float16), \n",
    "#                                       DPO_MODEL).to(device)\n",
    "dpo_model = AutoModelForCausalLM.from_pretrained(\"./dpo_model\", torch_dtype=torch.float16,).to(device)\n",
    "\n",
    "dpo_model.eval()\n",
    "\n",
    "dpo_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "dpo_model.config.bos_token_id = tokenizer.bos_token_id\n",
    "dpo_model.config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd91469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_prompts.pkl\", \"rb\") as f:\n",
    "    test_prompts = pickle.load(f)\n",
    "with open(\"test_completions.pkl\", \"rb\") as f:\n",
    "    test_completions = pickle.load(f)\n",
    "\n",
    "# ex = train_ultrafeedback[0]\n",
    "\n",
    "# ex[\"chosen\"]\n",
    "\n",
    "\n",
    "# ex = [{'content': 'how does the fed impact markets. give me 200 words.',\n",
    "#   'role': 'user'},\n",
    "#  {'content': \"The Federal Reserve, or the Fed, has a significant impact on financial markets through its monetary policy decisions. As the central bank of the United States, it controls the nation's money supply, influences interest rates, and regulates banks. Here's how the Fed affects markets in 200 words:\\n\\n1. Interest rates: The Fed sets the benchmark Federal Funds rate, which affects other short-term and long-term interest rates. Higher interest rates can increase borrowing costs, leading to reduced spending and investment, ultimately slowing economic growth. Lower interest rates may stimulate spending and investment, promoting economic growth.\\n2. Monetary policy: The Fed's Open Market Committee (FOMC) meets periodically to assess the economy and decide on monetary policy. Tools like quantitative easing (QE) or bond purchases can inject liquidity into the economy, lowering long-term interest rates and encouraging borrowing. Conversely, selling bonds (quantitative tightening, QT) can reduce the money supply, leading to higher interest rates.\\n3. Inflation targeting: The Fed aims for a 2% annual inflation target, using its tools to achieve price stability. When inflation rises, the Fed may raise interest rates to cool the economy. If deflation threatens, it may lower rates to stimulate growth.\\n4. Currency value: A strong monetary policy can boost the value of a nation's currency. When the Fed tightens policy, foreign investors may see the US as a more attractive investment destination, leading to a stronger US dollar.\\n5. Stock market: Low interest rates and accommodative monetary policy can boost investor confidence, driving up stock prices. Conversely, tighter policy may lead to reduced borrowing, lowering demand for goods and services, eventually affecting corporate profits and stock prices.\\n6. Fixed-income markets: The Fed's actions directly impact bond yields. Higher interest rates lead to higher yields and vice versa. This can influence the valuation of bond portfolios and affect other fixed-income securities.\\n7. Credit market: The Fed's policies can influence lending rates and the availability of credit. Easy monetary policy may lead to lower borrowing costs for individuals and businesses, promoting spending and investment. Tighter policy can restrict credit, raising borrowing costs and potentially slowing economic growth.\\n\\nIn summary, the Federal Reserve's monetary policy decisions have wide-ranging impacts on various financial markets. Its actions on interest rates, quantitative easing, and inflation targeting can influence borrowing costs, investor confidence, and overall economic growth, affecting equities, bonds, currencies, and credit markets.\",\n",
    "#   'role': 'assistant'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c10a8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\", \n",
    "    api_key=\"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\"  # MY KEY\n",
    ")\n",
    "\n",
    "def get_reward_score(prompt, response):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"nvidia/llama-3.1-nemotron-70b-reward\",\n",
    "        messages=messages\n",
    "    )\n",
    "    content = result.choices[0].message.content.strip()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9891607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, prompts, model, tokenizer, type = 'sft'):\n",
    "    outputs_list = []\n",
    "    \n",
    "    rep_penalty = 1.22 if type == 'dpo' else 1 + 1e-5\n",
    "    # rep_penalty = 1e-5\n",
    "    max_len = 590\n",
    "\n",
    "\n",
    "    for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "        batch = prompts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        output_sequences = model.generate(\n",
    "            input_ids=inputs['input_ids'].to(model.device),\n",
    "            attention_mask=inputs['attention_mask'].to(model.device),\n",
    "            tokenizer = tokenizer,\n",
    "            do_sample=False, # disable sampling to test if batching affects output\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            forced_eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=rep_penalty,\n",
    "            stop_strings = '<|im_end|>',\n",
    "            exponential_decay_length_penalty = (int(max_len * 0.7),1.1),\n",
    "            max_new_tokens= max_len\n",
    "        )\n",
    "        completions_only = output_sequences[:, inputs['input_ids'].shape[1]:]\n",
    "        outputs_decoded = tokenizer.batch_decode(completions_only, skip_special_tokens=True)\n",
    "        # print(output_completions)\n",
    "        # print(output_sequences)\n",
    "        outputs_list.extend(outputs_decoded)\n",
    "    return outputs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c375594d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 24/63 [04:14<06:54, 10.62s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 110.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 102.38 MiB is free. Including non-PyTorch memory, this process has 2.96 GiB memory in use. Process 13327 has 91.43 GiB memory in use. Of the allocated memory 1.93 GiB is allocated by PyTorch, and 346.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12993/1845584589.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# with open('sft_completions_nr.pkl', 'rb') as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     sft_completions = pickle.load(f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdpo_completions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpo_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dpo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'dpo'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ct'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12993/1547604490.py\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(batch_size, prompts, model, tokenizer, type)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         output_sequences = model.generate(\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2596\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2597\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2598\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3556\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3557\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3558\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3559\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    704\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    437\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 110.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 102.38 MiB is free. Including non-PyTorch memory, this process has 2.96 GiB memory in use. Process 13327 has 91.43 GiB memory in use. Of the allocated memory 1.93 GiB is allocated by PyTorch, and 346.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "ct_wins = 0\n",
    "total_evals = 0\n",
    "\n",
    "# sft_completions = generate_batch(BATCH_SIZE, eval_prompts, sft_model, tokenizer, type = 'sft')\n",
    "# with open('sft_completions_nr.pkl', 'rb') as f:\n",
    "#     sft_completions = pickle.load(f)\n",
    "dpo_completions = generate_batch(BATCH_SIZE, test_prompts, dpo_model, tokenizer, type = 'dpo')\n",
    "scores = {'dpo': [], 'ct': []}\n",
    "\n",
    "for prompt, ct_response, dpo_response in zip(test_prompts, test_completions, dpo_completions):\n",
    "    ct_reward = get_reward_score(prompt, ct_response)\n",
    "    ct_reward = float(ct_reward.split(':')[-1])\n",
    "    dpo_reward = get_reward_score(prompt, dpo_response)\n",
    "    dpo_reward = float(dpo_reward.split(':')[-1])\n",
    "\n",
    "    scores['dpo'].append(dpo_reward)\n",
    "    scores['ct'].append(ct_reward) \n",
    "    \n",
    "    if ct_reward >= dpo_reward:\n",
    "        ct_wins += 1\n",
    "        \n",
    "    total_evals += 1\n",
    "    \n",
    "winrate = ct_wins/total_evals\n",
    "print(winrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff9ee08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sft_completions_nr.pkl', 'wb') as f:\n",
    "#     pickle.dump(sft_completions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7420ca3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"How can I use C++ to analyze a painting's form, content, and context, while also evaluating its use of color, composition, and brushwork? Additionally, how can I consider the artist's intentions and the painting's cultural significance, as well as any influences from previous artists or movements? Finally, how can I assess the painting's impact on subsequent works of art and discuss its reflection of societal values and beliefs of its time period? Please provide specific C++ code examples and utilize art terminology in your analysis. [Attach Painting]\",\n",
       " 'Given the text: With the rise of digital channels and platforms, there are more marketing technology solutions available than ever before. According to Scott Brinker\\'s annual #MarTech landscape, there were more than 5,000 software solutions available for marketers in 2017.\\nBut all this marketing technology is nothing without a strategy. Ultimately, a marketer\\'s goal is to increase sales and drive customer retention.\\nWhat\\'s interesting about this intersection of marketing and technology is how machine learning is giving marketers more insights into what people are doing and searching. Despite the proliferation of software tools, I firmly believe there\\'s a shift back to the good old days of personalization at a 1:1 level.\\nAgain, the biggest challenge is aligning new technology with your marketing strategy to connect with the people you want to buy from your company. If you\\'re a CMO, you need to consider how this technology is going to help drive engagement at each stage of the buyer\\'s journey and customer experience. Adding a new tool isn\\'t useful unless you know how it will work to drive new revenue.\\nAt the end of the day, regardless of whether you\\'re in marketing for a business-to-business (B2B) or business-to-consumer (B2C) organization, it\\'s about creating human-to-human (H2H) connections.\\nEven if you\\'re selling on eCommerce, it\\'s a human on the other side of the computer writing marketing copy convincing another human to buy something. Until the rise of the robot overlords, people buy from other people.\\nHere are six reasons why I believe the future of marketing is H2H.\\n1. \"Engagement is the new form fill.\"\\nThis is a quote from Kristen Wendel, one of the smartest women working in digital marketing operations today. For marketers, we know that not everyone who fills out a form on our website is going to buy from our company. Less than 1% of leads ever become revenue-generating customers.\\nWith H2H, engagement is a better way to measure success to see if the right people matching your ideal customer profile are interested in your company\\'s offering.Look beyond a form fill or content download and use the data in your customer relationship management (CRM) tool to find other touch points of engagement.\\nTracking meaningful conversations is much more important than the number of cold calls and email blasts your team does every week. The future of H2H focuses on new ways to get in front of your target buyer.\\nInstead of having sales reps leave countless voicemails for prospects, there are a ton of marketing technology solutions to leave a personalized message. Two of my favorite tools are Vidyard for 1:1 videos and Drift for online chat. Our sales team at my company, Terminus, has found tons of success for reaching the right people in target account using these tools.\\nYour buyer isn\\'t just on one channel anymore. Back in the day, it was all about email. Now for personalization at scale, it\\'s a combination of email, targeted advertising, and direct mail. Especially if you\\'re selling to multiple people in the same organization, it\\'s about taking an account-based marketing (ABM) approach. Don\\'t assume that only one channel will work for your campaign.\\nAcross an array of industries, there\\'s a focus on building a community of like-minded individuals. Your community will be bigger than your current customer base, and the more humans that are part of that community, the more opportunities you\\'ll create for marketing and sales.\\nWith H2H marketing, it\\'s about creating a compelling narrative, a story that people will actually care about. It\\'s extremely rare to get people to care about your brand unless you already have a certain amount of prestige associated with your company. Crafting a story that makes people care about what you\\'re selling will have a much stronger impact.\\nIn the modern digital world, it\\'s so difficult for a salesperson to break through the noise. To truly make a connection, salespeople have to assume the role of a consultant or trusted advisor. Marketers can support sales by serving up content-;e-books, white papers, webinars, blogs, you name it-;to help create thought leadership for the brand.\\nI can\\'t stress enough how important it is to connect with your potential customers, prospects, and opportunities so they care about what your company has to offer.\\nAt the end of the day, nobody likes to buy from a company: people buy from people. Your customers don\\'t care whether you\\'re in marketing or sales, but they care if you can help solve the problem they have.\\nWe\\'re all humans, and none of us is perfect. The least we can do is help each other. It goes beyond marketing; it\\'s about caring.\\nHow can marketers align new technology with their marketing strategy to drive engagement at each stage of the buyer\\'s journey and customer experience?',\n",
       " 'List 13 colors in a rainbow.',\n",
       " 'Adam Mitchel Lambert (born January 29, 1982) is an American singer, songwriter and stage actor. Since 2009, he has sold over 3 million albums and 5 million singles worldwide. Lambert rose to fame in 2009 after finishing as runner-up on the eighth season of American Idol. Later that year, he released his debut album, For Your Entertainment, which debuted at number three on the U.S. Billboard 200.\\n\\nLambert is best known for his theatrical performance style and meticulous attention to detail in all aspects of his personal presentation. He draws upon extensive stage experience in the ease with which he can refine and define his image through fashion and other imagery, which are essential to how he chooses to inhabit his songs, rivet his audiences and showcase his individuality. While a contestant on American Idol, Lambert\\'s precise yet varied stagings of himself kept audiences and judges glued as much to his presentation as to his vocal talent. His signature flamboyance and glam rock styling was a break-out moment in men\\'s fashion, duly noted by fashion publications and taste-makers, who compared him to Lady Gaga in terms of crossing style boundaries and being unabashedly individual.  Lambert made three fashion related TV appearances at the close of 2010. He fused his passion for music and fashion on MTV\\'s \"Talk@Playground\", appearing in discussion with Skingraft designer Jonny Cota. He was a guest judge on Project Runway, in an episode that styled a rock band for their upcoming Rolling Stone cover. He was the subject for whom the young designers of \"All on the Line with Joe Zee\" created a modern look, which he then critiqued along with the show\\'s hosts.  Lambert continued to grace the covers of magazines, moving more specifically into the fashion and culture space. Reflecting the mood and concept behind his album Trespassing, the Fault Magazine fashion shoot exemplified Lambert\\'s commitment to aligning the elements of his artistic vision so that a cohesive narrative emerges. When Lambert appeared on the December 2012 cover of London-based high style magazine Fiasco\\'s \"Obsession\" issue, he again took the opportunity to manipulate and provoke with his image and style. Sporting a sophisticated, minimalist look that recalled old Hollywood, Lambert played with male stereotypes and representations; and in the interview, emphasized that his fashion and presentation are often disparate from gay as well as straight regimes: \"For the general audience, they look at the way I style myself and they go, \\'Errrr, that\\'s gay\\', but you ask a handful of gay guys and they\\'re like, \\'I would never wear that!\\'\" In August, 2015, he was one of four artists to appear on the cover of Billboard\\'s \"Music\\'s Men of Style\" issue. He discussed his natural shift towards a cleaner, more classic look; and reiterated that the intersection of music and fashion--the constant motion of trends--is a fascination and part of being a pop musician.  Lambert is represented by London-based MiLK Management modelling agency as of July 2016.\\n\\nUsing a quote from the above article, answer the following question: What designers does he use for his style?',\n",
       " 'how to commit a folder which has been commited to another repository?',\n",
       " 'Discuss the negative effects of large-scale monoculture forestry practices on soil quality, air and water quality, biodiversity, and local communities. Analyze the long-term consequences of deforestation and habitat destruction, and consider alternative forestry practices that prioritize sustainability and conservation. Use scientific evidence and case studies to support your arguments and recommendations for policy changes.',\n",
       " \"Here's a challenge for you: Can you name a country whose capital city has the same number of letters as the highest level of education you have completed? And can you then tell me an interesting fact about that country? Good luck!\",\n",
       " 'Detailed Instructions: In this task, you are presented with a term, a description of the term, and an expected answer (\\'yes\\' or \\'no\\'). You should write a yes-no question about the given term such that the answer is the one provided to you (i.e., If the answer is \"No\", you should ask a question that its answer would be \"No\", and if the answer is \"Yes\", you should ask a question that its answer is \"Yes\". ). The question should have a definitive answer (as opposed to ambiguous or subjective questions, e.g., Is Batman a good movie?). Create a question such that its answer can not be found easily on a single web page (e.g., mentioned in a Wikipedia page). This can be accomplished if answering the question requires more than one fact (facts = statements that can be found on a Wikipedia page or Google).  For example, answering the question \\'did Aristotle use a laptop?\\', one needs the know about the invention of the laptop and the death of Aristotle. Avoid questions should not just compare the properties of objects (e.g., Is a door bigger than an elephant?) or those that refer to details in the given description.\\nProblem:Term: Bing (search engine), Description: Web search engine from Microsoft, Answer:Yes\\nSolution:',\n",
       " 'Is it a bad idea to put bananas in the refrigerator?',\n",
       " 'Can you confirm if the Durkee Embroidery Hoop - 24cm x 24cm (9\"x9\") Square is compatible with the Brother PR600/Baby Lock Series machines? Answer according to: NOTE: This item comes in a GREY Color, not available in Black. Note: Can only use up to 8x9\" designs on Brother PR machines, unless you are splitting designs on your embroidery software or Brother\\'s PE Design software first. Double height, 24cm x 24cm (9″×9″) square frame. Fits Brother PR600/Baby Lock Series machines. Common uses: Children’s clothing, larger square designs that round hoops cannot accommodate. This hoop fits the Brother PR600, PR-600C, PR600II, PR620, PR650, PR1000, and the PR1000e and the Baby Lock EMP6, BMP8, BMP9 and ENT10 embroidery machines. The hoop carries a LIFETIME WARRANTY on the plastic components and feature Durkee’s exclusive EZ TURN Thumbscrew. Ships with the brackets attached. Grids not available. *To embroider a design of 5 x 7\" you must allow for the design itself, as well as, the sewing head. The next size up for that from Durkee would be our 9” x 9” hoop. As long as that size hoop will fit into the project you are embroidering, that would be your best bet from Durkee. It also happens to be the most popular hoop size we sell to the home embroidery market. It is our pleasure to detail the guarantee provided for our tubular hoop line. We guarantee the plastic component of the round hoops for a lifetime against manufacturer\\'s defects. We will exchange the plastic component at no cost provided the breakage is not due to abuse or using the parts in a manner not consistent with commercial embroidery practices. The Durkee Embroidery Hoop - 24cm x 24cm (9\"x9\") Square is great for embroidering children’s clothing and larger square designs that round hoops cannot accommodate.This Durkee Embroidery Hoop - 24cm x 24cm (9\"x9\") Square is constructed of high impact plastic making it virtually indestructible. The Durkee Embroidery Hoop - 24cm x 24cm (9\"x9\") Square has brass inserts for all screws, light texture on the hooping surface, directional guides, and extra height, which aids in holding thicker, harder to hold garments. The adjusting rings\\' thumb screws on the Durkee Embroidery Hoop - 24cm x 24cm (9\"x9\") Square are large and long to provide more torque and have a wider space for adjustments. The metal clips on the Durkee Embroidery Hoop - 24cm x 24cm (9\"x9\") Square are extra long, aiding in vibration reduction. The Durkee Embroidery Hoop - 24cm x 24cm (9\"x9\") Square metal clips are zinc plated to provide reduced wear, and they come complete with plastic screws and lock washers. Thumb screw is larger and longer than our competitors\\'. Wide space thumb screw adjusting. For optimum design registration, it is important to use a hoop that is closest to the size of the design. This will help eliminate puckering as well as wasting other supplies, such as backing or topping. Very efficient and the website worked out great for me! The hoop is wonderful. It allows for a larger size design than a 5x7 on a kid shirt. the only problem is that the threads strip after a while leaving the hoop unusable. I love the hoop though so I am about to order my 4th one. Will order again from your site! The price was right. I\\'ll be shopping here again. Love you guys... just you have so many items that sometimes it is hard to find what I am looking for. I purchased this hoop in 2011 and recently wanted to use it for an 8\" x 8\". The hoop is not large enough as the corners are too rounded and the hoop will not accommodate a square design. The wrong Durkee hoop was sent but Delores was wonderful and took care of the problem to my complete satisfaction. Will definitely order from All Brands again! I will order from Allbrands before any other internet business when possible because of the friendly, prompt response to all orders and the best prices on nearly all items. Very rarely do I find an item priced cheaper, but I still order from Allbrands because of the very prompt filling of orders. Shipment was fast, price was good. Love the hoop.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_prompts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88802ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8afe1466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I use C++ to analyze a painting's form, content, and context, while also evaluating its use\n",
      "of color, composition, and brushwork? Additionally, how can I consider the artist's intentions and\n",
      "the painting's cultural significance, as well as any influences from previous artists or movements?\n",
      "Finally, how can I assess the painting's impact on subsequent works of art and discuss its\n",
      "reflection of societal values and beliefs of its time period? Please provide specific C++ code\n",
      "examples and utilize art terminology in your analysis. [Attach Painting]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import textwrap\n",
    "\n",
    "wrapped_text = textwrap.fill(eval_prompts[idx], width=100)\n",
    "\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aee92c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Code Example]: Here is an example of using C++ to analyze a painting: ``` #include <iostream>\n",
      "using namespace std; int main() {     // Load image     Image img = loadImage(\"painting.jpg\");\n",
      "// Analyze form     int width = getWidth(img);     int height = getHeight(img);     cout << \"Width:\n",
      "\";     cout << width << endl;     cout << \"Height: \";     cout << height << endl;          //\n",
      "Evaluate content     string text = getWordCount(getText(img));     cout << \"Content: \" << text <<\n",
      "endl;          // Consider colors     Color c1 = getColorAt(0, 50);     Color c2 = getColorAt(width\n",
      "- 30, height - 40);     if (c1 != c2) {         cout << \"Colors do not match\" << endl;     } } ```\n",
      "This code loads an image file into memory, analyzes it for shape and texture, evaluates its content\n",
      "by counting words within the text area, considers the artwork's intended meaning through word count\n",
      "calculations, and then discusses potential impacts. This approach allows you to evaluate multiple\n",
      "aspects of the painting simultaneously without having access to all information about each element\n",
      "individually. The resulting output will be comprehensive insights that take into account various\n",
      "factors such as artistic intent, historical context, social implications, and more.  Here are some\n",
      "additional considerations when analyzing paintings:  - When considering the painter's\n",
      "intentionality, think about their style, technique, and overall aesthetic vision rather than just\n",
      "focusing solely on what they created physically. - Take note of other artworks related to this\n",
      "piece, including those made during different periods or with similar themes, which may offer\n",
      "valuable perspectives on the work itself. - Be aware of possible biases or assumptions inherent in\n",
      "certain interpretations or evaluations – always strive to remain objective and open-minded\n",
      "throughout the process. - Don't underestimate the importance of contextual understanding; even\n",
      "seemingly unrelated elements like lighting conditions, environmental settings, or personal emotions\n",
      "surrounding the creation can significantly influence our interpretation of a particular work\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wrapped_text = textwrap.fill(dpo_completions[idx], width=100)\n",
    "\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "660c94f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Attach C++ Code] To analyze a painting's form, content, and context, you can use C++ to create a\n",
      "program that takes in the painting's dimensions, content, and context, and then evaluates its form,\n",
      "content, and context using various C++ functions and algorithms. For example, you can use the\n",
      "`std::cout` and `std::cin` libraries to read the dimensions and content of the painting, and then\n",
      "use C++ functions such as `std::cout << \"Form: \" << form << std::endl;` and `std::cout << \"Content:\n",
      "\" << content << std::endl;` to display the form and content of the painting. To evaluate the use of\n",
      "color, composition, and brushwork, you can use C++ functions such as `std::cout << \"Color: \" <<\n",
      "color << std::endl;` and `std::cout << \"Composition: \" << composition << std::endl;` to display the\n",
      "color and composition of the painting. To consider the artist's intentions and the painting's\n",
      "cultural significance, you can use C++ functions such as `std::cout << \"Artist's Intention: \" <<\n",
      "artist << std::endl;` and `std::cout << \"Cultural Significance: \" << cultural significance <<\n",
      "std::endl;` to display the artist's intention and cultural significance of the painting. To assess\n",
      "the painting's impact on subsequent works of art, you can use C++ functions such as `std::cout <<\n",
      "\"Impact: \" << impact << std::endl;` to display the impact of the painting on subsequent works of\n",
      "art. To discuss the painting's reflection of societal values and beliefs of its time period, you can\n",
      "use C++ functions such as `std::cout << \"Reflection: \" << reflection << std::endl;` to display the\n",
      "reflection of societal values and beliefs of the painting in the time period. Here is an example of\n",
      "how you can use C++ to analyze a painting's form, content, and context, evaluate its use of color,\n",
      "composition, and brush\n"
     ]
    }
   ],
   "source": [
    "wrapped_text = textwrap.fill(sft_completions[idx], width=100)\n",
    "\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b47ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_reward = get_reward_score(eval_prompts[idx], sft_completions[idx])\n",
    "sft_reward = float(sft_reward.split(':')[-1])\n",
    "dpo_reward = get_reward_score(eval_prompts[idx], dpo_completions[idx])\n",
    "dpo_reward = float(dpo_reward.split(':')[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35145d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-26.75, -23.625)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_reward, dpo_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e62185b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ref_completions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mref_completions\u001b[49m[-\u001b[32m1\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'ref_completions' is not defined"
     ]
    }
   ],
   "source": [
    "ref_completions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ac6f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21642546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# from vllm import LLM, SamplingParams\n",
    "\n",
    "# # Sample prompts.\n",
    "# prompts = [\n",
    "#     \"Hello, my name is\",\n",
    "#     \"The president of the United States is\",\n",
    "#     \"The capital of France is\",\n",
    "#     \"The future of AI is\",\n",
    "# ]\n",
    "# # Create a sampling params object.\n",
    "# sampling_params = SamplingParams(repetition_penalty=1.5, max_tokens=590)\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     # Create an LLM.\n",
    "#     llm = LLM(model=\"facebook/opt-125m\")\n",
    "    \n",
    "#     llm = LLM(model=\"./finished_smoltalk\")\n",
    "#     # Generate texts from the prompts.\n",
    "#     # The output is a list of RequestOutput objects\n",
    "#     # that contain the prompt, generated text, and other information.\n",
    "#     outputs = llm.generate(prompts, sampling_params)\n",
    "#     # Print the outputs.\n",
    "#     print(\"\\nGenerated Outputs:\\n\" + \"-\" * 60)\n",
    "#     for output in outputs:\n",
    "#         prompt = output.prompt\n",
    "#         generated_text = output.outputs[0].text\n",
    "#         print(f\"Prompt:    {prompt!r}\")\n",
    "#         print(f\"Output:    {generated_text!r}\")\n",
    "#         print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ray\n",
    "# from ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor\n",
    "# import numpy as np\n",
    "\n",
    "# config = vLLMEngineProcessorConfig(\n",
    "#     model_source=\"unsloth/Llama-3.1-8B-Instruct\",\n",
    "#     engine_kwargs={\n",
    "#         \"enable_chunked_prefill\": True,\n",
    "#         \"max_num_batched_tokens\": 4096,\n",
    "#         \"max_model_len\": 16384,\n",
    "#     },\n",
    "#     concurrency=1,\n",
    "#     batch_size=32,\n",
    "# )\n",
    "# processor = build_llm_processor(\n",
    "#     config,\n",
    "#     preprocess=lambda row: dict(\n",
    "#         messages=[\n",
    "#             {\"role\": \"user\", \"content\": row[\"item\"]}\n",
    "#         ],\n",
    "#         sampling_params=dict(\n",
    "#             temperature=0.3,\n",
    "#             max_tokens=250,\n",
    "#         )\n",
    "#     ),\n",
    "#     postprocess=lambda row: dict(\n",
    "#         answer=row[\"generated_text\"],\n",
    "#         **row  # This will return all the original columns in the dataset.\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# ds = ray.data.from_items([\"Start of the haiku is: Complete this for me...\"])\n",
    "\n",
    "# ds = processor(ds)\n",
    "# ds.show(limit=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
