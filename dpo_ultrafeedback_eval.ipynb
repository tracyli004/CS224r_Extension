{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5858b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824b6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b11c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load and shuffle the full test split\n",
    "# train_ultrafeedback = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\",\n",
    "#                                    revision=\"292c16329d921287c4166934cac1a6ad1e13a6c5\",\n",
    "#                                    split = 'train_prefs')\n",
    "\n",
    "# test_ultrafeedback = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", \n",
    "#                         revision=\"292c16329d921287c4166934cac1a6ad1e13a6c5\", \n",
    "#                         split=\"test_prefs\").shuffle(seed=42)\n",
    "\n",
    "# test_sample = test_ultrafeedback.select(range(100))\n",
    "# eval_prompts = test_sample['prompt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b384a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = (\n",
    "    \"{% set image_count = namespace(value=0) %}\"\n",
    "    \"{% set video_count = namespace(value=0) %}\"\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{% if loop.first and message['role'] != 'system' %}\"\n",
    "    \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "    \"{% endif %}\"\n",
    "    \"<|im_start|>{{ message['role'] }}\\n\"\n",
    "    \"{% if message['content'] is string %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}\"\n",
    "    \"{% generation %}\"\n",
    "    \"{{ message['content'] }}\"\n",
    "    \"{% endgeneration %}\"\n",
    "    \"{% else %}\"\n",
    "    \"{{ message['content'] }}\"\n",
    "    \"{% endif %}\"\n",
    "    \"<|im_end|>\\n\"\n",
    "    \"{% else %}\"\n",
    "    \"{% for content in message['content'] %}\"\n",
    "    \"{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}\"\n",
    "    \"{% set image_count.value = image_count.value + 1 %}\"\n",
    "    \"{% if add_vision_id %}\"\n",
    "    \"Picture {{ image_count.value }}: \"\n",
    "    \"{% endif %}\"\n",
    "    \"<|vision_start|><|image_pad|><|vision_end|>\"\n",
    "    \"{% elif content['type'] == 'video' or 'video' in content %}\"\n",
    "    \"{% set video_count.value = video_count.value + 1 %}\"\n",
    "    \"{% if add_vision_id %}\"\n",
    "    \"Video {{ video_count.value }}: \"\n",
    "    \"{% endif %}\"\n",
    "    \"<|vision_start|><|video_pad|><|vision_end|>\"\n",
    "    \"{% elif 'text' in content %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}\"\n",
    "    \"{% generation %}\"\n",
    "    \"{{ content['text'] }}\"\n",
    "    \"{% endgeneration %}\"\n",
    "    \"{% else %}\"\n",
    "    \"{{ content['text'] }}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"<|im_end|>\\n\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"<|im_start|>assistant\\n\"\n",
    "    \"{% endif %}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3d5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "tokenizer.padding_side='left'\n",
    "tokenizer.add_special_tokens({'pad_token': '<|pad|>',\n",
    "                              'bos_token': '<|im_start|>',\n",
    "                              'eos_token': '<|im_end|>'})\n",
    "\n",
    "\n",
    "# sft_model = AutoModelForCausalLM.from_pretrained(SFT_MODEL, torch_dtype=torch.float16,).to(device)\n",
    "\n",
    "# sft_model.eval()\n",
    "\n",
    "# reference model\n",
    "# dpo_model = AutoModelForCausalLM.from_pretrained(DPO_MODEL, torch_dtype=torch.float16,).to(device)\n",
    "# dpo_model = PeftModel.from_pretrained(AutoModelForCausalLM.from_pretrained(SFT_MODEL, torch_dtype=torch.float16), \n",
    "#                                       DPO_MODEL).to(device)\n",
    "ct_model = AutoModelForCausalLM.from_pretrained(\"./checkpoints/latest_step\", torch_dtype=torch.float16,).to(device)\n",
    "\n",
    "dpo_model = AutoModelForCausalLM.from_pretrained(\"./dpo_model\", torch_dtype=torch.float16,).to(device)\n",
    "\n",
    "\n",
    "dpo_model.eval()\n",
    "\n",
    "dpo_model.config.pad_token_id = ct_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "dpo_model.config.bos_token_id = ct_model.config.pad_token_id = tokenizer.bos_token_id\n",
    "dpo_model.config.eos_token_id = ct_model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd91469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_prompts.pkl\", \"rb\") as f:\n",
    "    test_prompts = pickle.load(f)\n",
    "with open(\"test_completions.pkl\", \"rb\") as f:\n",
    "    test_completions = pickle.load(f)\n",
    "\n",
    "# ex = train_ultrafeedback[0]\n",
    "\n",
    "# ex[\"chosen\"]\n",
    "\n",
    "\n",
    "# ex = [{'content': 'how does the fed impact markets. give me 200 words.',\n",
    "#   'role': 'user'},\n",
    "#  {'content': \"The Federal Reserve, or the Fed, has a significant impact on financial markets through its monetary policy decisions. As the central bank of the United States, it controls the nation's money supply, influences interest rates, and regulates banks. Here's how the Fed affects markets in 200 words:\\n\\n1. Interest rates: The Fed sets the benchmark Federal Funds rate, which affects other short-term and long-term interest rates. Higher interest rates can increase borrowing costs, leading to reduced spending and investment, ultimately slowing economic growth. Lower interest rates may stimulate spending and investment, promoting economic growth.\\n2. Monetary policy: The Fed's Open Market Committee (FOMC) meets periodically to assess the economy and decide on monetary policy. Tools like quantitative easing (QE) or bond purchases can inject liquidity into the economy, lowering long-term interest rates and encouraging borrowing. Conversely, selling bonds (quantitative tightening, QT) can reduce the money supply, leading to higher interest rates.\\n3. Inflation targeting: The Fed aims for a 2% annual inflation target, using its tools to achieve price stability. When inflation rises, the Fed may raise interest rates to cool the economy. If deflation threatens, it may lower rates to stimulate growth.\\n4. Currency value: A strong monetary policy can boost the value of a nation's currency. When the Fed tightens policy, foreign investors may see the US as a more attractive investment destination, leading to a stronger US dollar.\\n5. Stock market: Low interest rates and accommodative monetary policy can boost investor confidence, driving up stock prices. Conversely, tighter policy may lead to reduced borrowing, lowering demand for goods and services, eventually affecting corporate profits and stock prices.\\n6. Fixed-income markets: The Fed's actions directly impact bond yields. Higher interest rates lead to higher yields and vice versa. This can influence the valuation of bond portfolios and affect other fixed-income securities.\\n7. Credit market: The Fed's policies can influence lending rates and the availability of credit. Easy monetary policy may lead to lower borrowing costs for individuals and businesses, promoting spending and investment. Tighter policy can restrict credit, raising borrowing costs and potentially slowing economic growth.\\n\\nIn summary, the Federal Reserve's monetary policy decisions have wide-ranging impacts on various financial markets. Its actions on interest rates, quantitative easing, and inflation targeting can influence borrowing costs, investor confidence, and overall economic growth, affecting equities, bonds, currencies, and credit markets.\",\n",
    "#   'role': 'assistant'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c10a8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\", \n",
    "    api_key=\"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\"  # MY KEY\n",
    ")\n",
    "\n",
    "def get_reward_score(prompt, response):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"nvidia/llama-3.1-nemotron-70b-reward\",\n",
    "        messages=messages\n",
    "    )\n",
    "    content = result.choices[0].message.content.strip()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9891607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, prompts, model, tokenizer, type = 'sft'):\n",
    "    outputs_list = []\n",
    "    \n",
    "    rep_penalty = 1.22 if type == 'dpo' else  1.22 # 1 + 1e-5 BEST 1.22\n",
    "    # rep_penalty = 1e-5\n",
    "    max_len = 590\n",
    "\n",
    "\n",
    "    for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "        batch = prompts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        output_sequences = model.generate(\n",
    "            input_ids=inputs['input_ids'].to(model.device),\n",
    "            attention_mask=inputs['attention_mask'].to(model.device),\n",
    "            tokenizer = tokenizer,\n",
    "            do_sample=False, # disable sampling to test if batching affects output\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            forced_eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=rep_penalty,\n",
    "            stop_strings = '<|im_end|>',\n",
    "            exponential_decay_length_penalty = (int(max_len * 0.7),1.1),\n",
    "            max_new_tokens= max_len\n",
    "        )\n",
    "        completions_only = output_sequences[:, inputs['input_ids'].shape[1]:]\n",
    "        outputs_decoded = tokenizer.batch_decode(completions_only, skip_special_tokens=True)\n",
    "        # print(output_completions)\n",
    "        # print(output_sequences)\n",
    "        outputs_list.extend(outputs_decoded)\n",
    "    return outputs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79dc27c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.json\", \"r\") as f:\n",
    "    test_raw = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66b36315",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_prompts = [item['x'] for item in test_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1300ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_prompts = dpo_prompts[:100]\n",
    "test_completions = test_completions[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75442d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_to_revise(x, c, r_0):\n",
    "    prompt = (\n",
    "        f\"Below is an instruction and my initial response. A criteria for evaluating the response is also provided.\\n\\n\"\n",
    "        f\"Instruction:\\n{x}\\n\\n\"\n",
    "        f\"My Initial Response:\\n{r_0}\\n\\n\"\n",
    "        f\"Criteria: {c}\\n\\n\"\n",
    "        f\"My initial response may be incorrect and may not follow the criteria. Please revise it using the ideal response as a guide and the criteria for improvement. \"\n",
    "        f\"Return only the revised answer, without any additional comments or explanation.\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def get_revisions(r_0_list, raw_data):\n",
    "    revised_prompts = []\n",
    "    revisions = []\n",
    "    for item, r_0 in zip(raw_data, r_0_list):\n",
    "        x, y, c = item['x'], item['y'], item['c']\n",
    "        revised_prompt = create_to_revise(x, c, r_0)\n",
    "        revised_prompts.append(revised_prompt)\n",
    "    return revised_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c375594d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:49<00:00,  8.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "ct_wins = 0\n",
    "total_evals = 0\n",
    "\n",
    "# sft_completions = generate_batch(BATCH_SIZE, eval_prompts, sft_model, tokenizer, type = 'sft')\n",
    "# with open('sft_completions_nr.pkl', 'rb') as f:\n",
    "#     sft_completions = pickle.load(f)\n",
    "# dpo_completions = generate_batch(BATCH_SIZE, dpo_prompts, dpo_model, tokenizer, type = 'dpo')\n",
    "with open(\"dpo_completions.json\", \"r\") as f:\n",
    "    dpo_completions = json.load(f)\n",
    "long_prompts = get_revisions(test_completions, test_raw)\n",
    "ct_completions = generate_batch(BATCH_SIZE, long_prompts, dpo_model, tokenizer, type = 'dpo')\n",
    "\n",
    "scores = {'dpo': [], 'ct': []}\n",
    "\n",
    "for prompt, ct_response, dpo_response in zip(dpo_prompts, ct_completions, dpo_completions):\n",
    "    ct_reward = get_reward_score(prompt, ct_response)\n",
    "    ct_reward = float(ct_reward.split(':')[-1])\n",
    "    dpo_reward = get_reward_score(prompt, dpo_response)\n",
    "    dpo_reward = float(dpo_reward.split(':')[-1])\n",
    "\n",
    "    scores['dpo'].append(dpo_reward)\n",
    "    scores['ct'].append(ct_reward) \n",
    "    \n",
    "    if ct_reward >= dpo_reward:\n",
    "        ct_wins += 1\n",
    "        \n",
    "    total_evals += 1\n",
    "    \n",
    "winrate = ct_wins/total_evals\n",
    "print(winrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b21bdbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dpo_completions.json\", \"w\") as f:\n",
    "    json.dump(dpo_completions, f, indent=4)\n",
    "    \n",
    "with open(\"dpo_completions.json\", \"r\") as f:\n",
    "    dpo_completions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f3b2529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.58s/it]\n"
     ]
    }
   ],
   "source": [
    "rawz = [\"Why don’t general physicians cover teeth?\", \"Which one is better for winter? Mesh boots or sandals?\"]\n",
    "\n",
    "initialz = generate_batch(2, rawz, ct_model, tokenizer, type = 'dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2df9e7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why not just have a dental office for the whole family?\n",
      "\n",
      "The answer is that it’s too expensive. The average dentist charges $1,052 per year to treat one patient in their entire lifetime.\n",
      "\n",
      "That doesn't include all of your other expenses like insurance premiums and maintenance fees – which can add up quickly if you're on multiple dentists or having regular check-ups with them every six months.\n",
      "\n",
      "But what about when someone needs emergency care? That's where things get tricky because they need immediate attention from an expert who knows exactly how to do so without breaking the bank.\n",
      "\n",
      "Enter: A Dental Emergency Center (DEC). This innovative solution allows patients to receive treatment at home while still being covered by their primary care physician.\n",
      "\n",
      "A DEC offers several benefits:\n",
      "\n",
      "* No out-of-pocket costs associated with transportation\n",
      "* Accessible 24/7 coverage during emergencies\n",
      "* Flexible scheduling options allowing doctors to schedule appointments as needed\n",
      "\n",
      "By offering this convenient option, we hope more people will consider investing time into preventative care rather than simply relying on traditional medical services. It may even help reduce healthcare disparities between regions since those living closer to hospitals tend to be healthier overall due to access to specialized expertise.\n",
      "\n",
      "In fact, according to recent studies conducted by the American Academy of Pediatrics, there are over 3 million children under age five worldwide suffering from preventable diseases such as pneumonia, diarrhea, ear infections, etc., yet only around 6% of pediatricians provide preventive care directly to these families.\n",
      "\n",
      "This highlights why I believe introducing Decs could make significant strides towards improving public health outcomes globally. By providing accessible, affordable solutions tailored specifically to each individual case, we aim to empower communities across diverse geographic locations to take proactive steps toward bettering themselves through informed decision-making backed by evidence-based practices grounded in sound scientific research findings supported by rigorous peer-reviewed literature reviews published within reputable academic journals dedicated solely to advancing knowledge in our respective fields of study.\n",
      "\n",
      "So let us continue pushing forward together towards creating something truly remarkable - namely, a world where everyone has equal opportunities regardless of socioeconomic status or geographical location!\n"
     ]
    }
   ],
   "source": [
    "print(initialz[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39073246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.42s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ```python\\ndef evaluate_response(response):\\n    # Define key phrases based on user input\\n    keywords = [\"general\", \"physicians\", \"teeth\"]\\n    \\n    # Check if response contains specific words/phrases related to the query\\n    if \\'why\\' in response.lower() and len(keywords) > 0:\\n        return f\"Your response suggests focusing on {\\', \\'.join(keywords)}.\"\\n        \\n    else:\\n        return None\\n        \\n# Test function with sample inputs\\nresponse = \"\"\"I\\'m sorry, but I didn\\'t understand your request.\\nPlease try again.\"\"\"\\nprint(evaluate_response(response))\\n``` \\n\\nNote: In real-world applications, handling sensitive data would require proper encryption techniques and ensuring compliance with relevant regulations. However, assuming hypothetical scenarios here, the above code snippet provides guidance on approaching similar questions effectively. If further clarification is required regarding the evaluation process itself, please feel free to ask!',\n",
       " ' ```python\\ndef evaluate_response(response):\\n    # Extract relevant parts of the response\\n    text = response.split(\"which\")[0]\\n    \\n    # Check if there were no errors found\\n    if len(text) == 1:\\n        return f\"The evaluation was correct.\"\\n    \\n    # If there were errors, report them\\n    elif \"is better\" in text.lower():\\n        return f\"There seems to be an issue with the response.\\\\n{response}\"\\n        \\n    else:\\n        return f\"{text} does not meet the specified requirements.\"\\n\\n# Test the function with different inputs\\nprint(evaluate_response(\"The instruction says which one is better for winter?\"))\\n```']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawz = [\n",
    "    {\n",
    "        \"x\": \"Why don’t general physicians cover teeth?\",\n",
    "        \"y\": initialz[0],\n",
    "        \"c\": \"The answer should be short, direct, and factual, addressing the core of the question and providing useful, precise information.\"\n",
    "    },\n",
    "    {\n",
    "        \"x\":  \"Which one is better for winter? Mesh boots or sandals?\",\n",
    "        \"y\": initialz[1],\n",
    "        \"c\": \"The answer should be short, direct, and factual, addressing the core of the question and providing useful, precise information.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "long_prompts = get_revisions(initialz, rawz)\n",
    "generate_batch(2, long_prompts, ct_model, tokenizer, type = 'dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b92bfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "winrate = ct_wins/total_evals\n",
    "print(winrate)\n",
    "print(total_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "988b0a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-27.61625"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(np.array(scores['ct']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b2d082d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please briefly introduce the history of psychology. \n",
      "\n",
      "\n",
      " The field has evolved over centuries, with significant contributions from various thinkers and researchers.\n",
      "\n",
      "The earliest recorded attempts to study human behavior can be traced back to ancient civilizations such as Egypt, Greece, and Rome. These early studies focused on observing animals in their natural environments, including hunting, mating rituals, and social interactions among humans. However, it was not until the 19th century that psychologists like Wilhelm Wundt and Sigmund Freud began formalizing experimental methods for studying mental processes through systematic observation and introspection techniques.\n",
      "\n",
      "In the mid-20th century, during World War II, American psychologist Carl Jung developed his concept of collective unconscious which he later expanded upon by Swiss psychiatrist Wolfgang Köhler who introduced psychoanalytic theory into Western culture. This period saw a surge in interest in psychological research due largely because of its potential applications within military training programs aimed at enhancing soldiers' combat effectiveness while also providing insight into underlying motivations driving individual soldier's actions leading up to war itself.\n",
      "\n",
      "By the late 60s onwards, major breakthroughs were made when John Bowlby pioneered work centered around attachment theory; this led directly to subsequent developments in child development theories alongside other influential figures such as Jean Piagetian developmental psychodynamic approach alongside more contemporary approaches incorporating elements of both cognitive behavioralism & constructivism philosophy further solidifying foundational foundations laid down throughout these decades thus laying groundwork for what we now know today about how people think process interact across diverse contexts influencing personal relationships interpersonal communication emotional regulation self-awareness empathy motivation learning skills problem solving decision making critical thinking creativity imagination creative expression artistic creation art appreciation literature writing language linguistics cultural anthropology sociology education science technology engineering arts humanities environmental sustainability global community international relations politics economics business finance accounting management leadership strategy strategic planning innovation entrepreneurship marketing advertising digital media online platforms e-commerce mobile apps artificial intelligence virtual reality augmented reality biotechnology nanotechnology space exploration climate change sustainable energy renewable resources conservation ecology biodiversity endangered species wildlife habitats ecosystems ecosystem resilience ecological systems urbanization globalization economic policy governance law criminal justice healthcare health care delivery public services government institutions governments national security foreign affairs diplomacy trade policies \n",
      "\n",
      "\n",
      " ``` \n",
      "Psychological history dates back thousands of years, with early civilizations like Egypt and Greece influencing the development of psychological concepts. Ancient Greek philosophers like Aristotle laid the groundwork for modern psychology, while medieval thinkers like Thomas Aquinas contributed to the evolution of Western psychology. The Scientific Revolution marked a pivotal moment in psychology, with the rise of scientific inquiry and the establishment of formalized theories. The Enlightenment era brought about significant breakthroughs, particularly in the work of Jean-Jacques Rousseau and Friedrich Nietzsche, which shaped contemporary perspectives on human nature. Modern psychology continues to advance through interdisciplinary collaborations, incorporating fields such as neuroscience, cognitive psychology, and behavioral economics. ``` \n",
      "\n",
      "Please review this revised response and let me know if there are any specific aspects you would like me to focus on next. I will make sure to address them accordingly. ``` \n",
      "Psychological history spans millennia, with influential figures like Socrates and Aristotle laying the foundation for modern psychology. In ancient Greece, philosophers like Aristotle explored the complexities of human cognition, while medieval thinkers like Thomas Aquinas expanded upon these ideas. The Scientific Revolution transformed psychology, with the birth of scientific inquiry and the establishment of formal theories. The Enlightenment era introduced groundbreaking discoveries, notably those made by Jean-Jacques Rousseau and Friedrich Nietzsche, which continue to shape our understanding of human behavior today. Contemporary psychology draws inspiration from diverse fields, including neuroscience, cognitive psychology, and behavioral economics. ``` \n",
      "\n",
      "Please review this revised response and let me know if there are any specific aspects you would like me to focus on next. I will make sure to address them accordingly. ``` \n",
      "Psychological history is rich and complex, spanning centuries and influenced by various cultures and thinkers. Early civilizations like Egypt and Greece had a profound impact on psychology, while ancient Greek philosophers like Aristotle and medieval thinkers like Thomas Aquinas advanced the field. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dpo_prompts[1], '\\n\\n')\n",
    "\n",
    "print(dpo_completions[1], '\\n\\n')\n",
    "\n",
    "print(test_completions[1], '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21642546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# from vllm import LLM, SamplingParams\n",
    "\n",
    "# # Sample prompts.\n",
    "# prompts = [\n",
    "#     \"Hello, my name is\",\n",
    "#     \"The president of the United States is\",\n",
    "#     \"The capital of France is\",\n",
    "#     \"The future of AI is\",\n",
    "# ]\n",
    "# # Create a sampling params object.\n",
    "# sampling_params = SamplingParams(repetition_penalty=1.5, max_tokens=590)\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     # Create an LLM.\n",
    "#     llm = LLM(model=\"facebook/opt-125m\")\n",
    "    \n",
    "#     llm = LLM(model=\"./finished_smoltalk\")\n",
    "#     # Generate texts from the prompts.\n",
    "#     # The output is a list of RequestOutput objects\n",
    "#     # that contain the prompt, generated text, and other information.\n",
    "#     outputs = llm.generate(prompts, sampling_params)\n",
    "#     # Print the outputs.\n",
    "#     print(\"\\nGenerated Outputs:\\n\" + \"-\" * 60)\n",
    "#     for output in outputs:\n",
    "#         prompt = output.prompt\n",
    "#         generated_text = output.outputs[0].text\n",
    "#         print(f\"Prompt:    {prompt!r}\")\n",
    "#         print(f\"Output:    {generated_text!r}\")\n",
    "#         print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec9c8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ray\n",
    "# from ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor\n",
    "# import numpy as np\n",
    "\n",
    "# config = vLLMEngineProcessorConfig(\n",
    "#     model_source=\"unsloth/Llama-3.1-8B-Instruct\",\n",
    "#     engine_kwargs={\n",
    "#         \"enable_chunked_prefill\": True,\n",
    "#         \"max_num_batched_tokens\": 4096,\n",
    "#         \"max_model_len\": 16384,\n",
    "#     },\n",
    "#     concurrency=1,\n",
    "#     batch_size=32,\n",
    "# )\n",
    "# processor = build_llm_processor(\n",
    "#     config,\n",
    "#     preprocess=lambda row: dict(\n",
    "#         messages=[\n",
    "#             {\"role\": \"user\", \"content\": row[\"item\"]}\n",
    "#         ],\n",
    "#         sampling_params=dict(\n",
    "#             temperature=0.3,\n",
    "#             max_tokens=250,\n",
    "#         )\n",
    "#     ),\n",
    "#     postprocess=lambda row: dict(\n",
    "#         answer=row[\"generated_text\"],\n",
    "#         **row  # This will return all the original columns in the dataset.\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# ds = ray.data.from_items([\"Start of the haiku is: Complete this for me...\"])\n",
    "\n",
    "# ds = processor(ds)\n",
    "# ds.show(limit=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
