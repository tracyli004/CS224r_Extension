{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721bffa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "# from peft import PeftModel\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "# import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from vllm import LLM, SamplingParams\n",
    "# import openai\n",
    "import time\n",
    "import tensorboard\n",
    "import pickle\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_printoptions(threshold=float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f799aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade \"jinja2>=3.1.0\"\n",
    "# %pip install datasets tensorboard openai tqdm\n",
    "# %pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ed2813a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 03:15:58.008901: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749525358.018499   22450 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749525358.023666   22450 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# load tokenizer and model\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\", padding_side='right')\n",
    "student = AutoModelForCausalLM.from_pretrained(\"./dpo_model\", torch_dtype=torch.float32)\n",
    "\n",
    "# add special tokens\n",
    "student_tokenizer.add_special_tokens({\n",
    "    'pad_token': '<|pad|>',\n",
    "    'bos_token': '<|im_start|>',\n",
    "    'eos_token': '<|im_end|>',\n",
    "})\n",
    "\n",
    "# resize model embeddings to include new tokens\n",
    "student.resize_token_embeddings(len(student_tokenizer))\n",
    "# set token ids in config\n",
    "student.config.pad_token_id = student_tokenizer.pad_token_id\n",
    "student.config.bos_token_id = student_tokenizer.bos_token_id\n",
    "student.config.eos_token_id = student_tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e36fdb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = (\n",
    "    \"{% set image_count = namespace(value=0) %}\"\n",
    "    \"{% set video_count = namespace(value=0) %}\"\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{% if loop.first and message['role'] != 'system' %}\"\n",
    "    \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "    \"{% endif %}\"\n",
    "    \"<|im_start|>{{ message['role'] }}\\n\"\n",
    "    \"{% if message['content'] is string %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}\"\n",
    "    \"{% generation %}\"\n",
    "    \"{{ message['content'] }}\"\n",
    "    \"{% endgeneration %}\"\n",
    "    \"{% else %}\"\n",
    "    \"{{ message['content'] }}\"\n",
    "    \"{% endif %}\"\n",
    "    \"<|im_end|>\\n\"\n",
    "    \"{% else %}\"\n",
    "    \"{% for content in message['content'] %}\"\n",
    "    \"{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}\"\n",
    "    \"{% set image_count.value = image_count.value + 1 %}\"\n",
    "    \"{% if add_vision_id %}\"\n",
    "    \"Picture {{ image_count.value }}: \"\n",
    "    \"{% endif %}\"\n",
    "    \"<|vision_start|><|image_pad|><|vision_end|>\"\n",
    "    \"{% elif content['type'] == 'video' or 'video' in content %}\"\n",
    "    \"{% set video_count.value = video_count.value + 1 %}\"\n",
    "    \"{% if add_vision_id %}\"\n",
    "    \"Video {{ video_count.value }}: \"\n",
    "    \"{% endif %}\"\n",
    "    \"<|vision_start|><|video_pad|><|vision_end|>\"\n",
    "    \"{% elif 'text' in content %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}\"\n",
    "    \"{% generation %}\"\n",
    "    \"{{ content['text'] }}\"\n",
    "    \"{% endgeneration %}\"\n",
    "    \"{% else %}\"\n",
    "    \"{{ content['text'] }}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"<|im_end|>\\n\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"<|im_start|>assistant\\n\"\n",
    "    \"{% endif %}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c0f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "MAX_TOKENS = 512\n",
    "\n",
    "MAX_INPUT_TOKENS = 512\n",
    "ACCUM_STEPS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e969cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length = MAX_INPUT_TOKENS):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        messages = self.dataset[idx]\n",
    "\n",
    "        new_messages = []\n",
    "        for m in messages:\n",
    "            if not new_messages and m[\"role\"] == \"user\":\n",
    "                new_messages.append(m)\n",
    "            elif new_messages and m[\"role\"] == \"assistant\":\n",
    "                new_messages.append(m)\n",
    "                break\n",
    "        if len(new_messages) != 2:\n",
    "            return self._empty_item()\n",
    "        \n",
    "        try:\n",
    "            tokenized = self.tokenizer.apply_chat_template(\n",
    "                new_messages,\n",
    "                tokenize = True,\n",
    "                max_length = self.max_length,\n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                return_dict = True,\n",
    "                return_assistant_tokens_mask=True,\n",
    "                add_generation_prompt = False,\n",
    "                chat_template = chat_template,\n",
    "                return_tensors = 'pt'\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(idx, e)\n",
    "            return self._empty_item()\n",
    "\n",
    "        input_ids = tokenized['input_ids']\n",
    "        assistant_masks = tokenized['assistant_masks']\n",
    "        if assistant_masks.sum() == 0:\n",
    "            return self._empty_item()\n",
    "\n",
    "        mod_assistant_mask = assistant_masks.clone()\n",
    "        matches = (input_ids == self.tokenizer.convert_tokens_to_ids(\"<|im_end|>\"))\n",
    "        indices = torch.nonzero(matches)\n",
    "        mod_assistant_mask[tuple(indices[-1])] = 1 # inlcude end speaking token in assistant to include in lables\n",
    "\n",
    "        attention_mask = tokenized['attention_mask']\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[mod_assistant_mask == 0] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids.squeeze(0),\n",
    "            'attention_mask': attention_mask.squeeze(0),\n",
    "            'labels': labels.squeeze(0)\n",
    "        }\n",
    "    \n",
    "    def _empty_item(self):\n",
    "        return {\n",
    "            'input_ids': torch.zeros(self.max_length, dtype=torch.int32),\n",
    "            'attention_mask': torch.zeros(self.max_length, dtype=torch.int32),\n",
    "            'labels': torch.full((self.max_length,), -100, dtype=torch.int32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94c40e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_generate_batch(batch_size, prompts, model, tokenizer):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\", padding_side = 'left')\n",
    "    outputs_list = []\n",
    "\n",
    "    for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "        batch = prompts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        output_sequences = model.generate(\n",
    "            input_ids=inputs['input_ids'].to(model.device),\n",
    "            attention_mask=inputs['attention_mask'].to(model.device),\n",
    "            tokenizer = tokenizer,\n",
    "            do_sample=False, # disable sampling to test if batching affects output\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            forced_eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.12,\n",
    "            stop_strings = '<|im_end|>',\n",
    "            exponential_decay_length_penalty = (int(MAX_TOKENS * 0.7),1.1),\n",
    "            max_new_tokens= MAX_TOKENS\n",
    "        )\n",
    "        completions_only = output_sequences[:, inputs['input_ids'].shape[1]:]\n",
    "        outputs_decoded = tokenizer.batch_decode(completions_only, skip_special_tokens=True)\n",
    "        # print(output_completions)\n",
    "        # print(output_sequences)\n",
    "        outputs_list.extend(outputs_decoded)\n",
    "    return outputs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1250d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_prompt(prompt, max_input_tokens=600):\n",
    "    tokens = student_tokenizer(prompt)[\"input_ids\"]\n",
    "    if len(tokens) > max_input_tokens:\n",
    "        tokens = tokens[:max_input_tokens]\n",
    "        return student_tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04274f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "def teacher_generate_batch(prompts, model=\"o4-mini-2025-04-16\", system_prompt=\"You are a helpful assistant.\"):\n",
    "    # Step 1: Write prompts to JSONL\n",
    "    input_path = \"batch_input.jsonl\"\n",
    "    with open(input_path, \"w\") as f:\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            print(\"Idx \", i)\n",
    "            trunc = truncate_prompt(prompt)\n",
    "            # time.sleep(3)\n",
    "            # print(prompt)\n",
    "            item = {\n",
    "                \"custom_id\": f\"request-{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": trunc}\n",
    "                    ],\n",
    "                    \"max_tokens\": MAX_TOKENS,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"stop\": [\"<|im_end|>\"],\n",
    "                    \"frequency_penalty\": 1.5,\n",
    "                    \"presence_penalty\": 0.0\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "    print(\"Wrote in prompts to json.\")\n",
    "    # Step 2: Upload the file using OpenAI client\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        upload = client.files.create(file=f, purpose=\"batch\")\n",
    "    file_id = upload.id\n",
    "\n",
    "    # Step 3: Submit batch\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\"\n",
    "    )\n",
    "    batch_id = batch.id\n",
    "    print(\"Batch submitted. ID:\", batch_id)\n",
    "\n",
    "    # Step 4: Poll until complete\n",
    "    print(\"Waiting for batch to complete...\")\n",
    "    while True:\n",
    "        batch_status = client.batches.retrieve(batch_id)\n",
    "        status = batch_status.status\n",
    "        # print(f\"Current status: {status}\")\n",
    "        if status in [\"completed\", \"failed\", \"cancelled\", \"expired\"]:\n",
    "            break\n",
    "        time.sleep(15)\n",
    "\n",
    "    if status != \"completed\":\n",
    "        raise RuntimeError(f\"Batch failed or didn't complete: {status}\")\n",
    "\n",
    "    # Step 5: Download result file\n",
    "    output_file_id = batch_status.output_file_id\n",
    "    output_response = client.files.content(output_file_id)\n",
    "    output_data = output_response.text\n",
    "\n",
    "    # Step 6: Parse results into dictionary\n",
    "    responses = {}\n",
    "    for line in output_data.splitlines():\n",
    "        obj = json.loads(line)\n",
    "        custom_id = obj[\"custom_id\"]\n",
    "        content = obj[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        responses[custom_id] = content\n",
    "\n",
    "    # Step 7: Return outputs in order\n",
    "    return [responses[f\"request-{i}\"] for i in range(len(prompts))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d76e62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_raw):\n",
    "    formatted_data = [[{\"content\": str(item[\"x\"]), \"role\": \"user\"},\n",
    "                       {\"content\": str(item[\"y\"]), \"role\": \"assistant\"}] for item in train_raw ]\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "088524b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_list(prompt, completion):\n",
    "    data = [[\n",
    "        {'content': p, 'role': 'user'},\n",
    "        {'content': c, 'role': 'assistant'}\n",
    "    ] for p, c in zip(prompt, completion)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "072f4ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_to_revise(x, c, r_0):\n",
    "    prompt = (\n",
    "        f\"Below is an instruction and my initial response. A criteria for evaluating the response is also provided.\\n\\n\"\n",
    "        f\"Instruction:\\n{x}\\n\\n\"\n",
    "        f\"My Initial Response:\\n{r_0}\\n\\n\"\n",
    "        f\"Criteria: {c}\\n\\n\"\n",
    "        f\"My initial response may be incorrect and may not follow the criteria. Please revise it using the ideal response as a guide and the criteria for improvement. \"\n",
    "        f\"Return only the revised answer, without any additional comments or explanation.\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55cc206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_revisions(r_0_list, raw_data):\n",
    "    \n",
    "    # student.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     r_0_list = student_generate_batch(batch_size=BATCH_SIZE, prompts=prompts, model=student, tokenizer=student_tokenizer)\n",
    "\n",
    "    revised_prompts = []\n",
    "    revisions = []\n",
    "    for item, r_0 in zip(raw_data, r_0_list):\n",
    "        x, y, c = item['x'], item['y'], item['c']\n",
    "        revised_prompt = create_to_revise(x, c, r_0)\n",
    "        revised_prompts.append(revised_prompt)\n",
    "        # revisions.append(y)\n",
    "        \n",
    "    # print(len(revised_prompts))\n",
    "\n",
    "    # # 2. teacher revises (no grad)\n",
    "    # for i in range(0, len(revised_prompts), 500):\n",
    "    #     print(\"\\n\\n batch: \", i)\n",
    "    #     batched_prompts = revised_prompts[i:i+500]\n",
    "    #     revisions.extend(teacher_generate_batch(batched_prompts, model=\"gpt-4o-mini\", system_prompt=\"You are an expert writer.\"))\n",
    "    #     time.sleep(180)\n",
    "    # # revisions = teacher_generate_batch(revised_prompts, model=\"gpt-4o-mini\", system_prompt=\"You are an expert writer.\")\n",
    "\n",
    "    # print(revised_prompts[332], revisions[332])\n",
    "    return revised_prompts #, revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07ef518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train(model, train_loader, optimizer, writer, epoch):\n",
    "    accumulation_steps=8\n",
    "\n",
    "    total_loss = 0\n",
    "    batch_times = []\n",
    "    progress = tqdm(train_loader, desc=f\"Training Epoch {epoch}\", leave=True)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, batch in enumerate(progress):\n",
    "        start = time.time()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        if torch.all(labels == -100) or torch.all(input_ids == 0):\n",
    "            print(f\"⏭️  Skipping empty batch {i}\")\n",
    "            continue\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        # loss = outputs.loss  # normalize loss for accumulation\n",
    "        loss = outputs.loss / accumulation_steps\n",
    "                \n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "        # assert not math.isnan(loss.item()), f'Loss: {loss}, \\nOutputs: {outputs}'\n",
    "        loss.backward()\n",
    "    \n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "            torch.cuda.synchronize()\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize()\n",
    "            optimizer.zero_grad()\n",
    "        # torch.cuda.synchronize()\n",
    "        # optimizer.step()\n",
    "        # torch.cuda.synchronize()\n",
    "        # optimizer.zero_grad()\n",
    "        total_loss += loss.item() * accumulation_steps  \n",
    "        # total_loss += loss.item()  \n",
    "        avg_loss = total_loss / (i + 1)\n",
    "        writer.add_scalar(\"Loss/train\", avg_loss, epoch * len(train_loader) + i)\n",
    "\n",
    "\n",
    "        batch_time = time.time() - start\n",
    "        batch_times.append(batch_time)\n",
    "        avg_time = sum(batch_times) / len(batch_times)\n",
    "        eta = avg_time * (len(train_loader) - (i + 1))\n",
    "        eta_hr, remainder = divmod(int(eta), 3600)\n",
    "        eta_min, eta_sec = divmod(remainder, 60)\n",
    "\n",
    "        # progress.set_postfix(loss=[loss.item(), avg_loss], eta=f\"{eta_hr}h {eta_min}m {eta_sec}s\")\n",
    "        progress.set_postfix(loss=[loss.item() * accumulation_steps, avg_loss], eta=f\"{eta_hr}h {eta_min}m {eta_sec}s\")\n",
    "\n",
    "\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            model.save_pretrained('./latest_model')\n",
    "            with open('latest_opt.pkl', 'wb') as f:\n",
    "                pickle.dump(optimizer, f)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    return float(avg_loss)\n",
    "\n",
    "\n",
    "def test(model, test_loader, writer, epoch, return_generations=False):\n",
    "    total_loss = 0\n",
    "    batch_times = []\n",
    "    progress = tqdm(test_loader, desc=f\"Testing Epoch {epoch}\", leave=True)\n",
    "\n",
    "    for i, batch in enumerate(progress):\n",
    "        start = time.time()\n",
    "        \n",
    "        input_ids = batch['input_ids'].long().to(device)\n",
    "        attention_mask = batch['attention_mask'].long().to(device)\n",
    "        labels = batch['labels'].long().to(device)\n",
    "        \n",
    "        if torch.all(labels == -100) or torch.all(input_ids == 0):\n",
    "            print(f\"⏭️  Skipping empty batch {i}\")\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        if math.isnan(loss):\n",
    "            print(\"NAN loss\")\n",
    "            continue\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        avg_loss = float(total_loss) / (i + 1)\n",
    "\n",
    "\n",
    "        batch_time = time.time() - start\n",
    "        batch_times.append(batch_time)\n",
    "        avg_time = sum(batch_times) / len(batch_times)\n",
    "        eta = avg_time * (len(test_loader) - (i + 1))\n",
    "        eta_hr, remainder = divmod(int(eta), 3600)\n",
    "        eta_min, eta_sec = divmod(remainder, 60)\n",
    "\n",
    "        progress.set_postfix(loss=[loss.item(), avg_loss], eta=f\"{eta_hr}h {eta_min}m {eta_sec}s\")\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            model.save_pretrained('./checkpoints/latest_step')\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    writer.add_scalar(\"Loss/val\", avg_loss, epoch)\n",
    "    return float(avg_loss)\n",
    "\n",
    "\n",
    "def fine_tune(model, train_loader, test_loader, optimizer, num_epochs):\n",
    "    writer = SummaryWriter()  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, optimizer, writer, epoch)\n",
    "        # model.save_pretrained('./citing_model')\n",
    "        val_loss = test(model, test_loader, writer, epoch)\n",
    "        print(f'Epoch: {epoch}. Train Loss: {train_loss}. Val Loss: {val_loss}.')\n",
    "        \n",
    "    # model.save_pretrained('./citing_model')\n",
    "\n",
    "    writer.close()\n",
    "    return train_loss, val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11713d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CITING(train_raw, r_0_list, train_completions, test_prompts, test_completions, student, tokenizer, num_epochs):\n",
    "    optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, student.parameters()), lr=1e-7)\n",
    "    \n",
    "    train_prompts = get_revisions(r_0_list, train_raw)\n",
    "    \n",
    "    train_set_loaded = load_data_from_list(train_prompts, train_completions)\n",
    "    train_set_CL = CLDataset(train_set_loaded, tokenizer)\n",
    "    train_loader = DataLoader(train_set_CL, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "    test_set_loaded = load_data_from_list(test_prompts, test_completions)\n",
    "    test_set_CL = CLDataset(test_set_loaded, tokenizer)\n",
    "    test_loader = DataLoader(test_set_CL, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    \n",
    "    train_loss, val_loss = fine_tune(\n",
    "        model = student,\n",
    "        train_loader = train_loader,\n",
    "        test_loader = test_loader,\n",
    "        optimizer = optim,\n",
    "        num_epochs = num_epochs\n",
    "    )\n",
    "\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8bcab36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student.to(device)\n",
    "print(student.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdae23df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_short.json\", \"r\") as f:\n",
    "    train_short = json.load(f)\n",
    "    train_short = train_short[:2000]\n",
    "with open(\"train_med.json\", \"r\") as f:\n",
    "    train_med = json.load(f)\n",
    "    train_med = train_med[:2000]\n",
    "with open(\"train_long.json\", \"r\") as f:\n",
    "    train_long = json.load(f)\n",
    "    train_long = train_long[:2000]\n",
    "with open(\"test.json\", \"r\") as f:\n",
    "    test_raw = json.load(f)\n",
    "    \n",
    "train_short_dataloaded = load_data(train_short)\n",
    "train_med_dataloaded = load_data(train_med)\n",
    "train_long_dataloaded = load_data(train_long)\n",
    "test_dataloaded = load_data(test_raw)\n",
    "\n",
    "# UR_train_short_prompts = [item[0][\"content\"] for item in train_short_dataloaded]\n",
    "# UR_train_med_prompts = [item[0][\"content\"] for item in train_med_dataloaded]\n",
    "# UR_train_long_prompts = [item[0][\"content\"] for item in train_long_dataloaded]\n",
    "# UR_test_prompts = [item[0][\"content\"] for item in test_dataloaded]\n",
    "\n",
    "with open(\"short_initial_responses.json\", \"r\") as f:\n",
    "    short_r_0_list = json.load(f)\n",
    "    short_r_0_list = short_r_0_list[:2000]\n",
    "with open(\"med_initial_responses.json\", \"r\") as f:\n",
    "    med_r_0_list = json.load(f)\n",
    "    med_r_0_list = med_r_0_list[:2000]\n",
    "with open(\"long_initial_responses.json\", \"r\") as f:\n",
    "    long_r_0_list = json.load(f)\n",
    "    long_r_0_list = long_r_0_list[:2000]\n",
    "# with open(\"test_initial_responses.json\", \"r\") as f:\n",
    "#     test_r_0_list = json.load(f)\n",
    "#     test_r_0_list = test_r_0_list[:1000]\n",
    "\n",
    "\n",
    "with open(\"short_revisions.json\", \"r\") as f:\n",
    "    short_completions = json.load(f)\n",
    "with open(\"med_revisions.json\", \"r\") as f:\n",
    "    med_completions = json.load(f)\n",
    "with open(\"long_revisions.json\", \"r\") as f:\n",
    "    long_completions = json.load(f)\n",
    "    \n",
    "    \n",
    "with open(\"short_revisions_2.json\", \"r\") as f:\n",
    "    short_completions_2 = json.load(f)\n",
    "with open(\"med_revisions_2.json\", \"r\") as f:\n",
    "    med_completions_2 = json.load(f)\n",
    "with open(\"long_revisions_2.json\", \"r\") as f:\n",
    "    long_completions_2 = json.load(f)\n",
    "    \n",
    "short_completions.extend(short_completions_2) \n",
    "med_completions.extend(med_completions_2) \n",
    "long_completions.extend(long_completions_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6969ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLAMA Y for teacher\n",
    "# test_x = [item[\"x\"] for item in test_raw]\n",
    "# crit = [item[\"c\"] for item in test_raw]\n",
    "# test_prompts = [create_to_revise(x, c, r_0) for x, c, r_0 in zip(test_x, crit, test_r_0_list)]\n",
    "# test_completions = [item[\"y\"] for item in test_raw]\n",
    "\n",
    "\n",
    "# Use GPT as teacher\n",
    "# test_prompts, test_completions = get_revisions(test_r_0_list, test_raw)\n",
    "# with open(\"test_prompts.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(test_prompts, f)\n",
    "# with open(\"test_completions.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(test_completions, f)\n",
    "\n",
    "with open(\"test_prompts.pkl\", \"rb\") as f:\n",
    "    test_prompts = pickle.load(f)\n",
    "with open(\"test_completions.pkl\", \"rb\") as f:\n",
    "    test_completions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10a66f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction and my initial response. A criteria for evaluating the response is also provided.\\n\\nInstruction:\\nCan you draw an SVG logo of a tree?\\n\\nMy Initial Response:\\n I want to know how to create it.\\nSure, here\\'s the SVG code for a simple tree:\\n```\\n<svg width=\"100\" height=\"100\">\\n  <circle cx=\"50\" cy=\"50\" r=\"40\" fill=\"green\"/>\\n</svg>\\nThis SVG code will generate a basic tree shape with a green circle at its center. You can adjust the size and color of the circle as needed to fit your design requirements. \\nAlternatively, if you prefer a more detailed representation, you could use this code in conjunction with other elements such as text or images:\\n```xml\\n<svg xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\\n  <!-- Tree -->\\n  <g transform=\"translate(50 50)\">\\n    <circle cx=\"50\" cy=\"50\" r=\"40\" fill=\"green\"/>\\n    <!-- Add branches and leaves as necessary -->\\n  </g>\\n</svg>\\nIn this case, the tree would be represented by the circle itself, while the branches and leaves are added using additional shapes and paths within the SVG element. This approach allows for greater flexibility in creating complex designs. ``` \\nI hope this helps! Let me know if you have any further questions about SVG trees. \\nIf you\\'d like to explore other SVG formats, such as SVG 2.0 or SVG 3D, feel free to ask too. The possibilities are endless when working with SVGs. \\nAlso, if you\\'re interested in learning more about SVGs, I recommend checking out the official documentation: https://www.w3.org/TR/SVG/. \\nThe tree is just one example of many possible SVG shapes that can be created from this code. Feel free to modify it according to your needs\\n\\nCriteria: Good responses are original, creative, and relevant to the context. They should align with the goals (e.g., improving engagement, enhancing accessibility) and include actionable, well-reasoned ideas.\\n\\nMy initial response may be incorrect and may not follow the criteria. Please revise it using the ideal response as a guide and the criteria for improvement. Return only the revised answer, without any additional comments or explanation.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompts[499]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12544712",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(short_r_0_list)\n",
    "len(short_initial_responses_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e154840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 63/63 [01:27<00:00,  1.38s/it, eta=0h 0m 0s, loss=[1.6973249912261963, 1.6106751494937472]] \n",
      "Testing Epoch 0: 100%|██████████| 16/16 [00:08<00:00,  1.92it/s, eta=0h 0m 0s, loss=[1.3585013151168823, 1.3668926022946835]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Train Loss: 1.6106751494937472. Val Loss: 1.3668926022946835.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 63/63 [01:20<00:00,  1.27s/it, eta=0h 0m 0s, loss=[1.634402871131897, 1.5631618480833749]]  \n",
      "Testing Epoch 1: 100%|██████████| 16/16 [00:08<00:00,  1.92it/s, eta=0h 0m 0s, loss=[1.154567837715149, 1.3534509614109993]] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Train Loss: 1.5631618480833749. Val Loss: 1.3534509614109993.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 63/63 [01:20<00:00,  1.28s/it, eta=0h 0m 0s, loss=[1.1338131427764893, 1.526224240424141]]  \n",
      "Testing Epoch 2: 100%|██████████| 16/16 [00:08<00:00,  1.93it/s, eta=0h 0m 0s, loss=[1.4087748527526855, 1.3300929069519043]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2. Train Loss: 1.526224240424141. Val Loss: 1.3300929069519043.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 63/63 [01:20<00:00,  1.28s/it, eta=0h 0m 0s, loss=[1.098493218421936, 1.5005906763530912]]  \n",
      "Testing Epoch 3: 100%|██████████| 16/16 [00:08<00:00,  1.91it/s, eta=0h 0m 0s, loss=[1.046703815460205, 1.3318162150681019]] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3. Train Loss: 1.5005906763530912. Val Loss: 1.3318162150681019.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 63/63 [01:20<00:00,  1.28s/it, eta=0h 0m 0s, loss=[1.2991149425506592, 1.4804627573679363]] \n",
      "Testing Epoch 4: 100%|██████████| 16/16 [00:08<00:00,  1.93it/s, eta=0h 0m 0s, loss=[1.0831286907196045, 1.326074369251728]] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4. Train Loss: 1.4804627573679363. Val Loss: 1.326074369251728.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 63/63 [01:20<00:00,  1.27s/it, eta=0h 0m 0s, loss=[1.0439282655715942, 1.4578704966439142]] \n",
      "Testing Epoch 5: 100%|██████████| 16/16 [00:08<00:00,  1.93it/s, eta=0h 0m 0s, loss=[1.2248600721359253, 1.3237892240285873]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5. Train Loss: 1.4578704966439142. Val Loss: 1.3237892240285873.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 63/63 [01:20<00:00,  1.27s/it, eta=0h 0m 0s, loss=[1.4727004766464233, 1.448213505366492]]  \n",
      "Testing Epoch 6: 100%|██████████| 16/16 [00:08<00:00,  1.93it/s, eta=0h 0m 0s, loss=[1.3707306385040283, 1.2920049652457237]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6. Train Loss: 1.448213505366492. Val Loss: 1.2920049652457237.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 63/63 [01:20<00:00,  1.27s/it, eta=0h 0m 0s, loss=[1.4036951065063477, 1.425606956557622]]  \n",
      "Testing Epoch 7: 100%|██████████| 16/16 [00:08<00:00,  1.87it/s, eta=0h 0m 0s, loss=[0.817584216594696, 1.2939880676567554]] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7. Train Loss: 1.425606956557622. Val Loss: 1.2939880676567554.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 63/63 [01:20<00:00,  1.28s/it, eta=0h 0m 0s, loss=[1.1577272415161133, 1.4091495918849157]] \n",
      "Testing Epoch 8: 100%|██████████| 16/16 [00:08<00:00,  1.90it/s, eta=0h 0m 0s, loss=[1.5069994926452637, 1.304226879030466]] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8. Train Loss: 1.4091495918849157. Val Loss: 1.304226879030466.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 63/63 [01:20<00:00,  1.28s/it, eta=0h 0m 0s, loss=[1.3388824462890625, 1.4059576647622245]] \n",
      "Testing Epoch 9: 100%|██████████| 16/16 [00:08<00:00,  1.90it/s, eta=0h 0m 0s, loss=[1.2392512559890747, 1.2909319698810577]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9. Train Loss: 1.4059576647622245. Val Loss: 1.2909319698810577.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 63/63 [01:20<00:00,  1.28s/it, eta=0h 0m 0s, loss=[1.4032148122787476, 1.3880148842221214]] \n",
      "Testing Epoch 10: 100%|██████████| 16/16 [00:08<00:00,  1.88it/s, eta=0h 0m 0s, loss=[1.2902824878692627, 1.3207586072385311]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10. Train Loss: 1.3880148842221214. Val Loss: 1.3207586072385311.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 63/63 [01:20<00:00,  1.28s/it, eta=0h 0m 0s, loss=[1.124807596206665, 1.3750402435423836]]  \n",
      "Testing Epoch 11: 100%|██████████| 16/16 [00:08<00:00,  1.89it/s, eta=0h 0m 0s, loss=[1.4010241031646729, 1.3092611208558083]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11. Train Loss: 1.3750402435423836. Val Loss: 1.3092611208558083.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 63/63 [01:24<00:00,  1.34s/it, eta=0h 0m 0s, loss=[1.6524016857147217, 1.3672098470112635]] \n",
      "Testing Epoch 12: 100%|██████████| 16/16 [00:09<00:00,  1.60it/s, eta=0h 0m 0s, loss=[1.3535377979278564, 1.2867965921759605]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12. Train Loss: 1.3672098470112635. Val Loss: 1.2867965921759605.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = run_CITING(train_short, short_r_0_list, short_completions, test_prompts, test_completions, student, student_tokenizer, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d262e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = run_CITING(train_med, med_r_0_list, med_completions, test_prompts, test_completions, student, student_tokenizer, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13096794",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = run_CITING(train_long, long_r_0_list, long_completions, test_prompts, test_completions, student, student_tokenizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4c2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [08:13<00:00,  7.83s/it]\n",
      " 45%|████▍     | 112/250 [14:23<17:52,  7.77s/it]"
     ]
    }
   ],
   "source": [
    "# REGEN TRAINING DATA\n",
    "# TEST\n",
    "new_prompts = []\n",
    "for item, r_0 in zip(test_raw, test_completions):\n",
    "    x, y, c = item['x'], item['y'], item['c']\n",
    "    revised_prompt = create_to_revise(x, c, r_0)\n",
    "    new_prompts.append(revised_prompt)\n",
    "test_completions = student_generate_batch(8, new_prompts, student, student_tokenizer)\n",
    "with open(\"test_completions.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_completions, f)\n",
    "\n",
    "# SHORT\n",
    "new_prompts = []\n",
    "for item, r_0 in zip(train_short, short_r_0_list):\n",
    "    x, y, c = item['x'], item['y'], item['c']\n",
    "    revised_prompt = create_to_revise(x, c, r_0)\n",
    "    new_prompts.append(revised_prompt)\n",
    "short_initial_responses_2 = student_generate_batch(8, new_prompts, student, student_tokenizer)\n",
    "with open(\"short_initial_responses_2.json\", \"w\") as f:\n",
    "    json.dump(short_initial_responses_2, f, indent=4)\n",
    "\n",
    "# # MED\n",
    "# new_prompts = []\n",
    "# for item, r_0 in zip(train_med, med_r_0_list):\n",
    "#     x, y, c = item['x'], item['y'], item['c']\n",
    "#     revised_prompt = create_to_revise(x, c, r_0)\n",
    "#     new_prompts.append(revised_prompt)\n",
    "# med_initial_responses_2 = student_generate_batch(8, new_prompts, student, student_tokenizer)\n",
    "# with open(\"med_initial_responses_2.json\", \"w\") as f:\n",
    "#     json.dump(med_initial_responses_2, f, indent=4)\n",
    "\n",
    "# # LONG\n",
    "# new_prompts = []\n",
    "# for item, r_0 in zip(train_long, long_r_0_list):\n",
    "#     x, y, c = item['x'], item['y'], item['c']\n",
    "#     revised_prompt = create_to_revise(x, c, r_0)\n",
    "#     new_prompts.append(revised_prompt) \n",
    "# long_initial_responses_2 = student_generate_batch(8, new_prompts, student, student_tokenizer)\n",
    "# with open(\"long_initial_responses_2.json\", \"w\") as f:\n",
    "#     json.dump(long_initial_responses_2, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47969976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE: TWO ROUNDs\n",
    "new_prompts = []\n",
    "for item, r_0 in zip(test_raw, test_completions):\n",
    "    x, y, c = item['x'], item['y'], item['c']\n",
    "    revised_prompt = create_to_revise(x, c, r_0)\n",
    "    new_prompts.append(revised_prompt)\n",
    "    \n",
    "test_completions_2 = student_generate_batch(8, new_prompts, student, student_tokenizer)\n",
    "with open(\"test_completions_2.json\", \"w\") as f:\n",
    "    json.dump(test_completions_2, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af19ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE: THREE ROUNDs\n",
    "# new_prompts = []\n",
    "# for item, r_0 in zip(test_raw, test_completions):\n",
    "#     x, y, c = item['x'], item['y'], item['c']\n",
    "#     revised_prompt = create_to_revise(x, c, r_0)\n",
    "#     new_prompts.append(revised_prompt)\n",
    "    \n",
    "# test_completions_2 = student_generate_batch(8, new_prompts, student, student_tokenizer)\n",
    "\n",
    "new_prompts = []\n",
    "for item, r_0 in zip(test_raw, test_completions_2):\n",
    "    x, y, c = item['x'], item['y'], item['c']\n",
    "    revised_prompt = create_to_revise(x, c, r_0)\n",
    "    new_prompts.append(revised_prompt)\n",
    "    \n",
    "test_completions_3 = student_generate_batch(8, new_prompts, student, student_tokenizer)\n",
    "with open(\"test_completions_3.json\", \"w\") as f:\n",
    "    json.dump(test_completions_3, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(raw, student, tokenizer):\n",
    "    prompts = [item[\"x\"] for item in raw]\n",
    "    outputs_list = student_generate_batch(8, prompts, student, tokenizer)\n",
    "    return outputs_list\n",
    "\n",
    "def generate_initial_responses_batched(short_raw, med_raw, long_raw, test_raw, student, tokenizer, batch_size=100) :\n",
    "    length = len(train_short)\n",
    "    test_length = len(test_raw)\n",
    "    \n",
    "    short_initial_responses = []\n",
    "    med_initial_responses = []\n",
    "    long_initial_responses = []\n",
    "    test_initial_responses = []\n",
    "    \n",
    "    for i in range(0, length, batch_size):\n",
    "        short_initial_responses.extend(generate_batch(short_raw[i:i+batch_size], student, tokenizer))\n",
    "        med_initial_responses.extend(generate_batch(med_raw[i:i+batch_size], student, tokenizer))\n",
    "        long_initial_responses.extend(generate_batch(long_raw[i:i+batch_size], student, tokenizer))\n",
    "        \n",
    "        with open(\"short_initial_responses_2.json\", \"w\") as f:\n",
    "            json.dump(short_initial_responses, f, indent=4)\n",
    "        with open(\"med_initial_responses_2.json\", \"w\") as f:\n",
    "            json.dump(med_initial_responses, f, indent=4)\n",
    "        with open(\"long_initial_responses_2.json\", \"w\") as f:\n",
    "            json.dump(long_initial_responses, f, indent=4)\n",
    "        \n",
    "        if i < test_length:\n",
    "            test_initial_responses.extend(generate_batch(test_raw[i:i+batch_size], student, tokenizer))\n",
    "            with open(\"test_initial_responses_2.json\", \"w\") as f:\n",
    "                json.dump(test_initial_responses, f, indent=4)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61437342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_initial_responses_batched(train_short, train_med, train_long, test_raw, student, student_tokenizer, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab364b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_inference(initial_completions, num_regens):\n",
    "    completions = initial_completions\n",
    "    \n",
    "    for i in range(num_regens):\n",
    "        for item, r_0 in zip(leaderboard_raw, completions):\n",
    "            x, _, c = item['x'], item['y'], item['c']\n",
    "            revised_prompt = create_to_revise(x, c, r_0)\n",
    "            new_prompts.append(revised_prompt)\n",
    "        completions = student_generate_batch(8, new_prompts, student, student_tokenizer)\n",
    "        \n",
    "    return completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e761305",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"leaderboard_raw.json\", \"r\") as f:\n",
    "    leaderboard_raw = json.load(f)\n",
    "leaderboard_prompts = [item[\"x\"] for item in leaderboard_raw]\n",
    "\n",
    "df = pd.read_json('leaderboard_subs.jsonl', lines=True)\n",
    "initial_completions = df[\"response\"].tolist() # student_generate_batch(8, leaderboard_prompts, student, student_tokenizer)\n",
    "\n",
    "df = pd.read_json('leaderboard_subs.jsonl', lines=True)\n",
    "df[\"response\"] = regenerate_inference(initial_completions, 1)\n",
    "\n",
    "# df.to_json('leaderboard_subs.jsonl', lines=True, orient='records')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
