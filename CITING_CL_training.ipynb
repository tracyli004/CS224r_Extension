{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721bffa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "# from peft import PeftModel\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "# import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from vllm import LLM, SamplingParams\n",
    "# import openai\n",
    "import time\n",
    "import tensorboard\n",
    "import pickle\n",
    "import math\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_printoptions(threshold=float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f799aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade \"jinja2>=3.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a663972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Prompts and Criteria and Teacher responses (Original UltraFeedback Llama responses)\n",
    "\n",
    "with open(\"train_short.json\", \"r\") as f:\n",
    "    train_short = json.load(f)\n",
    "with open(\"train_med.json\", \"r\") as f:\n",
    "    train_med = json.load(f)\n",
    "with open(\"train_long.json\", \"r\") as f:\n",
    "    train_long = json.load(f)\n",
    "with open(\"test.json\", \"r\") as f:\n",
    "    test_raw = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed2813a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 01:22:15.013875: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749345735.023498   51474 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749345735.028613   51474 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# load tokenizer and model\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\", padding_side='right')\n",
    "student = AutoModelForCausalLM.from_pretrained(\"./dpo_model\", torch_dtype=torch.float32)\n",
    "\n",
    "# add special tokens\n",
    "student_tokenizer.add_special_tokens({\n",
    "    'pad_token': '<|pad|>',\n",
    "    'bos_token': '<|im_start|>',\n",
    "    'eos_token': '<|im_end|>',\n",
    "})\n",
    "\n",
    "# resize model embeddings to include new tokens\n",
    "student.resize_token_embeddings(len(student_tokenizer))\n",
    "# set token ids in config\n",
    "student.config.pad_token_id = student_tokenizer.pad_token_id\n",
    "student.config.bos_token_id = student_tokenizer.bos_token_id\n",
    "student.config.eos_token_id = student_tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36fdb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = (\n",
    "    \"{% set image_count = namespace(value=0) %}\"\n",
    "    \"{% set video_count = namespace(value=0) %}\"\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{% if loop.first and message['role'] != 'system' %}\"\n",
    "    \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "    \"{% endif %}\"\n",
    "    \"<|im_start|>{{ message['role'] }}\\n\"\n",
    "    \"{% if message['content'] is string %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}\"\n",
    "    \"{% generation %}\"\n",
    "    \"{{ message['content'] }}\"\n",
    "    \"{% endgeneration %}\"\n",
    "    \"{% else %}\"\n",
    "    \"{{ message['content'] }}\"\n",
    "    \"{% endif %}\"\n",
    "    \"<|im_end|>\\n\"\n",
    "    \"{% else %}\"\n",
    "    \"{% for content in message['content'] %}\"\n",
    "    \"{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}\"\n",
    "    \"{% set image_count.value = image_count.value + 1 %}\"\n",
    "    \"{% if add_vision_id %}\"\n",
    "    \"Picture {{ image_count.value }}: \"\n",
    "    \"{% endif %}\"\n",
    "    \"<|vision_start|><|image_pad|><|vision_end|>\"\n",
    "    \"{% elif content['type'] == 'video' or 'video' in content %}\"\n",
    "    \"{% set video_count.value = video_count.value + 1 %}\"\n",
    "    \"{% if add_vision_id %}\"\n",
    "    \"Video {{ video_count.value }}: \"\n",
    "    \"{% endif %}\"\n",
    "    \"<|vision_start|><|video_pad|><|vision_end|>\"\n",
    "    \"{% elif 'text' in content %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}\"\n",
    "    \"{% generation %}\"\n",
    "    \"{{ content['text'] }}\"\n",
    "    \"{% endgeneration %}\"\n",
    "    \"{% else %}\"\n",
    "    \"{{ content['text'] }}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"<|im_end|>\\n\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"<|im_start|>assistant\\n\"\n",
    "    \"{% endif %}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c0f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "MAX_TOKENS = 512\n",
    "\n",
    "MAX_INPUT_TOKENS = 512\n",
    "ACCUM_STEPS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e969cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length = MAX_INPUT_TOKENS):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        messages = self.dataset[idx]\n",
    "\n",
    "        new_messages = []\n",
    "        for m in messages:\n",
    "            if not new_messages and m[\"role\"] == \"user\":\n",
    "                new_messages.append(m)\n",
    "            elif new_messages and m[\"role\"] == \"assistant\":\n",
    "                new_messages.append(m)\n",
    "                break\n",
    "        if len(new_messages) != 2:\n",
    "            return self._empty_item()\n",
    "        \n",
    "        try:\n",
    "            tokenized = self.tokenizer.apply_chat_template(\n",
    "                new_messages,\n",
    "                tokenize = True,\n",
    "                max_length = self.max_length,\n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                return_dict = True,\n",
    "                return_assistant_tokens_mask=True,\n",
    "                add_generation_prompt = False,\n",
    "                chat_template = chat_template,\n",
    "                return_tensors = 'pt'\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(idx, e)\n",
    "            return self._empty_item()\n",
    "\n",
    "        input_ids = tokenized['input_ids']\n",
    "        assistant_masks = tokenized['assistant_masks']\n",
    "        if assistant_masks.sum() == 0:\n",
    "            return self._empty_item()\n",
    "\n",
    "        mod_assistant_mask = assistant_masks.clone()\n",
    "        matches = (input_ids == self.tokenizer.convert_tokens_to_ids(\"<|im_end|>\"))\n",
    "        indices = torch.nonzero(matches)\n",
    "        mod_assistant_mask[tuple(indices[-1])] = 1 # inlcude end speaking token in assistant to include in lables\n",
    "\n",
    "        attention_mask = tokenized['attention_mask']\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[mod_assistant_mask == 0] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids.squeeze(0),\n",
    "            'attention_mask': attention_mask.squeeze(0),\n",
    "            'labels': labels.squeeze(0)\n",
    "        }\n",
    "    \n",
    "    def _empty_item(self):\n",
    "        return {\n",
    "            'input_ids': torch.zeros(self.max_length, dtype=torch.int32),\n",
    "            'attention_mask': torch.zeros(self.max_length, dtype=torch.int32),\n",
    "            'labels': torch.full((self.max_length,), -100, dtype=torch.int32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94c40e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_generate_batch(batch_size, prompts, model, tokenizer):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\", padding_side = 'left')\n",
    "    outputs_list = []\n",
    "\n",
    "    for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "        batch = prompts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        output_sequences = model.generate(\n",
    "            input_ids=inputs['input_ids'].to(model.device),\n",
    "            attention_mask=inputs['attention_mask'].to(model.device),\n",
    "            tokenizer = tokenizer,\n",
    "            do_sample=False, # disable sampling to test if batching affects output\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            forced_eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.5,\n",
    "            stop_strings = '<|im_end|>',\n",
    "            exponential_decay_length_penalty = (int(MAX_TOKENS * 0.7),1.1),\n",
    "            max_new_tokens= MAX_TOKENS\n",
    "        )\n",
    "        completions_only = output_sequences[:, inputs['input_ids'].shape[1]:]\n",
    "        outputs_decoded = tokenizer.batch_decode(completions_only, skip_special_tokens=True)\n",
    "        # print(output_completions)\n",
    "        # print(output_sequences)\n",
    "        outputs_list.extend(outputs_decoded)\n",
    "    return outputs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04274f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "def teacher_generate_batch(prompts, model=\"gpt-4\", system_prompt=\"You are a helpful assistant.\"):\n",
    "    # Step 1: Write prompts to JSONL\n",
    "    input_path = \"batch_input.jsonl\"\n",
    "    with open(input_path, \"w\") as f:\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            item = {\n",
    "                \"custom_id\": f\"request-{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    \"max_tokens\": MAX_TOKENS,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"stop\": [\"<|im_end|>\"],\n",
    "                    \"frequency_penalty\": 1.5,\n",
    "                    \"presence_penalty\": 0.0\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "    # Step 2: Upload the file using OpenAI client\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        upload = client.files.create(file=f, purpose=\"batch\")\n",
    "    file_id = upload.id\n",
    "\n",
    "    # Step 3: Submit batch\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\"\n",
    "    )\n",
    "    batch_id = batch.id\n",
    "    # print(\"Batch submitted. ID:\", batch_id)\n",
    "\n",
    "    # Step 4: Poll until complete\n",
    "    # print(\"Waiting for batch to complete...\")\n",
    "    while True:\n",
    "        batch_status = client.batches.retrieve(batch_id)\n",
    "        status = batch_status.status\n",
    "        # print(f\"Current status: {status}\")\n",
    "        if status in [\"completed\", \"failed\", \"cancelled\", \"expired\"]:\n",
    "            break\n",
    "        time.sleep(15)\n",
    "\n",
    "    if status != \"completed\":\n",
    "        raise RuntimeError(f\"Batch failed or didn't complete: {status}\")\n",
    "\n",
    "    # Step 5: Download result file\n",
    "    output_file_id = batch_status.output_file_id\n",
    "    output_response = client.files.content(output_file_id)\n",
    "    output_data = output_response.text\n",
    "\n",
    "    # Step 6: Parse results into dictionary\n",
    "    responses = {}\n",
    "    for line in output_data.splitlines():\n",
    "        obj = json.loads(line)\n",
    "        custom_id = obj[\"custom_id\"]\n",
    "        content = obj[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        responses[custom_id] = content\n",
    "\n",
    "    # Step 7: Return outputs in order\n",
    "    return [responses[f\"request-{i}\"] for i in range(len(prompts))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d76e62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_raw):\n",
    "    formatted_data = [[{\"content\": str(item[\"x\"]), \"role\": \"user\"},\n",
    "                       {\"content\": str(item[\"y\"]), \"role\": \"assistant\"}] for item in train_raw ]\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "088524b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_list(prompt, completion):\n",
    "    data = [[\n",
    "        {'content': p, 'role': 'user'},\n",
    "        {'content': c, 'role': 'assistant'}\n",
    "    ] for p, c in zip(prompt, completion)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "072f4ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_to_revise(x, c, r_0):\n",
    "    prompt = (\n",
    "        f\"Below is an instruction and my initial response. A criteria for evaluating the response is also provided.\\n\\n\"\n",
    "        f\"Instruction:\\n{x}\\n\\n\"\n",
    "        f\"My Initial Response:\\n{r_0}\\n\\n\"\n",
    "        f\"Criteria: {c}\\n\\n\"\n",
    "        f\"My initial response may be incorrect and may not follow the criteria. Please revise it using the ideal response as a guide and the criteria for improvement. \"\n",
    "        f\"Return only the revised answer, without any additional comments or explanation.\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55cc206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_revisions(r_0_list, raw_data):\n",
    "    \n",
    "    # student.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     r_0_list = student_generate_batch(batch_size=BATCH_SIZE, prompts=prompts, model=student, tokenizer=student_tokenizer)\n",
    "\n",
    "    revised_prompts = []\n",
    "    revisions = []\n",
    "    for item, r_0 in zip(raw_data, r_0_list):\n",
    "        x, y, c = item['x'], item['y'], item['c']\n",
    "        revised_prompt = create_to_revise(x, c, r_0)\n",
    "        revised_prompts.append(revised_prompt)\n",
    "        revisions.append(y)\n",
    "\n",
    "    # 2. teacher revises (no grad)\n",
    "    with torch.no_grad():\n",
    "        revisions = teacher_generate_batch(revised_prompts, model=\"gpt-4o-mini\", system_prompt=\"You are an expert writer.\")\n",
    "    \n",
    "    print(revised_prompts[332], revisions[332])\n",
    "    return revised_prompts, revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07ef518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train(model, train_loader, optimizer, writer, epoch):\n",
    "\n",
    "    total_loss = 0\n",
    "    batch_times = []\n",
    "    progress = tqdm(train_loader, desc=f\"Training Epoch {epoch}\", leave=True)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, batch in enumerate(progress):\n",
    "        start = time.time()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        if torch.all(labels == -100) or torch.all(input_ids == 0):\n",
    "            print(f\"⏭️  Skipping empty batch {i}\")\n",
    "            continue\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss  # normalize loss for accumulation\n",
    "                \n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "        # assert not math.isnan(loss.item()), f'Loss: {loss}, \\nOutputs: {outputs}'\n",
    "        loss.backward()\n",
    "    \n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        optimizer.step()\n",
    "        torch.cuda.synchronize()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()  \n",
    "        avg_loss = total_loss / (i + 1)\n",
    "\n",
    "\n",
    "        batch_time = time.time() - start\n",
    "        batch_times.append(batch_time)\n",
    "        avg_time = sum(batch_times) / len(batch_times)\n",
    "        eta = avg_time * (len(train_loader) - (i + 1))\n",
    "        eta_hr, remainder = divmod(int(eta), 3600)\n",
    "        eta_min, eta_sec = divmod(remainder, 60)\n",
    "\n",
    "        progress.set_postfix(loss=[loss.item(), avg_loss], eta=f\"{eta_hr}h {eta_min}m {eta_sec}s\")\n",
    "\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            model.save_pretrained('./latest_model')\n",
    "            with open('latest_opt.pkl', 'wb') as f:\n",
    "                pickle.dump(optimizer, f)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    writer.add_scalar(\"Loss/train\", avg_loss, epoch * len(train_loader) + i)\n",
    "    return float(avg_loss)\n",
    "\n",
    "\n",
    "def test(model, test_loader, writer, epoch, return_generations=False):\n",
    "    total_loss = 0\n",
    "    batch_times = []\n",
    "    progress = tqdm(test_loader, desc=f\"Testing Epoch {epoch}\", leave=True)\n",
    "\n",
    "    for i, batch in enumerate(progress):\n",
    "        start = time.time()\n",
    "        \n",
    "        input_ids = batch['input_ids'].long().to(device)\n",
    "        attention_mask = batch['attention_mask'].long().to(device)\n",
    "        labels = batch['labels'].long().to(device)\n",
    "        \n",
    "        if torch.all(labels == -100) or torch.all(input_ids == 0):\n",
    "            print(f\"⏭️  Skipping empty batch {i}\")\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        if math.isnan(loss):\n",
    "            print(\"NAN loss\")\n",
    "            continue\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        avg_loss = float(total_loss) / (i + 1)\n",
    "\n",
    "\n",
    "        batch_time = time.time() - start\n",
    "        batch_times.append(batch_time)\n",
    "        avg_time = sum(batch_times) / len(batch_times)\n",
    "        eta = avg_time * (len(test_loader) - (i + 1))\n",
    "        eta_hr, remainder = divmod(int(eta), 3600)\n",
    "        eta_min, eta_sec = divmod(remainder, 60)\n",
    "\n",
    "        progress.set_postfix(loss=[loss.item(), avg_loss], eta=f\"{eta_hr}h {eta_min}m {eta_sec}s\")\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            model.save_pretrained('./checkpoints/latest_step')\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    writer.add_scalar(\"Loss/val\", avg_loss, epoch)\n",
    "    return float(avg_loss)\n",
    "\n",
    "\n",
    "def fine_tune(model, train_loader, test_loader, optimizer, num_epochs):\n",
    "    writer = SummaryWriter()  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, optimizer, writer, epoch)\n",
    "        # model.save_pretrained('./citing_model')\n",
    "        val_loss = test(model, test_loader, writer, epoch)\n",
    "        print(f'Epoch: {epoch}. Train Loss: {train_loss}. Val Loss: {val_loss}.')\n",
    "        \n",
    "    # model.save_pretrained('./citing_model')\n",
    "\n",
    "    writer.close()\n",
    "    return train_loss, val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97edfec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_short_dataloaded = load_data(train_short)\n",
    "train_med_dataloaded = load_data(train_med)\n",
    "train_long_dataloaded = load_data(train_long)\n",
    "test_dataloaded = load_data(test_raw)\n",
    "\n",
    "UR_train_short_prompts = [item[0][\"content\"] for item in train_short_dataloaded]\n",
    "UR_train_med_prompts = [item[0][\"content\"] for item in train_med_dataloaded]\n",
    "UR_train_long_prompts = [item[0][\"content\"] for item in train_long_dataloaded]\n",
    "UR_test_prompts = [item[0][\"content\"] for item in test_dataloaded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "501967d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_prompts.pkl\", \"rb\") as f:\n",
    "    test_prompts = pickle.load(f)\n",
    "with open(\"test_completions.pkl\", \"rb\") as f:\n",
    "    test_completions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "868fcc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"short_initial_responses.json\", \"r\") as f:\n",
    "    short_r_0_list_raw = json.load(f)\n",
    "short_r_0_list = [ex[\"completion\"] for ex in short_r_0_list_raw]\n",
    "\n",
    "with open(\"med_initial_responses.json\", \"r\") as f:\n",
    "    med_r_0_list_raw = json.load(f)\n",
    "med_r_0_list = [ex[\"completion\"] for ex in med_r_0_list_raw]\n",
    "\n",
    "with open(\"long_initial_responses.json\", \"r\") as f:\n",
    "    long_r_0_list_raw = json.load(f)\n",
    "long_r_0_list = [ex[\"completion\"] for ex in long_r_0_list_raw]\n",
    "\n",
    "with open(\"test_initial_responses.json\", \"r\") as f:\n",
    "    test_r_0_list_raw = json.load(f)\n",
    "test_r_0_list = [ex[\"completion\"] for ex in test_r_0_list_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11713d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CITING(train_prompts, train_raw, r_0_list, test_prompts, test_completions, student, tokenizer, num_epochs):\n",
    "    optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, student.parameters()), lr=1e-6)\n",
    "    \n",
    "    train_prompts, train_completions = get_revisions(r_0_list, train_raw)\n",
    "    print(\"\\n LEN: \", len(train_raw))\n",
    "    print(\"\\n LEN: \", len(train_prompts))\n",
    "    train_set_loaded = load_data_from_list(train_prompts, train_completions)\n",
    "    print(\"\\n LEN: \", len(train_set_loaded))\n",
    "    train_set_CL = CLDataset(train_set_loaded, tokenizer)\n",
    "    train_loader = DataLoader(train_set_CL, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    \n",
    "    # test_prompts, test_completions = get_revisions(r_0_list, train_raw)\n",
    "    test_set_loaded = load_data_from_list(test_prompts, test_completions)\n",
    "    test_set_CL = CLDataset(test_set_loaded, tokenizer)\n",
    "    test_loader = DataLoader(test_set_CL, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    \n",
    "    train_loss, val_loss = fine_tune(\n",
    "        model = student,\n",
    "        train_loader = train_loader,\n",
    "        test_loader = test_loader,\n",
    "        optimizer = optim,\n",
    "        num_epochs = num_epochs\n",
    "    )\n",
    "\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8bcab36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student.to(device)\n",
    "print(student.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6969ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_r_0 = [item[\"completion\"] for item in test_r_0_list_raw]\n",
    "test_x = [item[\"prompt\"] for item in test_r_0_list_raw]\n",
    "crit = [item[\"c\"] for item in test_raw]\n",
    "\n",
    "test_prompts = [create_to_revise(x, c, r_0) for x, c, r_0 in zip(test_x, crit, test_r_0)]\n",
    "test_completions = [item[\"y\"] for item in test_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ce35f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0b932a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_prompts, train_completions = get_revisions(student, BATCH_SIZE, UR_train_short_prompts, r_0_list, train_short)\n",
    "\n",
    "# train_set_loaded = load_data_from_list(train_prompts, train_completions)\n",
    "# train_set_CL = CLDataset(train_set_loaded, student_tokenizer)\n",
    "# train_loader = DataLoader(train_set_CL, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "# test_set_loaded = load_data_from_list(test_prompts, test_completions)\n",
    "# test_set_CL = CLDataset(test_set_loaded, student_tokenizer)\n",
    "# test_loader = DataLoader(test_set_CL, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "# decoded_text = student_tokenizer.decode(train_set_CL[1][\"input_ids\"], skip_special_tokens=True)\n",
    "# print(decoded_text)\n",
    "\n",
    "# truncated_r0 = [s[:100] for s in short_r_0_list[:4]]\n",
    "# truncated_r0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "747a9cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_r_0_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f0ef5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is nextjs create robot.txt file on request via crawler?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UR_train_short_prompts[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e154840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction and my initial response. A criteria for evaluating the response is also provided.\n",
      "\n",
      "Instruction:\n",
      "For this chain-of-thought reasoning and answer, what was the question?\n",
      "The two men are either writing a check or cleaning debris.\n",
      " A: no\n",
      "\n",
      "My Initial Response:\n",
      "\n",
      " The case often causes trouble for people. People usually make mistakes while taking things apart and reassemble them in proper order.\n",
      "A simplest way to do it is by painting an item with tape to label its contents.\n",
      "It seems suspicious at first glance because there were three packages broken open.\n",
      "\n",
      "Quesmet�mail.com:\n",
      "In which of these statements would you agree? Statement 1: It's normal for human beings to experience frustration when attempting something new. Statement 2: To avoid dealing with such situations we should always err on safety measures during complex work sessions.\n",
      "What does option B imply if statement 2 is true?\n",
      "\n",
      "Options: \n",
      "- Only Sentence A (\"Human beings may someday create art\" from Reddit)\n",
      "- ...\n",
      "- None (we don't know whether Option B implies sentence C yet)\n",
      "\n",
      "Unfortunately, I am unable to process your query due to limitations in text-based interactions.\n",
      "\n",
      "Answering outside of provided context might be inaccurate unless specific information about the narrative flow is given. As per our previous conversation where I aimed to provide general assistance and formatting advice based on common knowledge points involved in solving puzzles and难题 solutions:\n",
      "\n",
      "Since the problem has not been presented here, I will continue without further guidance beyond identifying potential logical connections between elements that can lead to\n",
      "\n",
      "Criteria: Good responses use logical reasoning to justify conclusions, clearly showing how the inference aligns with the provided premise or context.\n",
      "\n",
      "My initial response may be incorrect and may not follow the criteria. Please revise it using the ideal response as a guide and the criteria for improvement. Return only the revised answer, without any additional comments or explanation. The question likely pertains to a scenario involving two men engaged in an activity, and the options presented seem to relate to their actions. A possible question could be: \"What are the two men doing?\" The answer \"no\" suggests that they are not performing one of the activities mentioned (writing a check or cleaning debris), indicating that they might be involved in something else entirely.\n",
      "\n",
      " LEN:  333\n",
      "\n",
      " LEN:  333\n",
      "\n",
      " LEN:  333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 11/11 [00:20<00:00,  1.86s/it, eta=0h 0m 0s, loss=[1.6422415971755981, 1.485398682680997]] \n",
      "Testing Epoch 0: 100%|██████████| 4/4 [00:01<00:00,  2.67it/s, eta=0h 0m 0s, loss=[2.714216947555542, 1.8334104120731354]] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Train Loss: 1.485398682680997. Val Loss: 1.8334104120731354.\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = run_CITING(UR_train_short_prompts, train_short, short_r_0_list, test_prompts, test_completions, student, student_tokenizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d262e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction and my initial response. A criteria for evaluating the response is also provided.\n",
      "\n",
      "Instruction:\n",
      "How do you prepare the ground turkey for the zucchini lasagna? Answer according to: Recipe for clean and lean zucchini lasagna, perfect for getting your gains on this fall!\n",
      "Cook ground turkey with diced onion, diced garlic and Mrs dash original blend spice till no longer pink.\n",
      "Then top with final amount of zucchini a thin layer of mozzarella and feta. Add a couple slices of tomato on top and fresh or dried basil, then drizzle one more time with olive oil and a little bit of balsamic.\n",
      "Remove cover and bake until cheese is golden!\n",
      "Enjoy this delicious recipe on a nice fall night!\n",
      "\n",
      "My Initial Response:\n",
      " Download Dessertful – Everything in our kitchen (except some). Cooked Louisiana Beef. Sampoo sheds oh twelve oz!\n",
      "\n",
      "To go ahead, I will please perform:\n",
      "\n",
      "1. Review what's been shared so far\n",
      "2. Continue making corrections as appropriate\n",
      "\n",
      "Please wait for instructions above before proceeding.\n",
      "\n",
      "Instructions should start from where you were most recently updated after reviewing previous text snippets. \n",
      "\n",
      "Once new information arrives, respond accordingly based on the provided context. Since there isn't any previously defined set phrase that requires repeated execution within the initial instruction format, the response would involve filling in the gaps using available prompts and follow-up actions without explicit guidance beyond the given content. For instance, if the problem statement was \"Travel west,\" you could provide an alternative destination instead of specifying it again like \"This step needs completion.\" But since only relevant parts are required here:\n",
      "The space once filled by past responses remains empty. New contents must be added under each column header (\"Problem\", \"Question\"), separated by semicolons. The user now starts responding below these guidelines, ready to proceed.\n",
      "\n",
      "Criteria: The response should provide clean, executable code that directly addresses the task, with clear comments explaining each step. It should handle any specified edge cases and, where relevant, demonstrate understanding of underlying concepts.\n",
      "\n",
      "My initial response may be incorrect and may not follow the criteria. Please revise it using the ideal response as a guide and the criteria for improvement. Return only the revised answer, without any additional comments or explanation. To prepare the ground turkey for the zucchini lasagna, follow these steps:\n",
      "\n",
      "1. **Ingredients Needed**:\n",
      "   - 1 pound of ground turkey\n",
      "   - 1 medium onion, diced\n",
      "   - 2-3 cloves of garlic, minced\n",
      "   - Mrs. Dash Original Blend spice (to taste)\n",
      "   \n",
      "2. **Cooking Instructions**:\n",
      "   a. In a large skillet over medium heat, add a drizzle of olive oil.\n",
      "   \n",
      "   b. Once the oil is hot, add the diced onion and sauté until translucent (about 3-4 minutes).\n",
      "   \n",
      "   c. Add minced garlic to the skillet and cook for an additional minute until fragrant.\n",
      "   \n",
      "   d. Increase heat to medium-high and add in the ground turkey.\n",
      "   \n",
      "   e. Cook while breaking it apart with a spatula until no longer pink (approximately 7-10 minutes).\n",
      "  \n",
      "3. **Seasoning**: \n",
      "    f. Sprinkle Mrs Dash Original Blend spice over cooked turkey mixture and stir well to combine.\n",
      "\n",
      "Now your prepared ground turkey is ready to be layered into your zucchini lasagna! Enjoy this delicious recipe on a nice fall night!\n",
      "\n",
      " LEN:  333\n",
      "\n",
      " LEN:  333\n",
      "\n",
      " LEN:  333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 11/11 [00:13<00:00,  1.23s/it, eta=0h 0m 0s, loss=[1.2420239448547363, 1.2146953019228848]]\n",
      "Testing Epoch 0: 100%|██████████| 4/4 [00:01<00:00,  2.65it/s, eta=0h 0m 0s, loss=[1.5889639854431152, 1.5455721020698547]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Train Loss: 1.2146953019228848. Val Loss: 1.5455721020698547.\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = run_CITING(UR_train_med_prompts, train_med, med_r_0_list, test_prompts, test_completions, student, student_tokenizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13096794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction and my initial response. A criteria for evaluating the response is also provided.\n",
      "\n",
      "Instruction:\n",
      "generate tests for this code:\n",
      "import json\n",
      "import os\n",
      "from dataclasses import dataclass\n",
      "from pathlib import Path\n",
      "from typing import Dict\n",
      "from typing import List, Union\n",
      "\n",
      "import pyperclip\n",
      "from aenum import MultiValueEnum\n",
      "\n",
      "from code\\_keeper import cast\\_enum\n",
      "class GardenArea(MultiValueEnum):\n",
      " main = 1, 'main'\n",
      " secondary = 2, 'secondary'\n",
      " inbox = 3, 'inbox'\n",
      "@dataclass\n",
      "class CodeFragment:\n",
      " code: str\n",
      " tags: List[str]\n",
      " area: GardenArea\n",
      "\n",
      " @property\n",
      " def name(self):\n",
      " return '.'.join(self.tags)\n",
      " # name: str\n",
      " # categories: List[str]\n",
      " # id: str = None\n",
      " #\n",
      " # def \\_\\_post\\_init\\_\\_(self):\n",
      " # if self.id is None:\n",
      " # self.id = str(uuid.uuid4())\n",
      " # # todo: \\_record\\_event - creation\n",
      "DEFAULT\\_GARDEN\\_ROOT = os.path.expanduser('~/.code\\_garden')\n",
      "class CodeGarden:\n",
      " def \\_\\_init\\_\\_(self, path=None):\n",
      " if path is None:\n",
      " path = self.\\_find\\_garden()\n",
      " self.path = Path(path).expanduser()\n",
      " # create if not exists\n",
      " self.path.mkdir(parents=True, exist\\_ok=True)\n",
      "\n",
      " @staticmethod\n",
      " def \\_find\\_garden():\n",
      " return os.getenv('CODE\\_GARDEN\\_ROOT', DEFAULT\\_GARDEN\\_ROOT)\n",
      "\n",
      " @staticmethod\n",
      " def \\_cast\\_area(area):\n",
      " return cast\\_enum(area, GardenArea)\n",
      "\n",
      " def find(self,\n",
      " keys, area: Union[str, GardenArea] = None,\n",
      " keys\\_and: bool = True\n",
      " ) -> Dict[str, CodeFragment]:\n",
      " if isinstance(area, str):\n",
      " area = self.\\_cast\\_area(area)\n",
      " areas = [area] if area is not None else GardenArea\n",
      " result = {}\n",
      " for area in areas:\n",
      " for path in self.\\_find\\_paths(keys, area, keys\\_and):\n",
      " code = path.read\\_text()\n",
      " tags = path.stem.split('.')\n",
      " code\\_fragment = CodeFragment(code, tags, area)\n",
      " result[code\\_fragment.name] = code\\_fragment\n",
      " return result\n",
      "\n",
      " def \\_find\\_paths(self, keys, area: GardenArea, keys\\_and: bool = True):\n",
      " area\\_path = self.path / area.value\n",
      "\n",
      " for path in area\\_path.iterdir():\n",
      " # just take .py files\n",
      " if path.suffix == '.py' and path.is\\_file():\n",
      " if keys\\_and and all(key in path.stem for key in keys):\n",
      " yield path\n",
      " elif any(key in path.stem for key in keys):\n",
      " yield path\n",
      "\n",
      " def save(self, code, tags, area, metadata=None, force=False):\n",
      " # todo: check if code is already saved\n",
      " path = self.\\_generate\\_path(area, tags)\n",
      " if path.exists() and not force:\n",
      " raise ValueError(f'Code already exists at {path}')\n",
      " path.parent.mkdir(parents=True, exist\\_ok=True)\n",
      " with path.open('w') as f:\n",
      " if metadata is not None:\n",
      " f.write('# metadata ')\n",
      " json.dump(metadata, f)\n",
      " f.write(code)\n",
      "\n",
      " def \\_generate\\_path(self, area, tags, ext='.py'):\n",
      " if not ext.startswith('.'):\n",
      " ext = '.' + ext\n",
      " return self.path / area.value / '.'.join(tags) + ext\n",
      "class CodeKeeper:\n",
      " def \\_\\_init\\_\\_(self, code\\_garden: CodeGarden = None):\n",
      " if code\\_garden is None:\n",
      " code\\_garden = CodeGarden() # finds path automatically\n",
      " elif isinstance(code\\_garden, str):\n",
      " code\\_garden = CodeGarden(code\\_garden)\n",
      " self.code\\_garden = code\\_garden\n",
      "\n",
      " # todo: read to\\_clipboard from config (env)\n",
      " def remind(self, keys: Union[str, List[str]] = None, area: Union[str,\n",
      " GardenArea] = None, keys\\_and: bool = True, to\\_clipboard=True) -> Dict[str,\n",
      " CodeFragment]:\n",
      " # step 1: parse the keys\n",
      " if keys is None:\n",
      " keys = ['remind']\n",
      " keys = self.\\_parse\\_keys(keys)\n",
      "\n",
      " # step 2: find the code\n",
      " code\\_fragments = self.code\\_garden.find(keys, area, keys\\_and)\n",
      "\n",
      " # step 3: compose the formatted code\n",
      " res = \"\"\n",
      " for code\\_fragment in sorted(code\\_fragments.values(),\n",
      " key=lambda x: x.area.value):\n",
      " res += f\"# {code\\_fragment.name}\"\n",
      " res += f\"{code\\_fragment.code}\"\n",
      " res += f\"\\n\\n\"\n",
      " # todo: formatted code for jupyter\n",
      "\n",
      " # step 4: copy the code to the clipboard\n",
      " if to\\_clipboard:\n",
      " pyperclip.copy(res)\n",
      "\n",
      " return res\n",
      "\n",
      " @staticmethod\n",
      " def \\_parse\\_keys(keys):\n",
      " if isinstance(keys, str):\n",
      " keys = keys.split('.')\n",
      " return keys\n",
      "\n",
      " find = remind\n",
      "\n",
      " def plant(self, code, tagline, area: Union[str, GardenArea] =\n",
      " GardenArea.inbox, force=False):\n",
      " # step 1: parse the tagline\n",
      " if '/' in tagline:\n",
      " area, tagline = tagline.rsplit('/', 1)\n",
      " tags = self.\\_parse\\_tagline(tagline)\n",
      "\n",
      " # step 2: save the code\n",
      " self.code\\_garden.save(\n",
      " code, tags, area, force=force\n",
      " )\n",
      "\n",
      " def \\_parse\\_tagline(self, tagline):\n",
      " return tagline.split('.')\n",
      "\n",
      " add = plant\n",
      "\n",
      " def generate\\_summary(self):\n",
      " summary = \"\"\n",
      " # genereate summary by tag (most used)\n",
      " summary += self.code\\_garden.\\_generate\\_summary\\_by\\_tag()\n",
      "\n",
      " # todo: generate summary by usage (most visited / least visited)\n",
      " # todo: generate summary by creaetion date (most recent few)\n",
      " # todo: something else? How do I highlight the most important code?\n",
      " # random items just in case? :)\n",
      " return summary\n",
      " # todo: figure out intuitive name for this\n",
      "\n",
      " # todo: SECTIONS TO ADD\n",
      " # SECTION 1: HOUSEKEEPING - sorting inbox etc.\n",
      " # SECTION 2: add Categories, Generate with AI, use for summary\n",
      " # SECTION 3: add logging, use for summary\n",
      " # SECTION 4: Notion, sync on startup, sync on save\n",
      "\n",
      " # def housekeeping(self):\n",
      " # \"\"\"Go over the garden and see about the plant names\"\"\"\n",
      " # # 1) start with main first, then inbox, then secondary\n",
      " # # 2) mark completed, completion time\n",
      " # # 3) revisit\n",
      " # # 4) revisit everything globally\n",
      " # # 5) keep a global tag registry - with counts and introduction date / event dates.\n",
      " # # 6) if there were many tag updates since the plant is finalised\n",
      " # # 7) periodically run housekeeping. Send the requests for input through telegram\n",
      "MemoryKeeper = CodeKeeper\n",
      "\n",
      "My Initial Response:\n",
      "\n",
      "\n",
      "Criteria: The response should provide clean, executable code that directly addresses the task, with clear comments explaining each step. It should handle any specified edge cases and, where relevant, demonstrate understanding of underlying concepts.\n",
      "\n",
      "My initial response may be incorrect and may not follow the criteria. Please revise it using the ideal response as a guide and the criteria for improvement. Return only the revised answer, without any additional comments or explanation. ```python\n",
      "import unittest\n",
      "from unittest.mock import patch, MagicMock\n",
      "from pathlib import Path\n",
      "\n",
      "class TestCodeGarden(unittest.TestCase):\n",
      "\n",
      "    @patch('os.getenv')\n",
      "    def test_find_garden_default(self, mock_getenv):\n",
      "        mock_getenv.return_value = None  # Simulate no environment variable set\n",
      "        garden = CodeGarden()\n",
      "        self.assertEqual(garden.path, Path.home() / '.code_garden')\n",
      "\n",
      "    @patch('os.getenv')\n",
      "    def test_find_garden_with_env(self, mock_getenv):\n",
      "        mock_getenv.return_value = '/custom/garden/path'\n",
      "        garden = CodeGarden()\n",
      "        self.assertEqual(garden.path, Path('/custom/garden/path'))\n",
      "\n",
      "    @patch('pathlib.Path.mkdir')\n",
      "    def test_init_creates_directory_if_not_exists(self, mock_mkdir):\n",
      "        garden = CodeGarden(Path('/test/path'))\n",
      "        mock_mkdir.assert_called_once_with(parents=True, exist_ok=True)\n",
      "\n",
      "class TestCodeFragment(unittest.TestCase):\n",
      "\n",
      "    def test_code_fragment_name_property(self):\n",
      "        fragment = CodeFragment(code='print(\"Hello World\")', tags=['greet'], area=GardenArea.main)\n",
      "        self.assertEqual(fragment.name, 'greet')\n",
      "\n",
      "class TestFindMethod(unittest.TestCase):\n",
      "\n",
      "    @patch.object(CodeGarden, '_find_paths', return_value=[Path('test.py')])\n",
      "    def test_find_method_returns_code_fragments(self, mock_find_paths):\n",
      "        \n",
      "         # Mocking the read_text method of the path object to return sample code.\n",
      "         with patch.object(Path('test.py'), 'read_text', return_value='print(\"Hello World\")'):\n",
      "            garden = CodeGarden()\n",
      "            fragments = garden.find(keys=['test'], area=GardenArea.main)\n",
      "            \n",
      "            expected_fragment_name = 'greet'  # Assuming tags are ['greet']\n",
      "            self.assertIn(expected_fragment_name , fragments)\n",
      "            self.assertIsInstance(fragments[expected_fragment_name], CodeFragment)\n",
      "\n",
      "class TestSaveMethod(unittest.TestCase):\n",
      "\n",
      "     @patch.object(CodeGardens,'_generate_path',return_value=Path('/mocked/path/test.py'))\n",
      "     def test_save_method_creates_file_and_writes_code(self,mock_generate_path): \n",
      "          with patch('builtins.open', new_callable=MagicMock) as mocked_open:\n",
      "              garden.save(code='print(\"Test Save\")',\n",
      "                          tags=['test'],\n",
      "                          area= GardenArea.inbox,\n",
      "                          metadata={'author': 'user'})\n",
      "              \n",
      "              mocked_open().write.assert_any_call('# metadata ')\n",
      "              mocked_open().write\n",
      "\n",
      " LEN:  334\n",
      "\n",
      " LEN:  334\n",
      "\n",
      " LEN:  334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 11/11 [00:13<00:00,  1.24s/it, eta=0h 0m 0s, loss=[0.11600193381309509, 0.316020819273862]]  \n",
      "Testing Epoch 0: 100%|██████████| 4/4 [00:01<00:00,  2.68it/s, eta=0h 0m 0s, loss=[2.421337127685547, 1.7997935116291046]] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Train Loss: 0.316020819273862. Val Loss: 1.7997935116291046.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = run_CITING(UR_train_long_prompts, train_long, long_r_0_list, test_prompts, test_completions, student, student_tokenizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f3af7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set_loaded = load_data_from_list(test_prompts, test_completions)\n",
    "# test_set_CL = CLDataset(test_set_loaded, student_tokenizer)\n",
    "# test_loader = DataLoader(test_set_CL, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"./citing_model\", torch_dtype=torch.float16,).to(device)\n",
    "# model.eval()\n",
    "# writer = SummaryWriter() \n",
    "# val_loss = test(model, test_loader, writer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2d36996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\"./citing_model\", torch_dtype=torch.float32)\n",
    "# revised_prompts = []\n",
    "# for item, r_0 in zip(test_raw, test_r_0_list):\n",
    "#     x, y, c = item['x'], item['y'], item['c']\n",
    "#     revised_prompt = create_to_revise(x, c, r_0)\n",
    "#     revised_prompts.append(revised_prompt)\n",
    "\n",
    "\n",
    "# # outputs_list = student_generate_batch(4, revised_prompts[:4], model, student_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95873f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"revised_prompts.txt\", \"w\") as f:\n",
    "#     json.dump(revised_prompts, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
