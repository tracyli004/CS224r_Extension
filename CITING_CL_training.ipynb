{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "721bffa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "# from peft import PeftModel\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "# import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from vllm import LLM, SamplingParams\n",
    "# import openai\n",
    "import time\n",
    "import tensorboard\n",
    "import pickle\n",
    "import math\n",
    "import json\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_printoptions(threshold=float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f799aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade \"jinja2>=3.1.0\"\n",
    "# %pip install datasets tensorboard openai tqdm\n",
    "# %pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed2813a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 22:21:28.335774: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749421288.345302   22991 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749421288.350362   22991 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# load tokenizer and model\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\", padding_side='right')\n",
    "student = AutoModelForCausalLM.from_pretrained(\"./dpo_model\", torch_dtype=torch.float32)\n",
    "\n",
    "# add special tokens\n",
    "student_tokenizer.add_special_tokens({\n",
    "    'pad_token': '<|pad|>',\n",
    "    'bos_token': '<|im_start|>',\n",
    "    'eos_token': '<|im_end|>',\n",
    "})\n",
    "\n",
    "# resize model embeddings to include new tokens\n",
    "student.resize_token_embeddings(len(student_tokenizer))\n",
    "# set token ids in config\n",
    "student.config.pad_token_id = student_tokenizer.pad_token_id\n",
    "student.config.bos_token_id = student_tokenizer.bos_token_id\n",
    "student.config.eos_token_id = student_tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36fdb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = (\n",
    "    \"{% set image_count = namespace(value=0) %}\"\n",
    "    \"{% set video_count = namespace(value=0) %}\"\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{% if loop.first and message['role'] != 'system' %}\"\n",
    "    \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "    \"{% endif %}\"\n",
    "    \"<|im_start|>{{ message['role'] }}\\n\"\n",
    "    \"{% if message['content'] is string %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}\"\n",
    "    \"{% generation %}\"\n",
    "    \"{{ message['content'] }}\"\n",
    "    \"{% endgeneration %}\"\n",
    "    \"{% else %}\"\n",
    "    \"{{ message['content'] }}\"\n",
    "    \"{% endif %}\"\n",
    "    \"<|im_end|>\\n\"\n",
    "    \"{% else %}\"\n",
    "    \"{% for content in message['content'] %}\"\n",
    "    \"{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}\"\n",
    "    \"{% set image_count.value = image_count.value + 1 %}\"\n",
    "    \"{% if add_vision_id %}\"\n",
    "    \"Picture {{ image_count.value }}: \"\n",
    "    \"{% endif %}\"\n",
    "    \"<|vision_start|><|image_pad|><|vision_end|>\"\n",
    "    \"{% elif content['type'] == 'video' or 'video' in content %}\"\n",
    "    \"{% set video_count.value = video_count.value + 1 %}\"\n",
    "    \"{% if add_vision_id %}\"\n",
    "    \"Video {{ video_count.value }}: \"\n",
    "    \"{% endif %}\"\n",
    "    \"<|vision_start|><|video_pad|><|vision_end|>\"\n",
    "    \"{% elif 'text' in content %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}\"\n",
    "    \"{% generation %}\"\n",
    "    \"{{ content['text'] }}\"\n",
    "    \"{% endgeneration %}\"\n",
    "    \"{% else %}\"\n",
    "    \"{{ content['text'] }}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"<|im_end|>\\n\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"<|im_start|>assistant\\n\"\n",
    "    \"{% endif %}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c0f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "MAX_TOKENS = 512\n",
    "\n",
    "MAX_INPUT_TOKENS = 512\n",
    "ACCUM_STEPS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e969cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length = MAX_INPUT_TOKENS):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        messages = self.dataset[idx]\n",
    "\n",
    "        new_messages = []\n",
    "        for m in messages:\n",
    "            if not new_messages and m[\"role\"] == \"user\":\n",
    "                new_messages.append(m)\n",
    "            elif new_messages and m[\"role\"] == \"assistant\":\n",
    "                new_messages.append(m)\n",
    "                break\n",
    "        if len(new_messages) != 2:\n",
    "            return self._empty_item()\n",
    "        \n",
    "        try:\n",
    "            tokenized = self.tokenizer.apply_chat_template(\n",
    "                new_messages,\n",
    "                tokenize = True,\n",
    "                max_length = self.max_length,\n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                return_dict = True,\n",
    "                return_assistant_tokens_mask=True,\n",
    "                add_generation_prompt = False,\n",
    "                chat_template = chat_template,\n",
    "                return_tensors = 'pt'\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(idx, e)\n",
    "            return self._empty_item()\n",
    "\n",
    "        input_ids = tokenized['input_ids']\n",
    "        assistant_masks = tokenized['assistant_masks']\n",
    "        if assistant_masks.sum() == 0:\n",
    "            return self._empty_item()\n",
    "\n",
    "        mod_assistant_mask = assistant_masks.clone()\n",
    "        matches = (input_ids == self.tokenizer.convert_tokens_to_ids(\"<|im_end|>\"))\n",
    "        indices = torch.nonzero(matches)\n",
    "        mod_assistant_mask[tuple(indices[-1])] = 1 # inlcude end speaking token in assistant to include in lables\n",
    "\n",
    "        attention_mask = tokenized['attention_mask']\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[mod_assistant_mask == 0] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids.squeeze(0),\n",
    "            'attention_mask': attention_mask.squeeze(0),\n",
    "            'labels': labels.squeeze(0)\n",
    "        }\n",
    "    \n",
    "    def _empty_item(self):\n",
    "        return {\n",
    "            'input_ids': torch.zeros(self.max_length, dtype=torch.int32),\n",
    "            'attention_mask': torch.zeros(self.max_length, dtype=torch.int32),\n",
    "            'labels': torch.full((self.max_length,), -100, dtype=torch.int32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94c40e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_generate_batch(batch_size, prompts, model, tokenizer):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\", padding_side = 'left')\n",
    "    outputs_list = []\n",
    "\n",
    "    for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "        batch = prompts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        output_sequences = model.generate(\n",
    "            input_ids=inputs['input_ids'].to(model.device),\n",
    "            attention_mask=inputs['attention_mask'].to(model.device),\n",
    "            tokenizer = tokenizer,\n",
    "            do_sample=False, # disable sampling to test if batching affects output\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            forced_eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.12,\n",
    "            stop_strings = '<|im_end|>',\n",
    "            exponential_decay_length_penalty = (int(MAX_TOKENS * 0.7),1.1),\n",
    "            max_new_tokens= MAX_TOKENS\n",
    "        )\n",
    "        completions_only = output_sequences[:, inputs['input_ids'].shape[1]:]\n",
    "        outputs_decoded = tokenizer.batch_decode(completions_only, skip_special_tokens=True)\n",
    "        # print(output_completions)\n",
    "        # print(output_sequences)\n",
    "        outputs_list.extend(outputs_decoded)\n",
    "    return outputs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1250d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_prompt(prompt, max_input_tokens=600):\n",
    "    tokens = student_tokenizer(prompt)[\"input_ids\"]\n",
    "    if len(tokens) > max_input_tokens:\n",
    "        tokens = tokens[:max_input_tokens]\n",
    "        return student_tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04274f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "def teacher_generate_batch(prompts, model=\"o4-mini-2025-04-16\", system_prompt=\"You are a helpful assistant.\"):\n",
    "    # Step 1: Write prompts to JSONL\n",
    "    input_path = \"batch_input.jsonl\"\n",
    "    with open(input_path, \"w\") as f:\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            print(\"Idx \", i)\n",
    "            trunc = truncate_prompt(prompt)\n",
    "            # time.sleep(3)\n",
    "            # print(prompt)\n",
    "            item = {\n",
    "                \"custom_id\": f\"request-{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": trunc}\n",
    "                    ],\n",
    "                    \"max_tokens\": MAX_TOKENS,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"stop\": [\"<|im_end|>\"],\n",
    "                    \"frequency_penalty\": 1.5,\n",
    "                    \"presence_penalty\": 0.0\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "    print(\"Wrote in prompts to json.\")\n",
    "    # Step 2: Upload the file using OpenAI client\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        upload = client.files.create(file=f, purpose=\"batch\")\n",
    "    file_id = upload.id\n",
    "\n",
    "    # Step 3: Submit batch\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\"\n",
    "    )\n",
    "    batch_id = batch.id\n",
    "    print(\"Batch submitted. ID:\", batch_id)\n",
    "\n",
    "    # Step 4: Poll until complete\n",
    "    print(\"Waiting for batch to complete...\")\n",
    "    while True:\n",
    "        batch_status = client.batches.retrieve(batch_id)\n",
    "        status = batch_status.status\n",
    "        # print(f\"Current status: {status}\")\n",
    "        if status in [\"completed\", \"failed\", \"cancelled\", \"expired\"]:\n",
    "            break\n",
    "        time.sleep(15)\n",
    "\n",
    "    if status != \"completed\":\n",
    "        raise RuntimeError(f\"Batch failed or didn't complete: {status}\")\n",
    "\n",
    "    # Step 5: Download result file\n",
    "    output_file_id = batch_status.output_file_id\n",
    "    output_response = client.files.content(output_file_id)\n",
    "    output_data = output_response.text\n",
    "\n",
    "    # Step 6: Parse results into dictionary\n",
    "    responses = {}\n",
    "    for line in output_data.splitlines():\n",
    "        obj = json.loads(line)\n",
    "        custom_id = obj[\"custom_id\"]\n",
    "        content = obj[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        responses[custom_id] = content\n",
    "\n",
    "    # Step 7: Return outputs in order\n",
    "    return [responses[f\"request-{i}\"] for i in range(len(prompts))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d76e62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_raw):\n",
    "    formatted_data = [[{\"content\": str(item[\"x\"]), \"role\": \"user\"},\n",
    "                       {\"content\": str(item[\"y\"]), \"role\": \"assistant\"}] for item in train_raw ]\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "088524b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_list(prompt, completion):\n",
    "    data = [[\n",
    "        {'content': p, 'role': 'user'},\n",
    "        {'content': c, 'role': 'assistant'}\n",
    "    ] for p, c in zip(prompt, completion)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "072f4ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_to_revise(x, c, r_0):\n",
    "    prompt = (\n",
    "        f\"Below is an instruction and my initial response. A criteria for evaluating the response is also provided.\\n\\n\"\n",
    "        f\"Instruction:\\n{x}\\n\\n\"\n",
    "        f\"My Initial Response:\\n{r_0}\\n\\n\"\n",
    "        f\"Criteria: {c}\\n\\n\"\n",
    "        f\"My initial response may be incorrect and may not follow the criteria. Please revise it using the ideal response as a guide and the criteria for improvement. \"\n",
    "        f\"Return only the revised answer, without any additional comments or explanation.\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55cc206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_revisions(r_0_list, raw_data):\n",
    "    \n",
    "    # student.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     r_0_list = student_generate_batch(batch_size=BATCH_SIZE, prompts=prompts, model=student, tokenizer=student_tokenizer)\n",
    "\n",
    "    revised_prompts = []\n",
    "    revisions = []\n",
    "    for item, r_0 in zip(raw_data, r_0_list):\n",
    "        x, y, c = item['x'], item['y'], item['c']\n",
    "        revised_prompt = create_to_revise(x, c, r_0)\n",
    "        revised_prompts.append(revised_prompt)\n",
    "        # revisions.append(y)\n",
    "        \n",
    "    # print(len(revised_prompts))\n",
    "\n",
    "    # 2. teacher revises (no grad)\n",
    "    for i in range(0, len(revised_prompts), 500):\n",
    "        print(\"\\n\\n batch: \", i)\n",
    "        batched_prompts = revised_prompts[i:i+500]\n",
    "        revisions.extend(teacher_generate_batch(batched_prompts, model=\"gpt-4o-mini\", system_prompt=\"You are an expert writer.\"))\n",
    "        time.sleep(180)\n",
    "    # revisions = teacher_generate_batch(revised_prompts, model=\"gpt-4o-mini\", system_prompt=\"You are an expert writer.\")\n",
    "\n",
    "    # print(revised_prompts[332], revisions[332])\n",
    "    return revised_prompts, revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07ef518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train(model, train_loader, optimizer, writer, epoch):\n",
    "\n",
    "    total_loss = 0\n",
    "    batch_times = []\n",
    "    progress = tqdm(train_loader, desc=f\"Training Epoch {epoch}\", leave=True)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, batch in enumerate(progress):\n",
    "        start = time.time()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        if torch.all(labels == -100) or torch.all(input_ids == 0):\n",
    "            print(f\"⏭️  Skipping empty batch {i}\")\n",
    "            continue\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss  # normalize loss for accumulation\n",
    "                \n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "        # assert not math.isnan(loss.item()), f'Loss: {loss}, \\nOutputs: {outputs}'\n",
    "        loss.backward()\n",
    "    \n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        optimizer.step()\n",
    "        torch.cuda.synchronize()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()  \n",
    "        avg_loss = total_loss / (i + 1)\n",
    "\n",
    "\n",
    "        batch_time = time.time() - start\n",
    "        batch_times.append(batch_time)\n",
    "        avg_time = sum(batch_times) / len(batch_times)\n",
    "        eta = avg_time * (len(train_loader) - (i + 1))\n",
    "        eta_hr, remainder = divmod(int(eta), 3600)\n",
    "        eta_min, eta_sec = divmod(remainder, 60)\n",
    "\n",
    "        progress.set_postfix(loss=[loss.item(), avg_loss], eta=f\"{eta_hr}h {eta_min}m {eta_sec}s\")\n",
    "\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            model.save_pretrained('./latest_model')\n",
    "            with open('latest_opt.pkl', 'wb') as f:\n",
    "                pickle.dump(optimizer, f)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    writer.add_scalar(\"Loss/train\", avg_loss, epoch * len(train_loader) + i)\n",
    "    return float(avg_loss)\n",
    "\n",
    "\n",
    "def test(model, test_loader, writer, epoch, return_generations=False):\n",
    "    total_loss = 0\n",
    "    batch_times = []\n",
    "    progress = tqdm(test_loader, desc=f\"Testing Epoch {epoch}\", leave=True)\n",
    "\n",
    "    for i, batch in enumerate(progress):\n",
    "        start = time.time()\n",
    "        \n",
    "        input_ids = batch['input_ids'].long().to(device)\n",
    "        attention_mask = batch['attention_mask'].long().to(device)\n",
    "        labels = batch['labels'].long().to(device)\n",
    "        \n",
    "        if torch.all(labels == -100) or torch.all(input_ids == 0):\n",
    "            print(f\"⏭️  Skipping empty batch {i}\")\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        if math.isnan(loss):\n",
    "            print(\"NAN loss\")\n",
    "            continue\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        avg_loss = float(total_loss) / (i + 1)\n",
    "\n",
    "\n",
    "        batch_time = time.time() - start\n",
    "        batch_times.append(batch_time)\n",
    "        avg_time = sum(batch_times) / len(batch_times)\n",
    "        eta = avg_time * (len(test_loader) - (i + 1))\n",
    "        eta_hr, remainder = divmod(int(eta), 3600)\n",
    "        eta_min, eta_sec = divmod(remainder, 60)\n",
    "\n",
    "        progress.set_postfix(loss=[loss.item(), avg_loss], eta=f\"{eta_hr}h {eta_min}m {eta_sec}s\")\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            model.save_pretrained('./checkpoints/latest_step')\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    writer.add_scalar(\"Loss/val\", avg_loss, epoch)\n",
    "    return float(avg_loss)\n",
    "\n",
    "\n",
    "def fine_tune(model, train_loader, test_loader, optimizer, num_epochs):\n",
    "    writer = SummaryWriter()  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, optimizer, writer, epoch)\n",
    "        # model.save_pretrained('./citing_model')\n",
    "        val_loss = test(model, test_loader, writer, epoch)\n",
    "        print(f'Epoch: {epoch}. Train Loss: {train_loss}. Val Loss: {val_loss}.')\n",
    "        \n",
    "    # model.save_pretrained('./citing_model')\n",
    "\n",
    "    writer.close()\n",
    "    return train_loss, val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11713d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CITING(train_prompts, train_raw, r_0_list, test_prompts, test_completions, student, tokenizer, num_epochs):\n",
    "    optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, student.parameters()), lr=1e-6)\n",
    "    \n",
    "    train_prompts, train_completions = get_revisions(r_0_list, train_raw)\n",
    "    print(\"\\n LEN: \", len(train_raw))\n",
    "    print(\"\\n LEN: \", len(train_prompts))\n",
    "    train_set_loaded = load_data_from_list(train_prompts, train_completions)\n",
    "    print(\"\\n LEN: \", len(train_set_loaded))\n",
    "    train_set_CL = CLDataset(train_set_loaded, tokenizer)\n",
    "    train_loader = DataLoader(train_set_CL, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    \n",
    "    # test_prompts, test_completions = get_revisions(r_0_list, train_raw)\n",
    "    test_set_loaded = load_data_from_list(test_prompts, test_completions)\n",
    "    test_set_CL = CLDataset(test_set_loaded, tokenizer)\n",
    "    test_loader = DataLoader(test_set_CL, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    \n",
    "    train_loss, val_loss = fine_tune(\n",
    "        model = student,\n",
    "        train_loader = train_loader,\n",
    "        test_loader = test_loader,\n",
    "        optimizer = optim,\n",
    "        num_epochs = num_epochs\n",
    "    )\n",
    "\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8bcab36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student.to(device)\n",
    "print(student.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdae23df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_short.json\", \"r\") as f:\n",
    "    train_short = json.load(f)\n",
    "with open(\"train_med.json\", \"r\") as f:\n",
    "    train_med = json.load(f)\n",
    "with open(\"train_long.json\", \"r\") as f:\n",
    "    train_long = json.load(f)\n",
    "with open(\"test.json\", \"r\") as f:\n",
    "    test_raw = json.load(f)\n",
    "    \n",
    "train_short_dataloaded = load_data(train_short)\n",
    "train_med_dataloaded = load_data(train_med)\n",
    "train_long_dataloaded = load_data(train_long)\n",
    "test_dataloaded = load_data(test_raw)\n",
    "\n",
    "UR_train_short_prompts = [item[0][\"content\"] for item in train_short_dataloaded]\n",
    "UR_train_med_prompts = [item[0][\"content\"] for item in train_med_dataloaded]\n",
    "UR_train_long_prompts = [item[0][\"content\"] for item in train_long_dataloaded]\n",
    "UR_test_prompts = [item[0][\"content\"] for item in test_dataloaded]\n",
    "\n",
    "\n",
    "with open(\"short_initial_responses.json\", \"r\") as f:\n",
    "    short_r_0_list = json.load(f)\n",
    "#short_r_0_list = [ex[\"completion\"] for ex in short_r_0_list_raw]\n",
    "\n",
    "with open(\"med_initial_responses.json\", \"r\") as f:\n",
    "    med_r_0_list = json.load(f)\n",
    "#med_r_0_list = [ex[\"completion\"] for ex in med_r_0_list_raw]\n",
    "\n",
    "with open(\"long_initial_responses.json\", \"r\") as f:\n",
    "    long_r_0_list = json.load(f)\n",
    "#long_r_0_list = [ex[\"completion\"] for ex in long_r_0_list_raw]\n",
    "\n",
    "with open(\"test_initial_responses.json\", \"r\") as f:\n",
    "    test_r_0_list = json.load(f)\n",
    "#test_r_0_list = [ex[\"completion\"] for ex in test_r_0_list_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6969ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLAMA Y for teacher\n",
    "# test_x = [item[\"x\"] for item in test_raw]\n",
    "# crit = [item[\"c\"] for item in test_raw]\n",
    "# test_prompts = [create_to_revise(x, c, r_0) for x, c, r_0 in zip(test_x, crit, test_r_0_list)]\n",
    "# test_completions = [item[\"y\"] for item in test_raw]\n",
    "\n",
    "\n",
    "# Use GPT as teacher\n",
    "# test_prompts, test_completions = get_revisions(test_r_0_list, test_raw)\n",
    "# with open(\"test_prompts.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(test_prompts, f)\n",
    "# with open(\"test_completions.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(test_completions, f)\n",
    "\n",
    "with open(\"test_prompts.pkl\", \"rb\") as f:\n",
    "    test_prompts = pickle.load(f)\n",
    "with open(\"test_completions.pkl\", \"rb\") as f:\n",
    "    test_completions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10a66f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction and my initial response. A criteria for evaluating the response is also provided.\\n\\nInstruction:\\nCan you draw an SVG logo of a tree?\\n\\nMy Initial Response:\\n I want to know how to create it.\\nSure, here\\'s the SVG code for a simple tree:\\n```\\n<svg width=\"100\" height=\"100\">\\n  <circle cx=\"50\" cy=\"50\" r=\"40\" fill=\"green\"/>\\n</svg>\\nThis SVG code will generate a basic tree shape with a green circle at its center. You can adjust the size and color of the circle as needed to fit your design requirements. \\nAlternatively, if you prefer a more detailed representation, you could use this code in conjunction with other elements such as text or images:\\n```xml\\n<svg xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\\n  <!-- Tree -->\\n  <g transform=\"translate(50 50)\">\\n    <circle cx=\"50\" cy=\"50\" r=\"40\" fill=\"green\"/>\\n    <!-- Add branches and leaves as necessary -->\\n  </g>\\n</svg>\\nIn this case, the tree would be represented by the circle itself, while the branches and leaves are added using additional shapes and paths within the SVG element. This approach allows for greater flexibility in creating complex designs. ``` \\nI hope this helps! Let me know if you have any further questions about SVG trees. \\nIf you\\'d like to explore other SVG formats, such as SVG 2.0 or SVG 3D, feel free to ask too. The possibilities are endless when working with SVGs. \\nAlso, if you\\'re interested in learning more about SVGs, I recommend checking out the official documentation: https://www.w3.org/TR/SVG/. \\nThe tree is just one example of many possible SVG shapes that can be created from this code. Feel free to modify it according to your needs\\n\\nCriteria: Good responses are original, creative, and relevant to the context. They should align with the goals (e.g., improving engagement, enhancing accessibility) and include actionable, well-reasoned ideas.\\n\\nMy initial response may be incorrect and may not follow the criteria. Please revise it using the ideal response as a guide and the criteria for improvement. Return only the revised answer, without any additional comments or explanation.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompts[499]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e154840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " batch:  0\n",
      "Idx  0\n",
      "Idx  1\n",
      "Idx  2\n",
      "Idx  3\n",
      "Idx  4\n",
      "Idx  5\n",
      "Idx  6\n",
      "Idx  7\n",
      "Idx  8\n",
      "Idx  9\n",
      "Idx  10\n",
      "Idx  11\n",
      "Idx  12\n",
      "Idx  13\n",
      "Idx  14\n",
      "Idx  15\n",
      "Idx  16\n",
      "Idx  17\n",
      "Idx  18\n",
      "Idx  19\n",
      "Idx  20\n",
      "Idx  21\n",
      "Idx  22\n",
      "Idx  23\n",
      "Idx  24\n",
      "Idx  25\n",
      "Idx  26\n",
      "Idx  27\n",
      "Idx  28\n",
      "Idx  29\n",
      "Idx  30\n",
      "Idx  31\n",
      "Idx  32\n",
      "Idx  33\n",
      "Idx  34\n",
      "Idx  35\n",
      "Idx  36\n",
      "Idx  37\n",
      "Idx  38\n",
      "Idx  39\n",
      "Idx  40\n",
      "Idx  41\n",
      "Idx  42\n",
      "Idx  43\n",
      "Idx  44\n",
      "Idx  45\n",
      "Idx  46\n",
      "Idx  47\n",
      "Idx  48\n",
      "Idx  49\n",
      "Idx  50\n",
      "Idx  51\n",
      "Idx  52\n",
      "Idx  53\n",
      "Idx  54\n",
      "Idx  55\n",
      "Idx  56\n",
      "Idx  57\n",
      "Idx  58\n",
      "Idx  59\n",
      "Idx  60\n",
      "Idx  61\n",
      "Idx  62\n",
      "Idx  63\n",
      "Idx  64\n",
      "Idx  65\n",
      "Idx  66\n",
      "Idx  67\n",
      "Idx  68\n",
      "Idx  69\n",
      "Idx  70\n",
      "Idx  71\n",
      "Idx  72\n",
      "Idx  73\n",
      "Idx  74\n",
      "Idx  75\n",
      "Idx  76\n",
      "Idx  77\n",
      "Idx  78\n",
      "Idx  79\n",
      "Idx  80\n",
      "Idx  81\n",
      "Idx  82\n",
      "Idx  83\n",
      "Idx  84\n",
      "Idx  85\n",
      "Idx  86\n",
      "Idx  87\n",
      "Idx  88\n",
      "Idx  89\n",
      "Idx  90\n",
      "Idx  91\n",
      "Idx  92\n",
      "Idx  93\n",
      "Idx  94\n",
      "Idx  95\n",
      "Idx  96\n",
      "Idx  97\n",
      "Idx  98\n",
      "Idx  99\n",
      "Idx  100\n",
      "Idx  101\n",
      "Idx  102\n",
      "Idx  103\n",
      "Idx  104\n",
      "Idx  105\n",
      "Idx  106\n",
      "Idx  107\n",
      "Idx  108\n",
      "Idx  109\n",
      "Idx  110\n",
      "Idx  111\n",
      "Idx  112\n",
      "Idx  113\n",
      "Idx  114\n",
      "Idx  115\n",
      "Idx  116\n",
      "Idx  117\n",
      "Idx  118\n",
      "Idx  119\n",
      "Idx  120\n",
      "Idx  121\n",
      "Idx  122\n",
      "Idx  123\n",
      "Idx  124\n",
      "Idx  125\n",
      "Idx  126\n",
      "Idx  127\n",
      "Idx  128\n",
      "Idx  129\n",
      "Idx  130\n",
      "Idx  131\n",
      "Idx  132\n",
      "Idx  133\n",
      "Idx  134\n",
      "Idx  135\n",
      "Idx  136\n",
      "Idx  137\n",
      "Idx  138\n",
      "Idx  139\n",
      "Idx  140\n",
      "Idx  141\n",
      "Idx  142\n",
      "Idx  143\n",
      "Idx  144\n",
      "Idx  145\n",
      "Idx  146\n",
      "Idx  147\n",
      "Idx  148\n",
      "Idx  149\n",
      "Idx  150\n",
      "Idx  151\n",
      "Idx  152\n",
      "Idx  153\n",
      "Idx  154\n",
      "Idx  155\n",
      "Idx  156\n",
      "Idx  157\n",
      "Idx  158\n",
      "Idx  159\n",
      "Idx  160\n",
      "Idx  161\n",
      "Idx  162\n",
      "Idx  163\n",
      "Idx  164\n",
      "Idx  165\n",
      "Idx  166\n",
      "Idx  167\n",
      "Idx  168\n",
      "Idx  169\n",
      "Idx  170\n",
      "Idx  171\n",
      "Idx  172\n",
      "Idx  173\n",
      "Idx  174\n",
      "Idx  175\n",
      "Idx  176\n",
      "Idx  177\n",
      "Idx  178\n",
      "Idx  179\n",
      "Idx  180\n",
      "Idx  181\n",
      "Idx  182\n",
      "Idx  183\n",
      "Idx  184\n",
      "Idx  185\n",
      "Idx  186\n",
      "Idx  187\n",
      "Idx  188\n",
      "Idx  189\n",
      "Idx  190\n",
      "Idx  191\n",
      "Idx  192\n",
      "Idx  193\n",
      "Idx  194\n",
      "Idx  195\n",
      "Idx  196\n",
      "Idx  197\n",
      "Idx  198\n",
      "Idx  199\n",
      "Idx  200\n",
      "Idx  201\n",
      "Idx  202\n",
      "Idx  203\n",
      "Idx  204\n",
      "Idx  205\n",
      "Idx  206\n",
      "Idx  207\n",
      "Idx  208\n",
      "Idx  209\n",
      "Idx  210\n",
      "Idx  211\n",
      "Idx  212\n",
      "Idx  213\n",
      "Idx  214\n",
      "Idx  215\n",
      "Idx  216\n",
      "Idx  217\n",
      "Idx  218\n",
      "Idx  219\n",
      "Idx  220\n",
      "Idx  221\n",
      "Idx  222\n",
      "Idx  223\n",
      "Idx  224\n",
      "Idx  225\n",
      "Idx  226\n",
      "Idx  227\n",
      "Idx  228\n",
      "Idx  229\n",
      "Idx  230\n",
      "Idx  231\n",
      "Idx  232\n",
      "Idx  233\n",
      "Idx  234\n",
      "Idx  235\n",
      "Idx  236\n",
      "Idx  237\n",
      "Idx  238\n",
      "Idx  239\n",
      "Idx  240\n",
      "Idx  241\n",
      "Idx  242\n",
      "Idx  243\n",
      "Idx  244\n",
      "Idx  245\n",
      "Idx  246\n",
      "Idx  247\n",
      "Idx  248\n",
      "Idx  249\n",
      "Idx  250\n",
      "Idx  251\n",
      "Idx  252\n",
      "Idx  253\n",
      "Idx  254\n",
      "Idx  255\n",
      "Idx  256\n",
      "Idx  257\n",
      "Idx  258\n",
      "Idx  259\n",
      "Idx  260\n",
      "Idx  261\n",
      "Idx  262\n",
      "Idx  263\n",
      "Idx  264\n",
      "Idx  265\n",
      "Idx  266\n",
      "Idx  267\n",
      "Idx  268\n",
      "Idx  269\n",
      "Idx  270\n",
      "Idx  271\n",
      "Idx  272\n",
      "Idx  273\n",
      "Idx  274\n",
      "Idx  275\n",
      "Idx  276\n",
      "Idx  277\n",
      "Idx  278\n",
      "Idx  279\n",
      "Idx  280\n",
      "Idx  281\n",
      "Idx  282\n",
      "Idx  283\n",
      "Idx  284\n",
      "Idx  285\n",
      "Idx  286\n",
      "Idx  287\n",
      "Idx  288\n",
      "Idx  289\n",
      "Idx  290\n",
      "Idx  291\n",
      "Idx  292\n",
      "Idx  293\n",
      "Idx  294\n",
      "Idx  295\n",
      "Idx  296\n",
      "Idx  297\n",
      "Idx  298\n",
      "Idx  299\n",
      "Idx  300\n",
      "Idx  301\n",
      "Idx  302\n",
      "Idx  303\n",
      "Idx  304\n",
      "Idx  305\n",
      "Idx  306\n",
      "Idx  307\n",
      "Idx  308\n",
      "Idx  309\n",
      "Idx  310\n",
      "Idx  311\n",
      "Idx  312\n",
      "Idx  313\n",
      "Idx  314\n",
      "Idx  315\n",
      "Idx  316\n",
      "Idx  317\n",
      "Idx  318\n",
      "Idx  319\n",
      "Idx  320\n",
      "Idx  321\n",
      "Idx  322\n",
      "Idx  323\n",
      "Idx  324\n",
      "Idx  325\n",
      "Idx  326\n",
      "Idx  327\n",
      "Idx  328\n",
      "Idx  329\n",
      "Idx  330\n",
      "Idx  331\n",
      "Idx  332\n",
      "Idx  333\n",
      "Idx  334\n",
      "Idx  335\n",
      "Idx  336\n",
      "Idx  337\n",
      "Idx  338\n",
      "Idx  339\n",
      "Idx  340\n",
      "Idx  341\n",
      "Idx  342\n",
      "Idx  343\n",
      "Idx  344\n",
      "Idx  345\n",
      "Idx  346\n",
      "Idx  347\n",
      "Idx  348\n",
      "Idx  349\n",
      "Idx  350\n",
      "Idx  351\n",
      "Idx  352\n",
      "Idx  353\n",
      "Idx  354\n",
      "Idx  355\n",
      "Idx  356\n",
      "Idx  357\n",
      "Idx  358\n",
      "Idx  359\n",
      "Idx  360\n",
      "Idx  361\n",
      "Idx  362\n",
      "Idx  363\n",
      "Idx  364\n",
      "Idx  365\n",
      "Idx  366\n",
      "Idx  367\n",
      "Idx  368\n",
      "Idx  369\n",
      "Idx  370\n",
      "Idx  371\n",
      "Idx  372\n",
      "Idx  373\n",
      "Idx  374\n",
      "Idx  375\n",
      "Idx  376\n",
      "Idx  377\n",
      "Idx  378\n",
      "Idx  379\n",
      "Idx  380\n",
      "Idx  381\n",
      "Idx  382\n",
      "Idx  383\n",
      "Idx  384\n",
      "Idx  385\n",
      "Idx  386\n",
      "Idx  387\n",
      "Idx  388\n",
      "Idx  389\n",
      "Idx  390\n",
      "Idx  391\n",
      "Idx  392\n",
      "Idx  393\n",
      "Idx  394\n",
      "Idx  395\n",
      "Idx  396\n",
      "Idx  397\n",
      "Idx  398\n",
      "Idx  399\n",
      "Idx  400\n",
      "Idx  401\n",
      "Idx  402\n",
      "Idx  403\n",
      "Idx  404\n",
      "Idx  405\n",
      "Idx  406\n",
      "Idx  407\n",
      "Idx  408\n",
      "Idx  409\n",
      "Idx  410\n",
      "Idx  411\n",
      "Idx  412\n",
      "Idx  413\n",
      "Idx  414\n",
      "Idx  415\n",
      "Idx  416\n",
      "Idx  417\n",
      "Idx  418\n",
      "Idx  419\n",
      "Idx  420\n",
      "Idx  421\n",
      "Idx  422\n",
      "Idx  423\n",
      "Idx  424\n",
      "Idx  425\n",
      "Idx  426\n",
      "Idx  427\n",
      "Idx  428\n",
      "Idx  429\n",
      "Idx  430\n",
      "Idx  431\n",
      "Idx  432\n",
      "Idx  433\n",
      "Idx  434\n",
      "Idx  435\n",
      "Idx  436\n",
      "Idx  437\n",
      "Idx  438\n",
      "Idx  439\n",
      "Idx  440\n",
      "Idx  441\n",
      "Idx  442\n",
      "Idx  443\n",
      "Idx  444\n",
      "Idx  445\n",
      "Idx  446\n",
      "Idx  447\n",
      "Idx  448\n",
      "Idx  449\n",
      "Idx  450\n",
      "Idx  451\n",
      "Idx  452\n",
      "Idx  453\n",
      "Idx  454\n",
      "Idx  455\n",
      "Idx  456\n",
      "Idx  457\n",
      "Idx  458\n",
      "Idx  459\n",
      "Idx  460\n",
      "Idx  461\n",
      "Idx  462\n",
      "Idx  463\n",
      "Idx  464\n",
      "Idx  465\n",
      "Idx  466\n",
      "Idx  467\n",
      "Idx  468\n",
      "Idx  469\n",
      "Idx  470\n",
      "Idx  471\n",
      "Idx  472\n",
      "Idx  473\n",
      "Idx  474\n",
      "Idx  475\n",
      "Idx  476\n",
      "Idx  477\n",
      "Idx  478\n",
      "Idx  479\n",
      "Idx  480\n",
      "Idx  481\n",
      "Idx  482\n",
      "Idx  483\n",
      "Idx  484\n",
      "Idx  485\n",
      "Idx  486\n",
      "Idx  487\n",
      "Idx  488\n",
      "Idx  489\n",
      "Idx  490\n",
      "Idx  491\n",
      "Idx  492\n",
      "Idx  493\n",
      "Idx  494\n",
      "Idx  495\n",
      "Idx  496\n",
      "Idx  497\n",
      "Idx  498\n",
      "Idx  499\n",
      "Wrote in prompts to json.\n",
      "Batch submitted. ID: batch_6846108e384c819090eacb152d089de2\n",
      "Waiting for batch to complete...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Batch failed or didn't complete: failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22991/2806116587.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_CITING\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUR_train_short_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_short\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshort_r_0_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_completions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_22991/2707988941.py\u001b[0m in \u001b[0;36mrun_CITING\u001b[0;34m(train_prompts, train_raw, r_0_list, test_prompts, test_completions, student, tokenizer, num_epochs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_completions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_revisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_0_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n LEN: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n LEN: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_prompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22991/3281760093.py\u001b[0m in \u001b[0;36mget_revisions\u001b[0;34m(r_0_list, raw_data)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n batch: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mbatched_prompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrevised_prompts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mrevisions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_generate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o-mini\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"You are an expert writer.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m180\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# revisions = teacher_generate_batch(revised_prompts, model=\"gpt-4o-mini\", system_prompt=\"You are an expert writer.\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22991/1659058414.py\u001b[0m in \u001b[0;36mteacher_generate_batch\u001b[0;34m(prompts, model, system_prompt)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"completed\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch failed or didn't complete: {status}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Step 5: Download result file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Batch failed or didn't complete: failed"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = run_CITING(UR_train_short_prompts, train_short, short_r_0_list, test_prompts, test_completions, student, student_tokenizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d262e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = run_CITING(UR_train_med_prompts, train_med, med_r_0_list, test_prompts, test_completions, student, student_tokenizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13096794",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = run_CITING(UR_train_long_prompts, train_long, long_r_0_list, test_prompts, test_completions, student, student_tokenizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3af7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set_loaded = load_data_from_list(test_prompts, test_completions)\n",
    "# test_set_CL = CLDataset(test_set_loaded, student_tokenizer)\n",
    "# test_loader = DataLoader(test_set_CL, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"./citing_model\", torch_dtype=torch.float16,).to(device)\n",
    "# model.eval()\n",
    "# writer = SummaryWriter() \n",
    "# val_loss = test(model, test_loader, writer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(raw, student, tokenizer):\n",
    "    prompts = [item[\"x\"] for item in raw]\n",
    "    outputs_list = student_generate_batch(8, prompts, student, tokenizer)\n",
    "    return outputs_list\n",
    "\n",
    "def generate_initial_responses_batched(short_raw, med_raw, long_raw, test_raw, student, tokenizer, batch_size=100) :\n",
    "    length = len(train_short)\n",
    "    test_length = len(test_raw)\n",
    "    \n",
    "    short_initial_responses = []\n",
    "    med_initial_responses = []\n",
    "    long_initial_responses = []\n",
    "    test_initial_responses = []\n",
    "    \n",
    "    for i in range(0, length, batch_size):\n",
    "        short_initial_responses.extend(generate_batch(short_raw[i:i+batch_size], student, tokenizer))\n",
    "        med_initial_responses.extend(generate_batch(med_raw[i:i+batch_size], student, tokenizer))\n",
    "        long_initial_responses.extend(generate_batch(long_raw[i:i+batch_size], student, tokenizer))\n",
    "        \n",
    "        with open(\"short_initial_responses.json\", \"w\") as f:\n",
    "            json.dump(short_initial_responses, f, indent=4)\n",
    "        with open(\"med_initial_responses.json\", \"w\") as f:\n",
    "            json.dump(med_initial_responses, f, indent=4)\n",
    "        with open(\"long_initial_responses.json\", \"w\") as f:\n",
    "            json.dump(long_initial_responses, f, indent=4)\n",
    "        \n",
    "        if i < test_length:\n",
    "            test_initial_responses.extend(generate_batch(test_raw[i:i+batch_size], student, tokenizer))\n",
    "            with open(\"test_initial_responses.json\", \"w\") as f:\n",
    "                json.dump(test_initial_responses, f, indent=4)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61437342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_initial_responses_batched(train_short, train_med, train_long, test_raw, student, student_tokenizer, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
