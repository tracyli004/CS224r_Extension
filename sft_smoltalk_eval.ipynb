{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5858b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824b6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b11c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and shuffle the full test split\n",
    "smoltalk_test_raw = load_dataset(\"HuggingFaceTB/smol-smoltalk\", split=\"test\").shuffle(seed=42)\n",
    "smoltalk_test_sample = smoltalk_test_raw.select(range(100))\n",
    "\n",
    "# Extract user prompts only\n",
    "def extract_user_prompt(messages):\n",
    "    for msg in messages:\n",
    "        if msg['role'] == 'user':\n",
    "            return msg['content']\n",
    "    return None\n",
    "\n",
    "eval_prompts = [extract_user_prompt(example['messages']) for example in smoltalk_test_sample if extract_user_prompt(example['messages'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e25b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL = \"./nonlora_sft_smoltalk_9k_b8_lr1e-5_ga32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3d5d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "sft_tokenizer.padding_side='left'\n",
    "sft_tokenizer.add_special_tokens({'pad_token': '<|pad|>',\n",
    "                              'bos_token': '<|im_start|>',\n",
    "                              'eos_token': '<|im_end|>'})\n",
    "\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(PRETRAINED_MODEL, torch_dtype=torch.float16,).to(device)\n",
    "sft_model.config.pad_token_id=sft_tokenizer.pad_token_id\n",
    "sft_model.config.eos_token_id=sft_tokenizer.eos_token_id\n",
    "sft_model.config.bos_token_id=sft_tokenizer.bos_token_id\n",
    "sft_model.eval()\n",
    "\n",
    "# reference model\n",
    "ref_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", torch_dtype=torch.float16,).to(device)\n",
    "ref_tokenizer.padding_side='left'\n",
    "ref_tokenizer.pad_token_id=ref_tokenizer.eos_token_id\n",
    "ref_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c10a8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\", \n",
    "    api_key=\"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\"  # MY KEY\n",
    ")\n",
    "\n",
    "def get_reward_score(prompt, response):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"nvidia/llama-3.1-nemotron-70b-reward\",\n",
    "        messages=messages\n",
    "    )\n",
    "    content = result.choices[0].message.content.strip()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "426db850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_end|>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9891607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, prompts, model, tokenizer, type = 'instruct'):\n",
    "    \n",
    "    rep_penalty = 1 if type == 'instruct' else 1 + 1e-5\n",
    "    \n",
    "    outputs_list = []\n",
    "    for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "        batch = prompts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=inputs['input_ids'].to(model.device),\n",
    "            attention_mask=inputs['attention_mask'].to(model.device),\n",
    "            do_sample=False, # disable sampling to test if batching affects output\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            forced_eos_token_id=tokenizer.eos_token_id,\n",
    "            tokenizer=tokenizer,\n",
    "            repetition_penalty=rep_penalty,\n",
    "            stop_strings = tokenizer.eos_token,\n",
    "            exponential_decay_length_penalty = (300,2),\n",
    "            max_new_tokens= 590\n",
    "        )\n",
    "        completions_only = output_sequences[:, inputs['input_ids'].shape[1]:]\n",
    "        outputs_decoded = tokenizer.batch_decode(completions_only, skip_special_tokens=True)\n",
    "        # print(output_completions)\n",
    "        # print(output_sequences)\n",
    "        outputs_list.extend(outputs_decoded)\n",
    "    return outputs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c375594d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:31<00:00,  7.02s/it]\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "  8%|▊         | 1/13 [00:06<01:14,  6.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 15%|█▌        | 2/13 [00:20<01:58, 10.74s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 23%|██▎       | 3/13 [00:26<01:27,  8.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 31%|███       | 4/13 [00:32<01:10,  7.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 38%|███▊      | 5/13 [00:39<00:58,  7.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 46%|████▌     | 6/13 [00:45<00:49,  7.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 54%|█████▍    | 7/13 [00:52<00:40,  6.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 62%|██████▏   | 8/13 [00:58<00:33,  6.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 69%|██████▉   | 9/13 [01:04<00:26,  6.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 77%|███████▋  | 10/13 [01:11<00:19,  6.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 85%|████████▍ | 11/13 [01:22<00:15,  7.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      " 92%|█████████▏| 12/13 [01:29<00:07,  7.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "100%|██████████| 13/13 [01:35<00:00,  7.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "sft_wins = 0\n",
    "total_evals = 0\n",
    "\n",
    "sft_completions = generate_batch(BATCH_SIZE, eval_prompts, sft_model, sft_tokenizer, type = 'sft')\n",
    "ref_completions = generate_batch(BATCH_SIZE, eval_prompts, ref_model, ref_tokenizer, type = 'instruct')\n",
    "# with open('ref_completions.pkl', 'rb') as f:\n",
    "#     ref_completions = pickle.load(f)\n",
    "\n",
    "for prompt, sft_response, ref_response in zip(eval_prompts, sft_completions, ref_completions):\n",
    "    sft_reward = get_reward_score(prompt, sft_response)\n",
    "    sft_reward = float(sft_reward.split(':')[-1])\n",
    "    ref_reward = get_reward_score(prompt, ref_response)\n",
    "    ref_reward = float(ref_reward.split(':')[-1])\n",
    "    \n",
    "    if sft_reward > ref_reward:\n",
    "        sft_wins += 1\n",
    "        \n",
    "    total_evals += 1\n",
    "    \n",
    "winrate = sft_wins/total_evals\n",
    "print(winrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2fc48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a193792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\n\\nDr. Emily Carter, a workshop workshop, reviewed feedback forms and suggested a more structured approach, but the plan deviated from the plan. Emily, a workshop workshop, suggested a more structured approach, but the feedback forms were not satisfied. Emily, a workshop workshop, reviewed feedback forms and suggested a more structured approach, but the feedback forms were not satisfied. Emily, a workshop workshop, suggested a more structured approach, but the feedback forms were not satisfied. Emily, a workshop workshop, suggested a more structured approach, but the feedback forms were not satisfied. Emily, a workshop workshop, suggested a more structured approach, but the feedback forms were not satisfied. Emily, a workshop workshop, suggested a more structured approach, but the feedback forms were not satisfied. Emily, a workshop workshop, suggested a more structured approach, but the feedback forms were not satisfied. Emily, a workshop workshop, suggested a more structured approach, but the feedback forms were not satisfied. Emily, a workshop workshop, suggested a more structured approach, but the feedback forms were not satisfied. Emily, a workshop workshop, suggested a more structured approach, but the feedback forms were not satisfied. Emily, a workshop workshop, suggested a more structured approach, but the feedback forms were not satisfied. Emily, a workshop workshop, suggested a more structured approach, but the feedback forms were not satisfied. Emily, a workshop workshop, suggested a more structured approach, but the feedback forms were not satisfied. Emily, a workshop workshop, suggested a more structured approach, but the feedback forms were not satisfied. Emily, a workshop workshop'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1814d4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nCan you please provide me with a summary of the feedback forms and the workshop's structure? Also, can you suggest a more structured approach to the feedback forms? Lastly, can you provide me with a sample of the feedback forms? \\n\\nThank you for your help.\\n\\nBest,\\nDr. Emily Carter\\n\\nSure, here is a summary of the feedback forms and the workshop's structure:\\n\\nFeedback Forms:\\n\\n1. Participant Feedback Form: This form is used to gather feedback from participants on the workshop. It includes questions about the workshop's content, delivery, and overall experience. The feedback forms are designed to be open and honest, allowing participants to share their thoughts and concerns.\\n\\n2. Workshop Structure Form: This form is used to outline the structure of the workshop, including the topics, activities, and delivery methods. The feedback forms are used to gather feedback on the workshop structure, ensuring that it meets the needs of the participants.\\n\\nTo address this issue, I would suggest the following:\\n\\n1. Provide more structured feedback forms: Instead of relying on open-ended questions, we can use structured feedback forms that are clear, concise, and easy to understand. These forms can be designed to gather feedback on specific aspects of the workshop, such as the delivery, content, or structure.\\n\\n2. Encourage participants to provide feedback: We can encourage participants to provide feedback on the workshop by asking them to complete the feedback forms. This will help us gather more accurate feedback and ensure that the workshop is meeting the needs of the participants\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b960c133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-34.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38374b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-28.75"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e83f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21642546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# from vllm import LLM, SamplingParams\n",
    "\n",
    "# # Sample prompts.\n",
    "# prompts = [\n",
    "#     \"Hello, my name is\",\n",
    "#     \"The president of the United States is\",\n",
    "#     \"The capital of France is\",\n",
    "#     \"The future of AI is\",\n",
    "# ]\n",
    "# # Create a sampling params object.\n",
    "# sampling_params = SamplingParams(repetition_penalty=1.5, max_tokens=590)\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     # Create an LLM.\n",
    "#     llm = LLM(model=\"facebook/opt-125m\")\n",
    "    \n",
    "#     llm = LLM(model=\"./finished_smoltalk\")\n",
    "#     # Generate texts from the prompts.\n",
    "#     # The output is a list of RequestOutput objects\n",
    "#     # that contain the prompt, generated text, and other information.\n",
    "#     outputs = llm.generate(prompts, sampling_params)\n",
    "#     # Print the outputs.\n",
    "#     print(\"\\nGenerated Outputs:\\n\" + \"-\" * 60)\n",
    "#     for output in outputs:\n",
    "#         prompt = output.prompt\n",
    "#         generated_text = output.outputs[0].text\n",
    "#         print(f\"Prompt:    {prompt!r}\")\n",
    "#         print(f\"Output:    {generated_text!r}\")\n",
    "#         print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec9c8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ray\n",
    "# from ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor\n",
    "# import numpy as np\n",
    "\n",
    "# config = vLLMEngineProcessorConfig(\n",
    "#     model_source=\"unsloth/Llama-3.1-8B-Instruct\",\n",
    "#     engine_kwargs={\n",
    "#         \"enable_chunked_prefill\": True,\n",
    "#         \"max_num_batched_tokens\": 4096,\n",
    "#         \"max_model_len\": 16384,\n",
    "#     },\n",
    "#     concurrency=1,\n",
    "#     batch_size=32,\n",
    "# )\n",
    "# processor = build_llm_processor(\n",
    "#     config,\n",
    "#     preprocess=lambda row: dict(\n",
    "#         messages=[\n",
    "#             {\"role\": \"user\", \"content\": row[\"item\"]}\n",
    "#         ],\n",
    "#         sampling_params=dict(\n",
    "#             temperature=0.3,\n",
    "#             max_tokens=250,\n",
    "#         )\n",
    "#     ),\n",
    "#     postprocess=lambda row: dict(\n",
    "#         answer=row[\"generated_text\"],\n",
    "#         **row  # This will return all the original columns in the dataset.\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# ds = ray.data.from_items([\"Start of the haiku is: Complete this for me...\"])\n",
    "\n",
    "# ds = processor(ds)\n",
    "# ds.show(limit=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
