{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb588b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ad69379",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\", padding_side = 'right')\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=TaskType.CAUSAL_LM,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],  # or add k/o_proj for broader adaptation\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(base_model, config)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2fe5b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = PeftModel.from_pretrained(base_model, './latest_model')\n",
    "# for name, param in model.named_parameters():\n",
    "#     if 'lora' in name:\n",
    "#         param.requires_grad = True\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '<|pad|>',\n",
    "                              'bos_token': '<|im_start|>',\n",
    "                              'eos_token': '<|im_end|>'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a721df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_template = (\n",
    "    \"{% set image_count = namespace(value=0) %}\"\n",
    "    \"{% set video_count = namespace(value=0) %}\"\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{% if loop.first and message['role'] != 'system' %}\"\n",
    "    \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "    \"{% endif %}\"\n",
    "    \"<|im_start|>{{ message['role'] }}\\n\"\n",
    "    \"{% if message['content'] is string %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}\"\n",
    "    \"{% generation %}\"\n",
    "    \"{{ message['content'] }}\"\n",
    "    \"{% endgeneration %}\"\n",
    "    \"{% else %}\"\n",
    "    \"{{ message['content'] }}\"\n",
    "    \"{% endif %}\"\n",
    "    \"<|im_end|>\\n\"\n",
    "    \"{% else %}\"\n",
    "    \"{% for content in message['content'] %}\"\n",
    "    \"{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}\"\n",
    "    \"{% set image_count.value = image_count.value + 1 %}\"\n",
    "    \"{% if add_vision_id %}\"\n",
    "    \"Picture {{ image_count.value }}: \"\n",
    "    \"{% endif %}\"\n",
    "    \"<|vision_start|><|image_pad|><|vision_end|>\"\n",
    "    \"{% elif content['type'] == 'video' or 'video' in content %}\"\n",
    "    \"{% set video_count.value = video_count.value + 1 %}\"\n",
    "    \"{% if add_vision_id %}\"\n",
    "    \"Video {{ video_count.value }}: \"\n",
    "    \"{% endif %}\"\n",
    "    \"<|vision_start|><|video_pad|><|vision_end|>\"\n",
    "    \"{% elif 'text' in content %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}\"\n",
    "    \"{% generation %}\"\n",
    "    \"{{ content['text'] }}\"\n",
    "    \"{% endgeneration %}\"\n",
    "    \"{% else %}\"\n",
    "    \"{{ content['text'] }}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"<|im_end|>\\n\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"<|im_start|>assistant\\n\"\n",
    "    \"{% endif %}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e67229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFTSmolTalkDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length = 620):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        messages = self.dataset[idx]['messages']\n",
    "\n",
    "        new_messages = []\n",
    "        for m in messages:\n",
    "            if not new_messages and m[\"role\"] == \"user\":\n",
    "                new_messages.append(m)\n",
    "            elif new_messages and m[\"role\"] == \"assistant\":\n",
    "                new_messages.append(m)\n",
    "                break\n",
    "        if len(new_messages) != 2:\n",
    "            return self._empty_item()\n",
    "        \n",
    "        print(new_messages)\n",
    "        \n",
    "        try:\n",
    "            not_tokenized = tokenizer.apply_chat_template(\n",
    "                new_messages,\n",
    "                tokenize = False\n",
    "            )\n",
    "            \n",
    "            tokenized = tokenizer.apply_chat_template(\n",
    "                new_messages,\n",
    "                tokenize = True,\n",
    "                max_length = self.max_length,\n",
    "                padding = 'max_length',\n",
    "                truncation = 'only_second',\n",
    "                return_dict = True,\n",
    "                return_assistant_tokens_mask=True,\n",
    "                add_generation_prompt = False,\n",
    "                chat_template = potential_template,\n",
    "                return_tensors = 'pt'\n",
    "            )\n",
    "        except:\n",
    "            return self._empty_item()\n",
    "\n",
    "        input_ids = tokenized['input_ids']\n",
    "        assistant_masks = tokenized['assistant_masks']\n",
    "\n",
    "        mod_assistant_mask = assistant_masks.clone()\n",
    "        matches = (input_ids == tokenizer.convert_tokens_to_ids(\"<|im_end|>\"))\n",
    "        indices = torch.nonzero(matches)\n",
    "        mod_assistant_mask[tuple(indices[-1])] = 1 # inlcude end speaking token in assistant to include in lables\n",
    "\n",
    "        attention_mask = tokenized['attention_mask']\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[mod_assistant_mask == 0] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids.squeeze(0),\n",
    "            'attention_mask': attention_mask.squeeze(0),\n",
    "            'labels': labels.squeeze(0),\n",
    "            # 'not_tokenized_str': not_tokenized,\n",
    "        }\n",
    "    \n",
    "    def _empty_item(self):\n",
    "        return {\n",
    "            'input_ids': torch.zeros(self.max_length, dtype=torch.int32),\n",
    "            'attention_mask': torch.zeros(self.max_length, dtype=torch.int32),\n",
    "            'labels': torch.full((self.max_length,), -100, dtype=torch.int32),\n",
    "            # 'not_tokenized_str': \"\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "476ad847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d42cfdca1a74287bcab91ca847091b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/460341 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/datasets/builder.py:1871\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[39m\n\u001b[32m   1870\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1871\u001b[39m     \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1872\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m CastError \u001b[38;5;28;01mas\u001b[39;00m cast_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/datasets/arrow_writer.py:648\u001b[39m, in \u001b[36mArrowWriter.write_table\u001b[39m\u001b[34m(self, pa_table, writer_batch_size)\u001b[39m\n\u001b[32m    647\u001b[39m \u001b[38;5;28mself\u001b[39m._num_examples += pa_table.num_rows\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpa_writer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/pyarrow/ipc.pxi:529\u001b[39m, in \u001b[36mpyarrow.lib._CRecordBatchWriter.write_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/pyarrow/error.pxi:89\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/fsspec/implementations/local.py:432\u001b[39m, in \u001b[36mLocalFileOpener.write\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/datasets/builder.py:1887\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[39m\n\u001b[32m   1886\u001b[39m num_shards = shard_id + \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1887\u001b[39m num_examples, num_bytes = \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1888\u001b[39m writer.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/datasets/arrow_writer.py:665\u001b[39m, in \u001b[36mArrowWriter.finalize\u001b[39m\u001b[34m(self, close_stream)\u001b[39m\n\u001b[32m    664\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m close_stream:\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/fsspec/implementations/local.py:450\u001b[39m, in \u001b[36mLocalFileOpener.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 28] No space left on device",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDatasetGenerationError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# train_smoltalk_loaded = load_dataset(\"HuggingFaceTB/smol-smoltalk\", split=\"train\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m val_smoltalk_loaded = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHuggingFaceTB/smol-smoltalk\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/datasets/load.py:2084\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2081\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance.as_streaming_dataset(split=split)\n\u001b[32m   2083\u001b[39m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2084\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2090\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2092\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   2093\u001b[39m keep_in_memory = (\n\u001b[32m   2094\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   2095\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/datasets/builder.py:925\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    924\u001b[39m     prepare_split_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_proc\u001b[39m\u001b[33m\"\u001b[39m] = num_proc\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[32m    932\u001b[39m \u001b[38;5;28mself\u001b[39m.info.dataset_size = \u001b[38;5;28msum\u001b[39m(split.num_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info.splits.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/datasets/builder.py:1001\u001b[39m, in \u001b[36mDatasetBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[39m\n\u001b[32m    997\u001b[39m split_dict.add(split_generator.split_info)\n\u001b[32m    999\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1000\u001b[39m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1003\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m   1004\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot find data file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1005\u001b[39m         + (\u001b[38;5;28mself\u001b[39m.manual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1006\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1007\u001b[39m         + \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[32m   1008\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/datasets/builder.py:1742\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split\u001b[39m\u001b[34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[39m\n\u001b[32m   1740\u001b[39m job_id = \u001b[32m0\u001b[39m\n\u001b[32m   1741\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m1742\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1743\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_prepare_split_args\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1745\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1746\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/datasets/builder.py:1898\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[39m\n\u001b[32m   1896\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[32m   1897\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[33m\"\u001b[39m\u001b[33mAn error occurred while generating the dataset\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1900\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\n",
      "\u001b[31mDatasetGenerationError\u001b[39m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "# train_smoltalk_loaded = load_dataset(\"HuggingFaceTB/smol-smoltalk\", split=\"train\")\n",
    "val_smoltalk_loaded = load_dataset(\"HuggingFaceTB/smol-smoltalk\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smoltalk = SFTSmolTalkDataset(train_smoltalk_loaded.select(range(72000,200000)), \n",
    "                                    tokenizer = tokenizer)\n",
    "val_smoltalk = SFTSmolTalkDataset(val_smoltalk_loaded.select(range(200000,210000)), \n",
    "                                   tokenizer = tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7605969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoltalk_batch_size = 8\n",
    "smoltalk_num_epochs = 1\n",
    "\n",
    "train_smoltalk_loader = DataLoader(train_smoltalk, batch_size=smoltalk_batch_size)\n",
    "val_smoltalk_loader = DataLoader(val_smoltalk, batch_size=smoltalk_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430eb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train(train_loader, optimizer, writer, epoch, accumulation_steps=64):\n",
    "\n",
    "    total_loss = 0\n",
    "    batch_times = []\n",
    "    progress = tqdm(train_loader, desc=f\"Training Epoch {epoch}\", leave=True)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, batch in enumerate(progress):\n",
    "        start = time.time()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        if torch.all(labels == -100):\n",
    "            continue\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss / accumulation_steps  # normalize loss for accumulation\n",
    "\n",
    "        assert not math.isnan(loss.item()), f'Loss: {loss}, \\nOutputs: {outputs}'\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * accumulation_steps  \n",
    "        avg_loss = total_loss / (i + 1)\n",
    "\n",
    "        if i % 50 == 0 and i != 0:\n",
    "            writer.add_scalar(\"Loss/train\", avg_loss, epoch * len(train_loader) + i)\n",
    "\n",
    "        batch_time = time.time() - start\n",
    "        batch_times.append(batch_time)\n",
    "        avg_time = sum(batch_times) / len(batch_times)\n",
    "        eta = avg_time * (len(train_loader) - (i + 1))\n",
    "        eta_hr, remainder = divmod(int(eta), 3600)\n",
    "        eta_min, eta_sec = divmod(remainder, 60)\n",
    "\n",
    "        progress.set_postfix(loss=[loss.item() * accumulation_steps, avg_loss], eta=f\"{eta_hr}h {eta_min}m {eta_sec}s\")\n",
    "\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            model.save_pretrained('./latest_model')\n",
    "            with open('latest_opt.pkl', 'wb') as f:\n",
    "                pickle.dump(optimizer, f)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def test(test_loader, writer, epoch):\n",
    "    total_loss = 0\n",
    "    batch_times = []\n",
    "    progress = tqdm(test_loader, desc=f\"Testing Epoch {epoch}\", leave=True)\n",
    "\n",
    "    for i, batch in enumerate(progress):\n",
    "        start = time.time()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        if torch.all(labels == -100):\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "\n",
    "        assert not math.isnan(loss.item()), f'Loss: {loss}, \\nOutputs: {outputs}'\n",
    "\n",
    "        batch_time = time.time() - start\n",
    "        batch_times.append(batch_time)\n",
    "        avg_time = sum(batch_times) / len(batch_times)\n",
    "        eta = avg_time * (len(test_loader) - (i + 1))\n",
    "        eta_hr, remainder = divmod(int(eta), 3600)\n",
    "        eta_min, eta_sec = divmod(remainder, 60)\n",
    "\n",
    "\n",
    "        progress.set_postfix(loss=[loss.item(), avg_loss], eta=f\"{eta_hr}h {eta_min}m {eta_sec}s\")\n",
    "\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    writer.add_scalar(\"Loss/val\", avg_loss, epoch)\n",
    "    return avg_loss\n",
    "\n",
    "def fine_tune(train_loader, test_loader, optimizer, num_epochs):\n",
    "    writer = SummaryWriter()  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(train_loader, optimizer, writer, epoch, accumulation_steps=8)\n",
    "        model.save_pretrained('./smoltalk')\n",
    "        val_loss = test(test_loader, writer, epoch)\n",
    "        print(f'Epoch: {epoch}. Train Loss: {train_loss}. Val Loss: {val_loss}.')\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "with open('latest_opt.pkl', 'rb') as f:\n",
    "    optim = pickle.load(f)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('./latest_model')\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.bos_token_id = tokenizer.bos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb783731",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "observations = \"\"\"\n",
    "\"Exception: Truncation error: Second sequence not provided\" happens during right side truncation \n",
    "if the user input is longer than max length in which case, the messages is no longer a message pair\n",
    "\"\"\"\n",
    "\n",
    "fine_tune(\n",
    "    train_loader = train_smoltalk_loader,\n",
    "    test_loader = val_smoltalk_loader,\n",
    "    optimizer = optim,\n",
    "    num_epochs = smoltalk_num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415f6912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
